{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Numerical Computing in Common Lisp Currently, numericals and dense-numericals provides fast arithmetic, comparison, logical, transcendental, and float-rounding operations. While it is possible to use linear algebra routines by interfacing with magicl , unless there is sufficient interest, given (i) the limited developer time (ii) the lack of fast basic mathematic libraries in Common Lisp*, numericals and dense-numericals does not aim to provide fast linear algebra operations at the moment. Improving the interface would be at a higher priority. *There are a fair number of maintained and unmaintained numerical/scientific computing libraries providing (incomplete) linear algebra routines in Common Lisp - magicl , gsll , lisp-matrix lisp-stat/numerical-utilities (see awesome-cl ) - yet none has bothered (or needed?) SIMD based fast basic mathematical operations. This library therefore does not wish to focus on linear algebra before getting the fast basic mathematical operations get suitably polished. Getting started Click one of the below links depending on your (i) proficiency in programming (ii) proficiency in Common Lisp (iii) the task at hand. I want to get things done reasonably fast. I don't care about maximizing runtime performance so long as it isn't abysmal. I want to operate on CLHS provided arrays. They are good enough for me. I don't care about your \"new fancy XYZ arrays\". I'm curious about these new fancy arrays. I myself have a fair share of problems with CLHS provided arrays, and certainly wish they could be better. I want to extract the maximum juice out of my machine. I know why I need it. I care about every last bit of performance. I'm happy with CLHS provided arrays, OR I wish we had a better alternative to CLHS arrays. I'm a reasonably proficient lisper and have a basic idea about maximizing performance, OR I'm new to lisp, but I have a basic idea about maximizing performance, OR I'm new to lisp, and this will be amongst my first times learning to optimize performance. Design Decisions Why not GSL? GSL uses its own vector and matrix wrappers, and while one can certainly make CFFI use them, it doesn't seem like a good way. Using BLAS and LAPACK looks better. Getting started If you are happy with Common Lisp arrays, you can (ql:quickload \"numericals\") . Otherwise if you think arrays could look nicer, and think numpy's way of doing things are sensible*, then you can (ql:quickload \"dense-numericals\") . (uiop:define-package :numericals-demo ;; (:mix :numericals :cl)) (:mix :dense-numericals :dense-arrays-plus-lite :cl)) An introduction to dense-arrays The arrays that dense-numericals works on are provided by dense-arrays . At its simplest, these are wrappers around cl:vector and enable numpy-like* copy-free slicing aka copy-free selection of array elements / axes. CL-USER> (setq cl:*print-length* 10) CL-USER> (let ((a (cl:make-array '(1000 1000)))) (select:select a t 0)) ; select the 0th element of every row #(0 0 0 0 0 0 0 0 0 0 ...) CL-USER> (let ((a (dense-arrays:make-array '(1000 1000)))) (dense-arrays:aref a nil 0)) ; select the 0th element of every row #<STANDARD-DENSE-ARRAY NIL 1000 T 0 0 0 0 0 0 0 0 0 0 ... {10511DAA23}> CL-USER> (let ((a (cl:make-array '(1000 1000)))) (time (loop for i below 1000 do (select:select a t i)))) Evaluation took: 0.159 seconds of real time 0.159541 seconds of total run time (0.159541 user, 0.000000 system) 100.63% CPU 352,387,000 processor cycles 10,863,248 bytes consed NIL CL-USER> (let ((a (dense-arrays:make-array '(1000 1000)))) (time (loop for i below 1000 do (dense-arrays:aref a nil i)))) Evaluation took: 0.000 seconds of real time 0.000910 seconds of total run time (0.000878 user, 0.000032 system) 100.00% CPU 2,001,298 processor cycles 261,904 bytes consed NIL Here, the select:select operation copies over the elements of the array into the new array. While the dense-arrays:aref merely returns another wrapper over a certain section of the array. Note that it is not that select:select purposefully copies over the elements; the case is that given the way Common Lisp arrays are, it actually is not possible to implement the functionality of select:select efficiently. For instance, numcl:aref provides a similar functionality as select:select , however it too is slow in the general case: CL-USER> (let ((a (numcl:zeros '(1000 1000) :type t))) ;; select the ith element of all the rows, aka select the ith column (time (loop for i below 1000 do (numcl:aref a t i)))) Evaluation took: 7.287 seconds of real time 7.290938 seconds of total run time (7.290938 user, 0.000000 system) [ Run times consist of 0.051 seconds GC time, and 7.240 seconds non-GC time. ] 100.05% CPU 16,097,057,600 processor cycles 1,752,658,096 bytes consed NIL CL-USER> (let ((a (numcl:zeros '(1000 1000) :type t))) ;; select the ith element of all the columns, aka select the ith row (time (loop for i below 1000 do (numcl:aref a i t)))) Evaluation took: 0.000 seconds of real time 0.001688 seconds of total run time (0.001618 user, 0.000070 system) 100.00% CPU 3,718,742 processor cycles 457,408 bytes consed NIL The second case is faster here, because cl:make-array does provide displaced-to and displaced-index-offset which enable the creation of \"wrapper\" non-simple arrays if the elements are contiguous. This works only for the rightmost dimension, which is why in (numcl:aref a i t) with t being the rightmost index indicating \"select all the elements along this axis\", there is a way to avoid copying the array elements, but (numcl:aref a t i) necessitates copying. By contrast, dense-arrays::dense-array provide for multidimensional strides and offsets, enabling copy free slicing for arbitrary axes. This enables the interpretation of the same cl:vector (the storage slot below) in multiple ways: DENSE-ARRAYS> (make-array '(2 3) :constructor #'+) #<STANDARD-DENSE-ARRAY 2x3 :ROW-MAJOR T (0 1 2) (1 2 3) {100330F653}> DENSE-ARRAYS> (describe *) #<STANDARD-DENSE-ARRAY 2x3 :ROW-MAJOR T {100330F653}> [standard-object] Slots with :INSTANCE allocation: STORAGE = #(0 1 2 1 2 3) DIMENSIONS = (2 3) ELEMENT-TYPE = T RANK = 2 TOTAL-SIZE = 6 STRIDES = (3 1) OFFSETS = (0 0) LAYOUT = :ROW-MAJOR ROOT-ARRAY = NIL ; No value DENSE-ARRAYS> (aref (make-array '(2 3) :constructor #'+) 1) #<STANDARD-DENSE-ARRAY NIL 3 T 1 2 3 {10033B1543}> DENSE-ARRAYS> (aref (make-array '(2 3) :constructor #'+) nil 1) #<STANDARD-DENSE-ARRAY NIL 2 T 1 2 {10033B23E3}> DENSE-ARRAYS> (describe *) #<STANDARD-DENSE-ARRAY NIL 2 T {10033B23E3}> [standard-object] Slots with :INSTANCE allocation: STORAGE = #(0 1 2 1 2 3) DIMENSIONS = (2) ELEMENT-TYPE = T RANK = 1 TOTAL-SIZE = 2 STRIDES = (3) OFFSETS = (1) CONTIGUOUS-P = NIL ROOT-ARRAY = #<STANDARD-DENSE-ARRAY 2x3 T {10033B2183}> ; No value Indeed this also means that the arrays might not be contiguous. Contiguous dense arrays are of type dense-arrays:simple-array ; for such arrays, (array-layout dense-array) returns either :row-major or :column-major which is also seen in the printed representations above. Otherwise if (array-layout dense-array) returns nil , then there is no guarantee that the array is a contiguous array, and one can use dense-arrays:make-array or dense-arrays:copy-array to obtain a dense-arrays:simple-array aka contiguous array. Dense arrays provides two major types (dense-arrays:array &optional element-type rank/dimensions) and (dense-arrays:simple-array &optional element-type rank/dimensions) analogous to their CL counterparts. These should work with typep and subtypep , a caveat being that without the use of extensible-compound-types , the dimensions are reduced to ranks. Thus, CL-USER> (alexandria:type= '(dense-arrays:array * (2 3 4)) ; based on CL:TYPEP '(dense-arrays:array * 3)) T T CL-USER> (extensible-compound-types:type= '(dense-arrays:array * (2 3 4)) '(dense-arrays:array * 3)) NIL T For advanced usage relating to customizing the behavior of dense-arrays, see here . For a demonstration on optimizing dense-arrays, see here (you just need to use dense-arrays:do-arrays and declare the types!). *Numpy arrays with their multidimensional strides and offsets might not be the best way to do things either. See here for a discussion. Using numericals to extend numericals: implementing mean As I write this, numericals already has the following operators: sum divide expt multiply operation. In this section, we implement the mean operator as a simple wrapper around the existing operators. We start with a basic runtime correct function, but also go into compile time notes and optimizations. Getting the correct answer at runtime We start with mean , and take an inspiration from numpy to formulate the function's parameter list: (numericals.common:compiler-in-package numericals.common:*compiler-package*) (define-polymorphic-function nu:mean (array-like &key out axes keep-dims) :overwrite t) We use array-like here, since from the user's perspective it will be convenient to avoid having to explicitly cast it into an array. Thus, immediately at the outset, we have two cases: (defpolymorph nu:mean ((array-like list) &key out axes keep-dims) t ...) (defpolymorph nu:mean ((array-like nu:array) &key out axes keep-dims) t ...) The subtype polymorphism provided by polymorphic-functions should help with the optimization: we should only be required to define a few cases for the nu:array , without bothering about each array-element-type. For the list case, the simplest definition would be: (defpolymorph nu:mean ((array-like list) &key out axes keep-dims) t (nu:mean (nu:asarray array-like) :out out :axes axes :keep-dims keep-dims)) Since a runtime array allocation is involved, we include a suboptimal-note in our definition, so that the user can be helped later. In addition, we also know that inlining this function won't cost much in terms of code bloat and there is a guarantee of non-recursion, thus we also provide an :inline t option to override polymorphic function's note about not inlining it. (defpolymorph (nu:mean :inline t :suboptimal-note runtime-array-allocation) ((array-like list) &key out axes) t (nu:mean (nu:asarray array-like) :out out :axes axes)) We will return to this case later. But now, let's start with the polymorph for nu:array . The plan is to calculate the sum and then divide by the number-of-elements : (defpolymorph nu:mean ((array-like nu:array) &key out axes keep-dims) t (let* ((sum (nu:sum array :out out :axes axes :keep-dims keep-dims)) (number-of-elements ...)) (nu:divide sum number-of-elements :out out))) Wherever possible, we supply the out argument. sum could either be an array, or it could be a real. To calculate the number-of-elements , we consider three cases: axes is NIL, or an integer, or a list of integers. (defpolymorph nu:mean ((array-like nu:array) &key out axes keep-dims) t (let* ((sum (nu:sum array-like :out out :axes axes :keep-dims keep-dims)) (number-of-elements (etypecase axes (null (array-total-size array-like)) (integer (nu:shape array-like axes)) (list (loop :for d :in (narray-dimensions array-like) :for i :from 0 :with number-of-elements := 1 :if (member i axes) :do (setf number-of-elements (* number-of-elements d)) :finally (return number-of-elements)))))) (declare (type real number-of-elements)) (nu:divide sum number-of-elements :out out))) These two polymorphs seem enough to get us the required functionality at runtime. We can check out a few cases to see if the function is working correctly. Ideally, writing a test case would be recommendable. IMPL> (numericals.common:compiler-in-package numericals.common:*compiler-package*) #<PACKAGE \"DENSE-NUMERICALS.IMPL\"> IMPL> (setq nu:*array-element-type* 'single-float) SINGLE-FLOAT IMPL> (nu:mean '(1 2 3)) 2.0 IMPL> (nu:mean '(1 2 3.5)) 2.1666667 IMPL> (nu:mean '((1 2 3) (4 5 6))) 3.5 IMPL> (nu:mean '((1 2 3) (4 5 6)) :axes 0) #<STANDARD-DENSE-ARRAY :ROW-MAJOR 3 SINGLE-FLOAT 2.500 3.500 4.500 {10596A0B33}> IMPL> (nu:mean '((1 2 3) (4 5 6)) :axes 1) #<STANDARD-DENSE-ARRAY :ROW-MAJOR 2 SINGLE-FLOAT 2.000 5.000 {10596A47C3}> IMPL> (nu:mean '((1 2 3) (4 5 6)) :axes 1 :keep-dims t) #<STANDARD-DENSE-ARRAY :ROW-MAJOR 2x1 SINGLE-FLOAT ( 2.000 ) ( 5.000 ) {102447F653}> IMPL> (nu:mean '((1 2 3) (4 5 6)) :keep-dims t) #<STANDARD-DENSE-ARRAY :ROW-MAJOR 1x1 SINGLE-FLOAT ( 3.500 ) {102448B023}> Indeed, these seem good enough. Now let's turn to optimization and compiler notes! Optimizing a specific case of nu:mean For this part, it is helpful to use ALIVE or SLIME or equivalent, so that you can compile one form at a time and see the notes and warnings corresponding to it. We will be trying to optimize a simpler single-float-mean function that calculates the overall mean of single float arrays. For our case, we note that one of the conditions which the mean function should satisfy includes that no runtime array allocation is involved. For the other conditions, let's use the compiler notes emitted by polymorphic-functions to discover what needs to be optimized! If you are using SBCL (see (cl:lisp-implementation-type) , you might also want to declaim the following to focus on the compiler notes emitted by polymorphic-functions alone. (declaim (sb-ext:muffle-conditions sb-ext:compiler-note)) ;; You can revert it with ;; (declaim (sb-ext:unmuffle-conditions sb-ext:compiler-note)) Now, compiling the previous nu:mean polymorph with (optimize speed) emits some notes: (defpolymorph nu:mean ((array-like nu:array) &key out axes keep-dims) t (declare (optimize speed)) (let* ((sum (nu:sum array-like :out out :axes axes :keep-dims keep-dims)) (number-of-elements (etypecase axes (null (array-total-size array-like)) (integer (nu:shape array-like axes)) (list (loop :for d :in (narray-dimensions array-like) :for i :from 0 :with number-of-elements := 1 :if (member i axes) :do (setf number-of-elements (* number-of-elements d)) :finally (return number-of-elements)))))) (declare (type real number-of-elements)) (nu:divide sum number-of-elements :out out))) These notes mainly concern nu:sum and nu:divide and are about the type of out being not known. If you are using SLIME (or ALIVE?) you should see the (nu:sum array-like :out out :axes axes :keep-dims keep-dims) and (nu:divide sum number-of-elements :out out) forms being underlined to indicate where the notes are being emitted from. What do we know about out ? That, if it is supplied and if it has to be supplied, then it must be a nu:array . To keep things simple, we check this using whether or not the value of out is of type null . This naive way makes our function looks like this: (defpolymorph nu:mean ((array-like nu:array) &key out axes keep-dims) t (declare (optimize speed)) (if (null out) (locally (declare (type null out)) (let* ((sum (nu:sum array-like :out out :axes axes :keep-dims keep-dims)) (number-of-elements (etypecase axes (null (array-total-size array-like)) (integer (nu:shape array-like axes)) (list (loop :for d :in (narray-dimensions array-like) :for i :from 0 :with number-of-elements := 1 :if (member i axes) :do (setf number-of-elements (* number-of-elements d)) :finally (return number-of-elements)))))) (declare (type real number-of-elements)) (nu:divide sum number-of-elements :out out))) (locally (declare (type nu:array out)) (let* ((sum (nu:sum array-like :out out :axes axes :keep-dims keep-dims)) (number-of-elements (etypecase axes (null (array-total-size array-like)) (integer (nu:shape array-like axes)) (list (loop :for d :in (narray-dimensions array-like) :for i :from 0 :with number-of-elements := 1 :if (member i axes) :do (setf number-of-elements (* number-of-elements d)) :finally (return number-of-elements)))))) (declare (type real number-of-elements)) (nu:divide sum number-of-elements :out out))))) And compiling this gets us quite a few notes. Let's focus on one of them: ; (Compiler) Macro of ; #<POLYMORPHIC-FUNCTIONS:POLYMORPHIC-FUNCTION TWO-ARG-FN/NON-COMPARISON (13)> ; is unable to optimize ; (TWO-ARG-FN/NON-COMPARISON 'NU:DIVIDE X Y :BROADCAST BROADCAST) ; in ; (LAMBDA (X Y &KEY (BROADCAST NU:*BROADCAST-AUTOMATICALLY*) (OUT NIL) &AUX) ; (DECLARE (TYPE NULL OUT) ; (TYPE T BROADCAST) ; (TYPE REAL Y) ; (TYPE T X) ; (IGNORABLE)) ; (DECLARE (IGNORABLE OUT)) ; (BLOCK NU:DIVIDE ; (LOCALLY ; (TWO-ARG-FN/NON-COMPARISON 'NU:DIVIDE X Y :BROADCAST BROADCAST)))) ; generated from ; (NU:DIVIDE SUM NUMBER-OF-ELEMENTS :OUT OUT) ; because: ; ; Type of ; X ; could not be determined ; Type of ; BROADCAST ; could not be determined From this, we can tell that x corresponds to sum and that nowhere have we told about the type of sum . In fact, the type of sum : it could be a real or it could be a nu:array . And in the case it is of type nu:array , we must necessarily broadcast. Thus, we have the following new definition: (defpolymorph nu:mean ((array-like nu:array) &key out axes keep-dims) t (declare (optimize speed)) (if (null out) (locally (declare (type null out)) (let* ((sum (nu:sum array-like :out out :axes axes :keep-dims keep-dims)) (number-of-elements (etypecase axes (null (array-total-size array-like)) (integer (nu:shape array-like axes)) (list (loop :for d :in (narray-dimensions array-like) :for i :from 0 :with number-of-elements := 1 :if (member i axes) :do (setf number-of-elements (* number-of-elements d)) :finally (return number-of-elements)))))) (declare (type real number-of-elements)) (if (realp sum) (locally (declare (type real sum)) (nu:divide sum number-of-elements)) (locally (declare (type nu:array sum)) (nu:divide sum number-of-elements :out out :broadcast t))))) (locally (declare (type nu:array out)) (let* ((sum (nu:sum array-like :out out :axes axes :keep-dims keep-dims)) (number-of-elements (etypecase axes (null (array-total-size array-like)) (integer (nu:shape array-like axes)) (list (loop :for d :in (narray-dimensions array-like) :for i :from 0 :with number-of-elements := 1 :if (member i axes) :do (setf number-of-elements (* number-of-elements d)) :finally (return number-of-elements)))))) (declare (type real number-of-elements)) (if (realp sum) (locally (declare (type real sum)) (nu:divide sum number-of-elements)) (locally (declare (type nu:array sum)) (nu:divide sum number-of-elements :out out :broadcast t))))))) Compiling this gets us some other notes now: ; While compiling ; (TWO-ARG-FN/NON-COMPARISON 'NU:DIVIDE X Y :BROADCAST BROADCAST) ; in ; (LAMBDA (X Y &KEY (BROADCAST NU:*BROADCAST-AUTOMATICALLY*) (OUT NIL) &AUX) ; (DECLARE (TYPE NULL OUT) ; (TYPE (EQL T) BROADCAST) ; (TYPE REAL Y) ; (TYPE ARRAY X) ; (IGNORABLE)) ; (DECLARE (IGNORABLE OUT)) ; (BLOCK NU:DIVIDE ; (LOCALLY ; (TWO-ARG-FN/NON-COMPARISON 'NU:DIVIDE X Y :BROADCAST BROADCAST)))) ; generated from ; (NU:DIVIDE SUM NUMBER-OF-ELEMENTS :OUT OUT :BROADCAST T) ; Following notes were encountered: ; ; No applicable POLYMORPH discovered for polymorphic-function ; TWO-ARG-FN/NON-COMPARISON ; and ARGS: ; ; ('NU:DIVIDE X Y :BROADCAST BROADCAST) ; ; derived to be of TYPES: ; ; ((EQL NU:DIVIDE) ARRAY REAL (EQL :BROADCAST) (EQL T)) ; ; Available Effective-Type-Lists include: ; ; (SYMBOL NUMBER NUMBER &KEY (:BROADCAST T) (:OUT NULL)) ; (SYMBOL LIST LIST &KEY (:BROADCAST (OR NULL T)) (:OUT ARRAY)) ; (SYMBOL NUMBER LIST &KEY (:BROADCAST (OR NULL T)) (:OUT ARRAY)) ; (SYMBOL LIST NUMBER &KEY (:BROADCAST (OR NULL T)) (:OUT ARRAY)) ; (SYMBOL LIST LIST &KEY (:BROADCAST (OR NULL T)) (:OUT NULL)) ; (SYMBOL NUMBER LIST &KEY (:BROADCAST (OR NULL T)) (:OUT NULL)) ; (SYMBOL LIST NUMBER &KEY (:BROADCAST (OR NULL T)) (:OUT NULL)) ; (SYMBOL (SIMPLE-ARRAY <TYPE>) (SIMPLE-ARRAY <TYPE>) &KEY ; (:BROADCAST (OR NULL NULL)) (:OUT (SIMPLE-ARRAY <TYPE>))) ; (SYMBOL (SIMPLE-ARRAY <TYPE>) (SIMPLE-ARRAY <TYPE>) &KEY ; (:BROADCAST (OR NULL NULL)) (:OUT NULL)) ; (SYMBOL (ARRAY <TYPE>) (ARRAY <TYPE>) &KEY (:BROADCAST (OR NULL T)) ; (:OUT (ARRAY <TYPE>))) ; (SYMBOL (ARRAY <TYPE>) (ARRAY <TYPE>) &KEY (:BROADCAST (OR NULL T)) ; (:OUT NULL)) ; (SYMBOL (ARRAY <TYPE>) NUMBER &KEY (:BROADCAST (OR NULL (NOT NULL))) ; (:OUT (OR NULL (ARRAY <TYPE>)))) ; (SYMBOL NUMBER (ARRAY <TYPE>) &KEY (:BROADCAST (OR NULL (NOT NULL))) ; (:OUT (OR NULL (ARRAY <TYPE>)))) No applicable polymorph for divide ? How come? Certainly there exists a polymorph for divide that can take in a nu:array and a real as input, right? Careful observation tells us that even though such a polymorph exists (see the last two type lists), such a polymorph also has constraints on the type of out . In particular, the element-type of out must match the element type of x or y , or in our case, the sum . In fact, the element-type of sum if it happens to be an nu:array is also determined by the element-type of the input array-like . divide: We know that the second argument to divide will always be a real , and as such broadcasting cannot be avoided. However, divide accepts an out argument, and we would like to avoid runtime allocation of out . But, while supplying out we would want to check if or not sum is a real, because if sum is a real, then both sum and number-of-elements are real, and then we do not need to supply out . The following is our modified function then. Let's try to compile with (optimize speed) to see what notes are emitted. (defpolymorph nu:mean ((array-like nu:array) &key out axes keep-dims) t (let* ((sum (nu:sum array-like :out out :axes axes :keep-dims keep-dims)) (number-of-elements (etypecase axes (null (array-total-size array-like)) (integer (nu:shape array-like axes)) (list (loop :for d :in (narray-dimensions array-like) :for i :from 0 :with number-of-elements := 1 :if (member i axes) :do (setf number-of-elements (* number-of-elements d)) :finally (return number-of-elements)))))) (declare (type real number-of-elements)) (if (realp sum) (locally (declare (type real sum)) (nu:divide sum number-of-elements)) (locally (declare (type nu:array sum)) (nu:divide sum number-of-elements :out out :broadcast t))))) (defun mean-caller (x) (declare (optimize speed)) (nu:mean x)) The crucial note this emits is: Type of X could not be determined So, we specify the type of x : (defun mean-caller (x) (declare (optimize speed) (type (simple-array single-float) x)) (nu:mean x)) This gets us more information. (defpolymorph nu:mean ((array-like nu:array) &key out axes keep-dims) t (let* ((sum (nu:sum array-like :out out :axes axes :keep-dims keep-dims)) (number-of-elements (etypecase axes (null (array-total-size array-like)) (integer (nu:shape array-like axes)) (list (loop :for d :in (narray-dimensions array-like) :for i :from 0 :with number-of-elements := 1 :if (member i axes) :do (setf number-of-elements (* number-of-elements d)) :finally (return number-of-elements)))))) (declare (type real number-of-elements)) (nu:divide sum number-of-elements :out out))) Interfacing with magicl","title":"Numerical Computing in Common Lisp"},{"location":"#numerical-computing-in-common-lisp","text":"Currently, numericals and dense-numericals provides fast arithmetic, comparison, logical, transcendental, and float-rounding operations. While it is possible to use linear algebra routines by interfacing with magicl , unless there is sufficient interest, given (i) the limited developer time (ii) the lack of fast basic mathematic libraries in Common Lisp*, numericals and dense-numericals does not aim to provide fast linear algebra operations at the moment. Improving the interface would be at a higher priority. *There are a fair number of maintained and unmaintained numerical/scientific computing libraries providing (incomplete) linear algebra routines in Common Lisp - magicl , gsll , lisp-matrix lisp-stat/numerical-utilities (see awesome-cl ) - yet none has bothered (or needed?) SIMD based fast basic mathematical operations. This library therefore does not wish to focus on linear algebra before getting the fast basic mathematical operations get suitably polished.","title":"Numerical Computing in Common Lisp"},{"location":"#getting-started","text":"Click one of the below links depending on your (i) proficiency in programming (ii) proficiency in Common Lisp (iii) the task at hand. I want to get things done reasonably fast. I don't care about maximizing runtime performance so long as it isn't abysmal. I want to operate on CLHS provided arrays. They are good enough for me. I don't care about your \"new fancy XYZ arrays\". I'm curious about these new fancy arrays. I myself have a fair share of problems with CLHS provided arrays, and certainly wish they could be better. I want to extract the maximum juice out of my machine. I know why I need it. I care about every last bit of performance. I'm happy with CLHS provided arrays, OR I wish we had a better alternative to CLHS arrays. I'm a reasonably proficient lisper and have a basic idea about maximizing performance, OR I'm new to lisp, but I have a basic idea about maximizing performance, OR I'm new to lisp, and this will be amongst my first times learning to optimize performance.","title":"Getting started"},{"location":"#design-decisions","text":"Why not GSL? GSL uses its own vector and matrix wrappers, and while one can certainly make CFFI use them, it doesn't seem like a good way. Using BLAS and LAPACK looks better.","title":"Design Decisions"},{"location":"#getting-started_1","text":"If you are happy with Common Lisp arrays, you can (ql:quickload \"numericals\") . Otherwise if you think arrays could look nicer, and think numpy's way of doing things are sensible*, then you can (ql:quickload \"dense-numericals\") . (uiop:define-package :numericals-demo ;; (:mix :numericals :cl)) (:mix :dense-numericals :dense-arrays-plus-lite :cl))","title":"Getting started"},{"location":"#an-introduction-to-dense-arrays","text":"The arrays that dense-numericals works on are provided by dense-arrays . At its simplest, these are wrappers around cl:vector and enable numpy-like* copy-free slicing aka copy-free selection of array elements / axes. CL-USER> (setq cl:*print-length* 10) CL-USER> (let ((a (cl:make-array '(1000 1000)))) (select:select a t 0)) ; select the 0th element of every row #(0 0 0 0 0 0 0 0 0 0 ...) CL-USER> (let ((a (dense-arrays:make-array '(1000 1000)))) (dense-arrays:aref a nil 0)) ; select the 0th element of every row #<STANDARD-DENSE-ARRAY NIL 1000 T 0 0 0 0 0 0 0 0 0 0 ... {10511DAA23}> CL-USER> (let ((a (cl:make-array '(1000 1000)))) (time (loop for i below 1000 do (select:select a t i)))) Evaluation took: 0.159 seconds of real time 0.159541 seconds of total run time (0.159541 user, 0.000000 system) 100.63% CPU 352,387,000 processor cycles 10,863,248 bytes consed NIL CL-USER> (let ((a (dense-arrays:make-array '(1000 1000)))) (time (loop for i below 1000 do (dense-arrays:aref a nil i)))) Evaluation took: 0.000 seconds of real time 0.000910 seconds of total run time (0.000878 user, 0.000032 system) 100.00% CPU 2,001,298 processor cycles 261,904 bytes consed NIL Here, the select:select operation copies over the elements of the array into the new array. While the dense-arrays:aref merely returns another wrapper over a certain section of the array. Note that it is not that select:select purposefully copies over the elements; the case is that given the way Common Lisp arrays are, it actually is not possible to implement the functionality of select:select efficiently. For instance, numcl:aref provides a similar functionality as select:select , however it too is slow in the general case: CL-USER> (let ((a (numcl:zeros '(1000 1000) :type t))) ;; select the ith element of all the rows, aka select the ith column (time (loop for i below 1000 do (numcl:aref a t i)))) Evaluation took: 7.287 seconds of real time 7.290938 seconds of total run time (7.290938 user, 0.000000 system) [ Run times consist of 0.051 seconds GC time, and 7.240 seconds non-GC time. ] 100.05% CPU 16,097,057,600 processor cycles 1,752,658,096 bytes consed NIL CL-USER> (let ((a (numcl:zeros '(1000 1000) :type t))) ;; select the ith element of all the columns, aka select the ith row (time (loop for i below 1000 do (numcl:aref a i t)))) Evaluation took: 0.000 seconds of real time 0.001688 seconds of total run time (0.001618 user, 0.000070 system) 100.00% CPU 3,718,742 processor cycles 457,408 bytes consed NIL The second case is faster here, because cl:make-array does provide displaced-to and displaced-index-offset which enable the creation of \"wrapper\" non-simple arrays if the elements are contiguous. This works only for the rightmost dimension, which is why in (numcl:aref a i t) with t being the rightmost index indicating \"select all the elements along this axis\", there is a way to avoid copying the array elements, but (numcl:aref a t i) necessitates copying. By contrast, dense-arrays::dense-array provide for multidimensional strides and offsets, enabling copy free slicing for arbitrary axes. This enables the interpretation of the same cl:vector (the storage slot below) in multiple ways: DENSE-ARRAYS> (make-array '(2 3) :constructor #'+) #<STANDARD-DENSE-ARRAY 2x3 :ROW-MAJOR T (0 1 2) (1 2 3) {100330F653}> DENSE-ARRAYS> (describe *) #<STANDARD-DENSE-ARRAY 2x3 :ROW-MAJOR T {100330F653}> [standard-object] Slots with :INSTANCE allocation: STORAGE = #(0 1 2 1 2 3) DIMENSIONS = (2 3) ELEMENT-TYPE = T RANK = 2 TOTAL-SIZE = 6 STRIDES = (3 1) OFFSETS = (0 0) LAYOUT = :ROW-MAJOR ROOT-ARRAY = NIL ; No value DENSE-ARRAYS> (aref (make-array '(2 3) :constructor #'+) 1) #<STANDARD-DENSE-ARRAY NIL 3 T 1 2 3 {10033B1543}> DENSE-ARRAYS> (aref (make-array '(2 3) :constructor #'+) nil 1) #<STANDARD-DENSE-ARRAY NIL 2 T 1 2 {10033B23E3}> DENSE-ARRAYS> (describe *) #<STANDARD-DENSE-ARRAY NIL 2 T {10033B23E3}> [standard-object] Slots with :INSTANCE allocation: STORAGE = #(0 1 2 1 2 3) DIMENSIONS = (2) ELEMENT-TYPE = T RANK = 1 TOTAL-SIZE = 2 STRIDES = (3) OFFSETS = (1) CONTIGUOUS-P = NIL ROOT-ARRAY = #<STANDARD-DENSE-ARRAY 2x3 T {10033B2183}> ; No value Indeed this also means that the arrays might not be contiguous. Contiguous dense arrays are of type dense-arrays:simple-array ; for such arrays, (array-layout dense-array) returns either :row-major or :column-major which is also seen in the printed representations above. Otherwise if (array-layout dense-array) returns nil , then there is no guarantee that the array is a contiguous array, and one can use dense-arrays:make-array or dense-arrays:copy-array to obtain a dense-arrays:simple-array aka contiguous array. Dense arrays provides two major types (dense-arrays:array &optional element-type rank/dimensions) and (dense-arrays:simple-array &optional element-type rank/dimensions) analogous to their CL counterparts. These should work with typep and subtypep , a caveat being that without the use of extensible-compound-types , the dimensions are reduced to ranks. Thus, CL-USER> (alexandria:type= '(dense-arrays:array * (2 3 4)) ; based on CL:TYPEP '(dense-arrays:array * 3)) T T CL-USER> (extensible-compound-types:type= '(dense-arrays:array * (2 3 4)) '(dense-arrays:array * 3)) NIL T For advanced usage relating to customizing the behavior of dense-arrays, see here . For a demonstration on optimizing dense-arrays, see here (you just need to use dense-arrays:do-arrays and declare the types!). *Numpy arrays with their multidimensional strides and offsets might not be the best way to do things either. See here for a discussion.","title":"An introduction to dense-arrays"},{"location":"#using-numericals-to-extend-numericals-implementing-mean","text":"As I write this, numericals already has the following operators: sum divide expt multiply operation. In this section, we implement the mean operator as a simple wrapper around the existing operators. We start with a basic runtime correct function, but also go into compile time notes and optimizations.","title":"Using numericals to extend numericals: implementing mean"},{"location":"#getting-the-correct-answer-at-runtime","text":"We start with mean , and take an inspiration from numpy to formulate the function's parameter list: (numericals.common:compiler-in-package numericals.common:*compiler-package*) (define-polymorphic-function nu:mean (array-like &key out axes keep-dims) :overwrite t) We use array-like here, since from the user's perspective it will be convenient to avoid having to explicitly cast it into an array. Thus, immediately at the outset, we have two cases: (defpolymorph nu:mean ((array-like list) &key out axes keep-dims) t ...) (defpolymorph nu:mean ((array-like nu:array) &key out axes keep-dims) t ...) The subtype polymorphism provided by polymorphic-functions should help with the optimization: we should only be required to define a few cases for the nu:array , without bothering about each array-element-type. For the list case, the simplest definition would be: (defpolymorph nu:mean ((array-like list) &key out axes keep-dims) t (nu:mean (nu:asarray array-like) :out out :axes axes :keep-dims keep-dims)) Since a runtime array allocation is involved, we include a suboptimal-note in our definition, so that the user can be helped later. In addition, we also know that inlining this function won't cost much in terms of code bloat and there is a guarantee of non-recursion, thus we also provide an :inline t option to override polymorphic function's note about not inlining it. (defpolymorph (nu:mean :inline t :suboptimal-note runtime-array-allocation) ((array-like list) &key out axes) t (nu:mean (nu:asarray array-like) :out out :axes axes)) We will return to this case later. But now, let's start with the polymorph for nu:array . The plan is to calculate the sum and then divide by the number-of-elements : (defpolymorph nu:mean ((array-like nu:array) &key out axes keep-dims) t (let* ((sum (nu:sum array :out out :axes axes :keep-dims keep-dims)) (number-of-elements ...)) (nu:divide sum number-of-elements :out out))) Wherever possible, we supply the out argument. sum could either be an array, or it could be a real. To calculate the number-of-elements , we consider three cases: axes is NIL, or an integer, or a list of integers. (defpolymorph nu:mean ((array-like nu:array) &key out axes keep-dims) t (let* ((sum (nu:sum array-like :out out :axes axes :keep-dims keep-dims)) (number-of-elements (etypecase axes (null (array-total-size array-like)) (integer (nu:shape array-like axes)) (list (loop :for d :in (narray-dimensions array-like) :for i :from 0 :with number-of-elements := 1 :if (member i axes) :do (setf number-of-elements (* number-of-elements d)) :finally (return number-of-elements)))))) (declare (type real number-of-elements)) (nu:divide sum number-of-elements :out out))) These two polymorphs seem enough to get us the required functionality at runtime. We can check out a few cases to see if the function is working correctly. Ideally, writing a test case would be recommendable. IMPL> (numericals.common:compiler-in-package numericals.common:*compiler-package*) #<PACKAGE \"DENSE-NUMERICALS.IMPL\"> IMPL> (setq nu:*array-element-type* 'single-float) SINGLE-FLOAT IMPL> (nu:mean '(1 2 3)) 2.0 IMPL> (nu:mean '(1 2 3.5)) 2.1666667 IMPL> (nu:mean '((1 2 3) (4 5 6))) 3.5 IMPL> (nu:mean '((1 2 3) (4 5 6)) :axes 0) #<STANDARD-DENSE-ARRAY :ROW-MAJOR 3 SINGLE-FLOAT 2.500 3.500 4.500 {10596A0B33}> IMPL> (nu:mean '((1 2 3) (4 5 6)) :axes 1) #<STANDARD-DENSE-ARRAY :ROW-MAJOR 2 SINGLE-FLOAT 2.000 5.000 {10596A47C3}> IMPL> (nu:mean '((1 2 3) (4 5 6)) :axes 1 :keep-dims t) #<STANDARD-DENSE-ARRAY :ROW-MAJOR 2x1 SINGLE-FLOAT ( 2.000 ) ( 5.000 ) {102447F653}> IMPL> (nu:mean '((1 2 3) (4 5 6)) :keep-dims t) #<STANDARD-DENSE-ARRAY :ROW-MAJOR 1x1 SINGLE-FLOAT ( 3.500 ) {102448B023}> Indeed, these seem good enough. Now let's turn to optimization and compiler notes!","title":"Getting the correct answer at runtime"},{"location":"#optimizing-a-specific-case-of-numean","text":"For this part, it is helpful to use ALIVE or SLIME or equivalent, so that you can compile one form at a time and see the notes and warnings corresponding to it. We will be trying to optimize a simpler single-float-mean function that calculates the overall mean of single float arrays. For our case, we note that one of the conditions which the mean function should satisfy includes that no runtime array allocation is involved. For the other conditions, let's use the compiler notes emitted by polymorphic-functions to discover what needs to be optimized! If you are using SBCL (see (cl:lisp-implementation-type) , you might also want to declaim the following to focus on the compiler notes emitted by polymorphic-functions alone. (declaim (sb-ext:muffle-conditions sb-ext:compiler-note)) ;; You can revert it with ;; (declaim (sb-ext:unmuffle-conditions sb-ext:compiler-note)) Now, compiling the previous nu:mean polymorph with (optimize speed) emits some notes: (defpolymorph nu:mean ((array-like nu:array) &key out axes keep-dims) t (declare (optimize speed)) (let* ((sum (nu:sum array-like :out out :axes axes :keep-dims keep-dims)) (number-of-elements (etypecase axes (null (array-total-size array-like)) (integer (nu:shape array-like axes)) (list (loop :for d :in (narray-dimensions array-like) :for i :from 0 :with number-of-elements := 1 :if (member i axes) :do (setf number-of-elements (* number-of-elements d)) :finally (return number-of-elements)))))) (declare (type real number-of-elements)) (nu:divide sum number-of-elements :out out))) These notes mainly concern nu:sum and nu:divide and are about the type of out being not known. If you are using SLIME (or ALIVE?) you should see the (nu:sum array-like :out out :axes axes :keep-dims keep-dims) and (nu:divide sum number-of-elements :out out) forms being underlined to indicate where the notes are being emitted from. What do we know about out ? That, if it is supplied and if it has to be supplied, then it must be a nu:array . To keep things simple, we check this using whether or not the value of out is of type null . This naive way makes our function looks like this: (defpolymorph nu:mean ((array-like nu:array) &key out axes keep-dims) t (declare (optimize speed)) (if (null out) (locally (declare (type null out)) (let* ((sum (nu:sum array-like :out out :axes axes :keep-dims keep-dims)) (number-of-elements (etypecase axes (null (array-total-size array-like)) (integer (nu:shape array-like axes)) (list (loop :for d :in (narray-dimensions array-like) :for i :from 0 :with number-of-elements := 1 :if (member i axes) :do (setf number-of-elements (* number-of-elements d)) :finally (return number-of-elements)))))) (declare (type real number-of-elements)) (nu:divide sum number-of-elements :out out))) (locally (declare (type nu:array out)) (let* ((sum (nu:sum array-like :out out :axes axes :keep-dims keep-dims)) (number-of-elements (etypecase axes (null (array-total-size array-like)) (integer (nu:shape array-like axes)) (list (loop :for d :in (narray-dimensions array-like) :for i :from 0 :with number-of-elements := 1 :if (member i axes) :do (setf number-of-elements (* number-of-elements d)) :finally (return number-of-elements)))))) (declare (type real number-of-elements)) (nu:divide sum number-of-elements :out out))))) And compiling this gets us quite a few notes. Let's focus on one of them: ; (Compiler) Macro of ; #<POLYMORPHIC-FUNCTIONS:POLYMORPHIC-FUNCTION TWO-ARG-FN/NON-COMPARISON (13)> ; is unable to optimize ; (TWO-ARG-FN/NON-COMPARISON 'NU:DIVIDE X Y :BROADCAST BROADCAST) ; in ; (LAMBDA (X Y &KEY (BROADCAST NU:*BROADCAST-AUTOMATICALLY*) (OUT NIL) &AUX) ; (DECLARE (TYPE NULL OUT) ; (TYPE T BROADCAST) ; (TYPE REAL Y) ; (TYPE T X) ; (IGNORABLE)) ; (DECLARE (IGNORABLE OUT)) ; (BLOCK NU:DIVIDE ; (LOCALLY ; (TWO-ARG-FN/NON-COMPARISON 'NU:DIVIDE X Y :BROADCAST BROADCAST)))) ; generated from ; (NU:DIVIDE SUM NUMBER-OF-ELEMENTS :OUT OUT) ; because: ; ; Type of ; X ; could not be determined ; Type of ; BROADCAST ; could not be determined From this, we can tell that x corresponds to sum and that nowhere have we told about the type of sum . In fact, the type of sum : it could be a real or it could be a nu:array . And in the case it is of type nu:array , we must necessarily broadcast. Thus, we have the following new definition: (defpolymorph nu:mean ((array-like nu:array) &key out axes keep-dims) t (declare (optimize speed)) (if (null out) (locally (declare (type null out)) (let* ((sum (nu:sum array-like :out out :axes axes :keep-dims keep-dims)) (number-of-elements (etypecase axes (null (array-total-size array-like)) (integer (nu:shape array-like axes)) (list (loop :for d :in (narray-dimensions array-like) :for i :from 0 :with number-of-elements := 1 :if (member i axes) :do (setf number-of-elements (* number-of-elements d)) :finally (return number-of-elements)))))) (declare (type real number-of-elements)) (if (realp sum) (locally (declare (type real sum)) (nu:divide sum number-of-elements)) (locally (declare (type nu:array sum)) (nu:divide sum number-of-elements :out out :broadcast t))))) (locally (declare (type nu:array out)) (let* ((sum (nu:sum array-like :out out :axes axes :keep-dims keep-dims)) (number-of-elements (etypecase axes (null (array-total-size array-like)) (integer (nu:shape array-like axes)) (list (loop :for d :in (narray-dimensions array-like) :for i :from 0 :with number-of-elements := 1 :if (member i axes) :do (setf number-of-elements (* number-of-elements d)) :finally (return number-of-elements)))))) (declare (type real number-of-elements)) (if (realp sum) (locally (declare (type real sum)) (nu:divide sum number-of-elements)) (locally (declare (type nu:array sum)) (nu:divide sum number-of-elements :out out :broadcast t))))))) Compiling this gets us some other notes now: ; While compiling ; (TWO-ARG-FN/NON-COMPARISON 'NU:DIVIDE X Y :BROADCAST BROADCAST) ; in ; (LAMBDA (X Y &KEY (BROADCAST NU:*BROADCAST-AUTOMATICALLY*) (OUT NIL) &AUX) ; (DECLARE (TYPE NULL OUT) ; (TYPE (EQL T) BROADCAST) ; (TYPE REAL Y) ; (TYPE ARRAY X) ; (IGNORABLE)) ; (DECLARE (IGNORABLE OUT)) ; (BLOCK NU:DIVIDE ; (LOCALLY ; (TWO-ARG-FN/NON-COMPARISON 'NU:DIVIDE X Y :BROADCAST BROADCAST)))) ; generated from ; (NU:DIVIDE SUM NUMBER-OF-ELEMENTS :OUT OUT :BROADCAST T) ; Following notes were encountered: ; ; No applicable POLYMORPH discovered for polymorphic-function ; TWO-ARG-FN/NON-COMPARISON ; and ARGS: ; ; ('NU:DIVIDE X Y :BROADCAST BROADCAST) ; ; derived to be of TYPES: ; ; ((EQL NU:DIVIDE) ARRAY REAL (EQL :BROADCAST) (EQL T)) ; ; Available Effective-Type-Lists include: ; ; (SYMBOL NUMBER NUMBER &KEY (:BROADCAST T) (:OUT NULL)) ; (SYMBOL LIST LIST &KEY (:BROADCAST (OR NULL T)) (:OUT ARRAY)) ; (SYMBOL NUMBER LIST &KEY (:BROADCAST (OR NULL T)) (:OUT ARRAY)) ; (SYMBOL LIST NUMBER &KEY (:BROADCAST (OR NULL T)) (:OUT ARRAY)) ; (SYMBOL LIST LIST &KEY (:BROADCAST (OR NULL T)) (:OUT NULL)) ; (SYMBOL NUMBER LIST &KEY (:BROADCAST (OR NULL T)) (:OUT NULL)) ; (SYMBOL LIST NUMBER &KEY (:BROADCAST (OR NULL T)) (:OUT NULL)) ; (SYMBOL (SIMPLE-ARRAY <TYPE>) (SIMPLE-ARRAY <TYPE>) &KEY ; (:BROADCAST (OR NULL NULL)) (:OUT (SIMPLE-ARRAY <TYPE>))) ; (SYMBOL (SIMPLE-ARRAY <TYPE>) (SIMPLE-ARRAY <TYPE>) &KEY ; (:BROADCAST (OR NULL NULL)) (:OUT NULL)) ; (SYMBOL (ARRAY <TYPE>) (ARRAY <TYPE>) &KEY (:BROADCAST (OR NULL T)) ; (:OUT (ARRAY <TYPE>))) ; (SYMBOL (ARRAY <TYPE>) (ARRAY <TYPE>) &KEY (:BROADCAST (OR NULL T)) ; (:OUT NULL)) ; (SYMBOL (ARRAY <TYPE>) NUMBER &KEY (:BROADCAST (OR NULL (NOT NULL))) ; (:OUT (OR NULL (ARRAY <TYPE>)))) ; (SYMBOL NUMBER (ARRAY <TYPE>) &KEY (:BROADCAST (OR NULL (NOT NULL))) ; (:OUT (OR NULL (ARRAY <TYPE>)))) No applicable polymorph for divide ? How come? Certainly there exists a polymorph for divide that can take in a nu:array and a real as input, right? Careful observation tells us that even though such a polymorph exists (see the last two type lists), such a polymorph also has constraints on the type of out . In particular, the element-type of out must match the element type of x or y , or in our case, the sum . In fact, the element-type of sum if it happens to be an nu:array is also determined by the element-type of the input array-like . divide: We know that the second argument to divide will always be a real , and as such broadcasting cannot be avoided. However, divide accepts an out argument, and we would like to avoid runtime allocation of out . But, while supplying out we would want to check if or not sum is a real, because if sum is a real, then both sum and number-of-elements are real, and then we do not need to supply out . The following is our modified function then. Let's try to compile with (optimize speed) to see what notes are emitted. (defpolymorph nu:mean ((array-like nu:array) &key out axes keep-dims) t (let* ((sum (nu:sum array-like :out out :axes axes :keep-dims keep-dims)) (number-of-elements (etypecase axes (null (array-total-size array-like)) (integer (nu:shape array-like axes)) (list (loop :for d :in (narray-dimensions array-like) :for i :from 0 :with number-of-elements := 1 :if (member i axes) :do (setf number-of-elements (* number-of-elements d)) :finally (return number-of-elements)))))) (declare (type real number-of-elements)) (if (realp sum) (locally (declare (type real sum)) (nu:divide sum number-of-elements)) (locally (declare (type nu:array sum)) (nu:divide sum number-of-elements :out out :broadcast t))))) (defun mean-caller (x) (declare (optimize speed)) (nu:mean x)) The crucial note this emits is: Type of X could not be determined So, we specify the type of x : (defun mean-caller (x) (declare (optimize speed) (type (simple-array single-float) x)) (nu:mean x)) This gets us more information. (defpolymorph nu:mean ((array-like nu:array) &key out axes keep-dims) t (let* ((sum (nu:sum array-like :out out :axes axes :keep-dims keep-dims)) (number-of-elements (etypecase axes (null (array-total-size array-like)) (integer (nu:shape array-like axes)) (list (loop :for d :in (narray-dimensions array-like) :for i :from 0 :with number-of-elements := 1 :if (member i axes) :do (setf number-of-elements (* number-of-elements d)) :finally (return number-of-elements)))))) (declare (type real number-of-elements)) (nu:divide sum number-of-elements :out out)))","title":"Optimizing a specific case of nu:mean"},{"location":"#interfacing-with-magicl","text":"","title":"Interfacing with magicl"},{"location":"api/","text":"API Reference Configuration Package: numericals or dense-numericals numericals and dense-numericals come with a number of dynamically bound configuration variables that are put to use in non-inlined code. These include: *multithreaded-threshold* *default-float-format* *inline-with-multithreading* *array-element-type* *array-layout* *broadcast-automatically* Generating and transforming arrays Package: numericals or dense-numericals There are also a number of ways to generate arrays: from lists using asarray simple filled arrays using zeros ones rand full eye and their counterparts using zeros-like ones-like rand-like full-like You can also use other arrays to generate arrays using: copy transpose concat reshape Element-wise operators Package: numericals or dense-numericals Binary arithmetic operators: add subtract multiply divide Binary logical operators: two-arg-< two-arg-<= two-arg-= two-arg-/= two-arg-> two-arg->= Their n-ary counterparts: / * < <= = /= > >= Transcendental operators: sin asin sinh asinh cos acos cosh acosh tan atan tanh atanh exp expt log Rounding operators: abs ffloor floor fceiling ftruncate Miscellaneous: two-arg-max two-arg-min Array reduction operators Package: numericals or dense-numericals sum vdot maximum minimum Linear algebra operators Package: numericals.linalg or dense-numericals.linalg inv lu svd vdot cholesky solve eigvals qr pinv eigvecs rank norm2 det matmul","title":"API Reference"},{"location":"api/#api-reference","text":"","title":"API Reference"},{"location":"api/#configuration","text":"Package: numericals or dense-numericals numericals and dense-numericals come with a number of dynamically bound configuration variables that are put to use in non-inlined code. These include: *multithreaded-threshold* *default-float-format* *inline-with-multithreading* *array-element-type* *array-layout* *broadcast-automatically*","title":"Configuration"},{"location":"api/#generating-and-transforming-arrays","text":"Package: numericals or dense-numericals There are also a number of ways to generate arrays: from lists using asarray simple filled arrays using zeros ones rand full eye and their counterparts using zeros-like ones-like rand-like full-like You can also use other arrays to generate arrays using: copy transpose concat reshape","title":"Generating and transforming arrays"},{"location":"api/#element-wise-operators","text":"Package: numericals or dense-numericals Binary arithmetic operators: add subtract multiply divide Binary logical operators: two-arg-< two-arg-<= two-arg-= two-arg-/= two-arg-> two-arg->= Their n-ary counterparts: / * < <= = /= > >= Transcendental operators: sin asin sinh asinh cos acos cosh acosh tan atan tanh atanh exp expt log Rounding operators: abs ffloor floor fceiling ftruncate Miscellaneous: two-arg-max two-arg-min","title":"Element-wise operators"},{"location":"api/#array-reduction-operators","text":"Package: numericals or dense-numericals sum vdot maximum minimum","title":"Array reduction operators"},{"location":"api/#linear-algebra-operators","text":"Package: numericals.linalg or dense-numericals.linalg inv lu svd vdot cholesky solve eigvals qr pinv eigvecs rank norm2 det matmul","title":"Linear algebra operators"},{"location":"current-state/","text":"Current state of numericals | dense-numericals Included core There are two main asdf systems: numericals is designed to work with cl:array so that interfacing with the rest of the lisp ecosystem is trivial. dense-numericals is designed to work with dense-arrays:array Currently, (asdf:load-system \"numericals\") provides 3(+1) packages: numericals provides basic math functionality numericals.random provides array generators of random numbers sampled from various distributions numericals.linalg contains some common linear algebra operations Equivalent packages are provided by the (asdf:load-system \"dense-numericals\") asdf system: dense-numericals dense-numericals.random dense-numericals.linalg tests Tests for various functions are littered through all the files in the form of (5am:def-test ...) forms. Once the system is loaded, run (asdf:test-system \"numericals\") or (asdf:test-system \"dense-numericals\") to run the tests. dense-numericals and extensible-compound-types Using CL like specialized type declarations for dense-arrays:array requires the use of extensible-compound-types along with (cl:pushnew :extensible-compound-types cl:*features*) and a complete recompilation. See installation for more details. magicl The following two systems provide packages for using magicl functions: numericals/magicl , and dense-numericals/magicl Functions in the numericals.magicl and the dense-numericals.magicl package are essentially wrappers around magicl . Comparison with other libraries Native CL arrays As of this writing, The only libraries that offer broadcasted operations on arrays are this and numcl numcl does not yet have a focus on high performance - though, it should be possible to implement the current einsum based backend using BLAS and BMAS ; instead the focus there is on functionality; by contrast, the focus here is on performance first, and functionality second. Users do not have to choose. :mix option of uiop:define-package can be useful for mixing the two libraries as per user preferences Other minor differences wrt numcl include: Both (ones 2 3 :type 'single-float) and (ones '(2 3) :type 'single-float) are legal in numericals; while only the latter is legal in numcl/numpy numericals provides a *array-element-type-alist* equivalent to swank:*readtable-alist* to provide a package local way of specifying the default element-type for arrays. This can be further overriden by binding *array-element-type* . This does impose performance penalties however. numcl relies on JIT backed by specialized-function , while numericals relies on AOT backed by polymorphic-functions and cl-form-types . Again, these are not either-or, high level user functions can (in theory) utilize specialized-function, while the innards can use static-dispatch either by polymorphic-functions or static-dispatch or fast-generic-functions . In addition to these two, another performant library operating on CL arrays includes lla . Again, uiop:define-package with :mix can be used suitably. The author of numericals did not find other libraries operating on native CL arrays . Non-native CL arrays There are quite a few libraries in Common Lisp in this domain. I have only managed to take a peak at femlisp-matlisp . That said, the goal of numericals is not to replace python ecosystems, at least not in the short run, but instead to overcome the limitations of libraries like py4cl / 2 of sub-10,000 instructions per second. However, now there's also py4cl2-cffi which is about a 10 times faster than py4cl / 2 without using compiler macros yet. Foreign function interfaces gsll A number of functions in gsl and gsll operate on double precision floats, while some applications appreciate operators working on single precision floats without a conversion overhead. Thus, while extremely featureful, gsll is insufficient for everyone's needs. eigen eigen has wonderful documentation. I have only ever done a basic course in linear algebra, and often feel as if I am missing out on something when people work on linear algebra, but I found eigen's documentation to be superb! For instance, see this QuickRef page , or this Linear Algebra tutorial comparing different decompositions , or this linked page on benchmark comparison of various decompositions . So, if eigen and C++ suit your needs, you might as well use it! That said, it isn't the fastest in everything - SLEEF can often be faster in areas where it is specifically designed for. History Curiosity got the better of me one day, and I set out to explore the limits of numerical computing with Common Lisp. I mean - what does speed require? Just memory-locality and SIMD? SBCL has memory-locality. What about SIMD? Well, the functionality hasn't been \"standardized\" yet, and there are several attempts. Indeed, SBCL needs more documentation - think Emacs! But knowledge exists in people's heads. People are willing to share it. So, this was possible. PS: This library began as a reddit post , that, in turn, was triggered by this reddit post . You should probably use the latest SBCL (get from git) , at least SBCL-2.0.9. The build is fairly easy: sh make.sh && sh run-sbcl.sh # or install.sh or pick a binary from here . Project Predecessors sbcl-numericals numericals-2020.08 The project renaming reflects an attempt to separate the portable parts of the codebase from the SBCL-specific part, so that a portability attempt may be made in the future.","title":"Current State"},{"location":"current-state/#current-state-of-numericals-dense-numericals","text":"","title":"Current state of numericals | dense-numericals"},{"location":"current-state/#included","text":"","title":"Included"},{"location":"current-state/#core","text":"There are two main asdf systems: numericals is designed to work with cl:array so that interfacing with the rest of the lisp ecosystem is trivial. dense-numericals is designed to work with dense-arrays:array Currently, (asdf:load-system \"numericals\") provides 3(+1) packages: numericals provides basic math functionality numericals.random provides array generators of random numbers sampled from various distributions numericals.linalg contains some common linear algebra operations Equivalent packages are provided by the (asdf:load-system \"dense-numericals\") asdf system: dense-numericals dense-numericals.random dense-numericals.linalg","title":"core"},{"location":"current-state/#tests","text":"Tests for various functions are littered through all the files in the form of (5am:def-test ...) forms. Once the system is loaded, run (asdf:test-system \"numericals\") or (asdf:test-system \"dense-numericals\") to run the tests.","title":"tests"},{"location":"current-state/#dense-numericals-and-extensible-compound-types","text":"Using CL like specialized type declarations for dense-arrays:array requires the use of extensible-compound-types along with (cl:pushnew :extensible-compound-types cl:*features*) and a complete recompilation. See installation for more details.","title":"dense-numericals and extensible-compound-types"},{"location":"current-state/#magicl","text":"The following two systems provide packages for using magicl functions: numericals/magicl , and dense-numericals/magicl Functions in the numericals.magicl and the dense-numericals.magicl package are essentially wrappers around magicl .","title":"magicl"},{"location":"current-state/#comparison-with-other-libraries","text":"","title":"Comparison with other libraries"},{"location":"current-state/#native-cl-arrays","text":"As of this writing, The only libraries that offer broadcasted operations on arrays are this and numcl numcl does not yet have a focus on high performance - though, it should be possible to implement the current einsum based backend using BLAS and BMAS ; instead the focus there is on functionality; by contrast, the focus here is on performance first, and functionality second. Users do not have to choose. :mix option of uiop:define-package can be useful for mixing the two libraries as per user preferences Other minor differences wrt numcl include: Both (ones 2 3 :type 'single-float) and (ones '(2 3) :type 'single-float) are legal in numericals; while only the latter is legal in numcl/numpy numericals provides a *array-element-type-alist* equivalent to swank:*readtable-alist* to provide a package local way of specifying the default element-type for arrays. This can be further overriden by binding *array-element-type* . This does impose performance penalties however. numcl relies on JIT backed by specialized-function , while numericals relies on AOT backed by polymorphic-functions and cl-form-types . Again, these are not either-or, high level user functions can (in theory) utilize specialized-function, while the innards can use static-dispatch either by polymorphic-functions or static-dispatch or fast-generic-functions . In addition to these two, another performant library operating on CL arrays includes lla . Again, uiop:define-package with :mix can be used suitably. The author of numericals did not find other libraries operating on native CL arrays .","title":"Native CL arrays"},{"location":"current-state/#non-native-cl-arrays","text":"There are quite a few libraries in Common Lisp in this domain. I have only managed to take a peak at femlisp-matlisp . That said, the goal of numericals is not to replace python ecosystems, at least not in the short run, but instead to overcome the limitations of libraries like py4cl / 2 of sub-10,000 instructions per second. However, now there's also py4cl2-cffi which is about a 10 times faster than py4cl / 2 without using compiler macros yet.","title":"Non-native CL arrays"},{"location":"current-state/#foreign-function-interfaces","text":"","title":"Foreign function interfaces"},{"location":"current-state/#gsll","text":"A number of functions in gsl and gsll operate on double precision floats, while some applications appreciate operators working on single precision floats without a conversion overhead. Thus, while extremely featureful, gsll is insufficient for everyone's needs.","title":"gsll"},{"location":"current-state/#eigen","text":"eigen has wonderful documentation. I have only ever done a basic course in linear algebra, and often feel as if I am missing out on something when people work on linear algebra, but I found eigen's documentation to be superb! For instance, see this QuickRef page , or this Linear Algebra tutorial comparing different decompositions , or this linked page on benchmark comparison of various decompositions . So, if eigen and C++ suit your needs, you might as well use it! That said, it isn't the fastest in everything - SLEEF can often be faster in areas where it is specifically designed for.","title":"eigen"},{"location":"current-state/#history","text":"Curiosity got the better of me one day, and I set out to explore the limits of numerical computing with Common Lisp. I mean - what does speed require? Just memory-locality and SIMD? SBCL has memory-locality. What about SIMD? Well, the functionality hasn't been \"standardized\" yet, and there are several attempts. Indeed, SBCL needs more documentation - think Emacs! But knowledge exists in people's heads. People are willing to share it. So, this was possible. PS: This library began as a reddit post , that, in turn, was triggered by this reddit post . You should probably use the latest SBCL (get from git) , at least SBCL-2.0.9. The build is fairly easy: sh make.sh && sh run-sbcl.sh # or install.sh or pick a binary from here .","title":"History"},{"location":"current-state/#project-predecessors","text":"sbcl-numericals numericals-2020.08 The project renaming reflects an attempt to separate the portable parts of the codebase from the SBCL-specific part, so that a portability attempt may be made in the future.","title":"Project Predecessors"},{"location":"install/","text":"Installation Recall that two main asdf systems are provided: numericals , which works with cl:array , and dense-numericals , which works with dense-arrays Load either or both of them according to your needs. quicklisp is a defacto package manager for Common Lisp. All the below methods rely on the availability of the quicklisp client. The instructions to install it can be found here . Other useful resources to get started with quicklisp include: quicklisp - Common Lisp Libraries Installing quicklisp - The Common Lisp Cookbook Once the quicklisp client is installed, proceed with one of the below methods to install numericals or dense-numericals or both. Using quicklisp Simply quickload the required asdf system. (ql:quickload \"numericals\") ;;; OR (ql:quickload \"dense-numericals\") Depending on when you are reading this, the quicklisp versions could be a bit older than what you might wish for. If you want to use recent feature updates or bug fixes, you might want to use download-dependencies or ultralisp. The latest from github and download-dependencies First clone the github repository of numericals to where quicklisp can find. cd /path/to/quicklisp/local-projects/ # This is usually $HOME/quicklisp/local-projects git clone https://github.com/digikar99/numericals download-dependencies is a simple tool to download the dependencies of any given project. In order to use it, clone also the download-dependencies asdf system into your quicklisp local projects. git clone https://github.com/digikar99/download-dependencies Then create the directory where you want to download the dependencies for numericals or dense-numericals . mkdir -p /path/to/dependencies Finally, quickload and download the dependencies: (ql:quickload \"download-dependencies\") (let ((download-dependencies:*dependencies-home* #P\"/path/to/dependencies/\")) (download-dependencies:ensure-system \"numericals\")) Finally, you will need to instruct the quicklisp client to look into this path. After that, you can simply use quickload : (push #P\"/path/to/dependencies\" ql:*local-project-directories*) (ql:quickload \"numericals\") ;;; OR (ql:quickload \"dense-numericals\") Using extensible-compound-types If you are using numericals , you do not need to look into extensible-compound-types . TODO: What to do if you are using dense-numericals ? Using ultralisp Fetch from this dist of ultralisp . Once (ql:quickload \"numericals\") or (ql:quickload \"dense-numericals\") is successful; use inside your own package using :mix option of uiop:define-package (see above discussion), or package-local-nicknames . Using clpm TODO","title":"Installation"},{"location":"install/#installation","text":"Recall that two main asdf systems are provided: numericals , which works with cl:array , and dense-numericals , which works with dense-arrays Load either or both of them according to your needs. quicklisp is a defacto package manager for Common Lisp. All the below methods rely on the availability of the quicklisp client. The instructions to install it can be found here . Other useful resources to get started with quicklisp include: quicklisp - Common Lisp Libraries Installing quicklisp - The Common Lisp Cookbook Once the quicklisp client is installed, proceed with one of the below methods to install numericals or dense-numericals or both.","title":"Installation"},{"location":"install/#using-quicklisp","text":"Simply quickload the required asdf system. (ql:quickload \"numericals\") ;;; OR (ql:quickload \"dense-numericals\") Depending on when you are reading this, the quicklisp versions could be a bit older than what you might wish for. If you want to use recent feature updates or bug fixes, you might want to use download-dependencies or ultralisp.","title":"Using quicklisp"},{"location":"install/#the-latest-from-github-and-download-dependencies","text":"First clone the github repository of numericals to where quicklisp can find. cd /path/to/quicklisp/local-projects/ # This is usually $HOME/quicklisp/local-projects git clone https://github.com/digikar99/numericals download-dependencies is a simple tool to download the dependencies of any given project. In order to use it, clone also the download-dependencies asdf system into your quicklisp local projects. git clone https://github.com/digikar99/download-dependencies Then create the directory where you want to download the dependencies for numericals or dense-numericals . mkdir -p /path/to/dependencies Finally, quickload and download the dependencies: (ql:quickload \"download-dependencies\") (let ((download-dependencies:*dependencies-home* #P\"/path/to/dependencies/\")) (download-dependencies:ensure-system \"numericals\")) Finally, you will need to instruct the quicklisp client to look into this path. After that, you can simply use quickload : (push #P\"/path/to/dependencies\" ql:*local-project-directories*) (ql:quickload \"numericals\") ;;; OR (ql:quickload \"dense-numericals\")","title":"The latest from github and download-dependencies"},{"location":"install/#using-extensible-compound-types","text":"If you are using numericals , you do not need to look into extensible-compound-types . TODO: What to do if you are using dense-numericals ?","title":"Using extensible-compound-types"},{"location":"install/#using-ultralisp","text":"Fetch from this dist of ultralisp . Once (ql:quickload \"numericals\") or (ql:quickload \"dense-numericals\") is successful; use inside your own package using :mix option of uiop:define-package (see above discussion), or package-local-nicknames .","title":"Using ultralisp"},{"location":"install/#using-clpm","text":"TODO","title":"Using clpm"},{"location":"intro/","text":"numericals | dense-numericals: write fast code fast Github Watch Star Issue In case of any inaccuracies, ambiguities or suggestions, please create an issue here . numericals / dense-numericals brings to you the performance of proto-NumPy with the debugging and development abilities of Common Lisp. Even before that Why two names: numericals and dense-numericals? These are the names of the two main asdf systems provided as a part of this project. numericals is designed to work with cl:array so that interfacing with the rest of the lisp ecosystem is trivial. dense-numericals is designed to work with dense-arrays:array which provides a numpy-like array object for common lisp. dense-arrays itself is fairly extensible, and trivial extensions are provided for static-vectors as well as cl-cuda *. *Currently dense-numericals only works with cl:vector as the backend storage. Extension to static-vectors or cl-cuda will be provided in the future if at least one user expresses their needs (through an email or an issue ). And while, in theory, this choice can be provided through a configuration variable, but it will only add to the usage overhead. About common lisp Common Lisp is great for prototyping: REPL-based programming means that you can write your program one function at a time. Type, structure, function, and class redefinitions are handled more gracefully compared to most other languages. Global variables in common lisp have dynamic scope, while local variables have default lexical scope. This means you can use global variables locally :). See this example . Its condition system builds upon this and goes beyond traditional exception handling, allowing one to provide restarts for resuming a \"crashed\" program. But it is also great in the longer run: Optional static typing with implementations like SBCL means you do not have to worry yourself with type declarations during prototyping. But as your code and types stabilize, you can add the type declarations to aid documentation as well as speed and safety, while also inlining the small functions whose call overhead exceeds the work they do. A 1994 ANSI standard coupled with several portability libraries means that it is possible to write code that works without changes even after decades. Common Lisp also allows for the deployment of binaries, and there are tools to distribute the shared libraries that the binary depends upon. Indeed, there are limitations: If you want to write fast code, then redefinitions, dynamic scoping, and condition systems can get in the way. So, to some extent, it is a tradeoff that you can work around as your program stabilizes. The type system is much limited than ML like systems and does not provide true parametric types. But see this example . Coalton is an effort to work around this. It provides a ML like programming system built on top of Common Lisp. About numericals Like Julia, Common Lisp (SBCL) already offers a solution to the purported two-language problem. However, lisp code built using generic functions is not helpful for solving this. specialized-function and static-dispatch help here, but generic functions have other disadvantages: inability to dispatch on specialized array types inability to dispatch on optional or keyword arguments In addition, while Common Lisp's ANSI standard offers compiler macros that allows one to control what specialized form a particular call site compiles to (see this or this ), compiler macros are of limited use without access to the type information of the arguments. Unfortunately, this type information is not available trivially . To access it, one requires a somewhat sophisticated machinery of CLTL2 in the form of cl-environments , cl-form-types , and polymorphic-functions . In particular, the latter overcomes both the disadvantages of generic functions, and allows dispatching over specialized array types, as well as optional or keyword arguments. And coupled with the former two, it is also able to dispatch statically to a reasonable extent if the call site is compiled with (optimize speed) with safety<speed and debug<speed . Put together, this allows compiling the following code: (disassemble (lambda (x) (declare (optimize speed) (type (simple-array single-float 1) x)) (nu:sin! x))) into code containing no high level function calls but only: ... ; BFA: 4D8B55F0 MOV R10, [R13-16] ; thread.alien-linkage-table-base ; BFE: 41FF92082E0000 CALL [R10+11784] ; &BMAS_ssin ; C05: 488BE3 MOV RSP, RBX ... and the following slight variant (using double-float instead of single-float ) (disassemble (lambda (x) (declare (optimize speed) (type (simple-array double-float 1) x)) (nu:sin! x))) into the following: ... ; 672: 4D8B55F0 MOV R10, [R13-16] ; thread.alien-linkage-table-base ; 676: 41FF92C82E0000 CALL [R10+11976] ; &BMAS_dsin ; 67D: 488BE3 MOV RSP, RBX ... What this aggressive inlining means is that the performance is minimally impacted with increasing loop lengths. Below, notice that the overall number of sin calculated are the same in each of the three cases, although the loop lengths are different. IMPL> (let ((a (nu:rand 100000 :type 'single-float)) (b (nu:rand 100000 :type 'single-float))) (declare (optimize speed) (type (simple-array single-float) a b)) (time (loop repeat 1000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.296 seconds of real time 0.292903 seconds of total run time (0.292903 user, 0.000000 system) 98.99% CPU 646,713,220 processor cycles 32,512 bytes consed NIL IMPL> (let ((a (nu:rand 1000 :type 'single-float)) (b (nu:rand 1000 :type 'single-float))) (declare (optimize speed) (type (simple-array single-float) a b)) (time (loop repeat 100000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.288 seconds of real time 0.290469 seconds of total run time (0.290469 user, 0.000000 system) 100.69% CPU 641,386,456 processor cycles 3,186,176 bytes consed NIL IMPL> (let ((a (nu:rand 10 :type 'single-float)) (b (nu:rand 10 :type 'single-float))) (declare (optimize speed) (type (simple-array single-float 1) a b)) (time (loop repeat 10000000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 1.579 seconds of real time 1.580698 seconds of total run time (1.580698 user, 0.000000 system) [ Run times consist of 0.017 seconds GC time, and 1.564 seconds non-GC time. ] 100.13% CPU 3,487,968,606 processor cycles 320,015,504 bytes consed NIL Without inlining, the performance can be impacted severely. For instance, below, the lack of (optimize speed) declaration prevents inlining. Note that the below can be optimized further by avoiding type checks if a (safety 0) declaration is added. IMPL> (let ((a (nu:rand 100000 :type 'single-float)) (b (nu:rand 100000 :type 'single-float))) (declare (type (simple-array single-float) a b)) (time (loop repeat 1000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.251 seconds of real time 0.251034 seconds of total run time (0.251034 user, 0.000000 system) 100.00% CPU 554,269,126 processor cycles 97,792 bytes consed NIL IMPL> (let ((a (nu:rand 1000 :type 'single-float)) (b (nu:rand 1000 :type 'single-float))) (declare (type (simple-array single-float) a b)) (time (loop repeat 100000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.308 seconds of real time 0.308766 seconds of total run time (0.308766 user, 0.000000 system) [ Run times consist of 0.013 seconds GC time, and 0.296 seconds non-GC time. ] 100.32% CPU 679,509,546 processor cycles 9,583,360 bytes consed NIL IMPL> (let ((a (nu:rand 10 :type 'single-float)) (b (nu:rand 10 :type 'single-float))) (declare (type (simple-array single-float 1) a b)) (time (loop repeat 10000000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 5.343 seconds of real time 5.349964 seconds of total run time (5.346585 user, 0.003379 system) [ Run times consist of 0.049 seconds GC time, and 5.301 seconds non-GC time. ] 100.13% CPU 11,804,640,438 processor cycles 959,968,016 bytes consed NIL This may or may not matter for most use cases. But when it does matter, a naive lisp implementation using generic functions stops being as useful, and one is either forced to reimplement the lisp code to suit their needs or move to a different language altogether which handles this well. Thus, numericals and dense-numericals intend to provide a solution to this separation between \"write code fast\" and \"write fast code\". PS: The above BMAS_ssin foreign function is built over SLEEF and uses SIMD to compute the sine. The equivalent SBCL non-SIMD equivalent is about 30 times slower: IMPL> (let ((a (nu:rand 100000 :type 'single-float :max 1.0)) (b (nu:rand 100000 :type 'single-float))) (declare (type (simple-array single-float 1) a b) (optimize speed)) (time (loop repeat 1000 do (loop for i below 100000 do (setf (row-major-aref b i) (sin (row-major-aref a i))))))) Evaluation took: 2.495 seconds of real time 2.493383 seconds of total run time (2.493383 user, 0.000000 system) 99.92% CPU 5,505,542,370 processor cycles 0 bytes consed NIL Some overhead of the computation indeed stems from the conversion of single-float to double-float and back again; however, even if we stick to double-float, there is a difference of about 10x. But that's also the point, if an application wants performance over accuracy, they should not be forced to look beyond lisp. What numericals/dense-numericals do not intend to provide? numericals / dense-numericals do not intend to offer a one-stop solution to the numerical computation ecosystem in common lisp. Even planning to do so is delusionary and should be considered a severe case of NIH syndrome. To that extent, we widely adopt: BMAS backed by SLEEF , and more recently, Eigen backed by BLAS and LAPACK , as well as gsll . This is further coupled with multithreading using lparallel and the C-library builtin OpenMP . As further testament to the profoundly found elsewhere attitude adopted here, we have numericals working with cl:array that can be used in conjunction with other libraries two systems numericals/magicl and dense-numericals/magicl are provided that are essentially wrappers around magicl Thus, we intend to provide facilities for writing fast code fast, but at the same time, we want to promote the use of existing facilities wherever appropriate. Where next? Install it , Checkout the current state of numericals/dense-numericals , Or visit the API reference Old documentation page Acknowledgements Everyone who has contributed to SBCL. u/love5an and u/neil-lindquist for the required hand-holding and the gist . Paul Khuong for some blog posts . guicho271828 for SBCL Wiki as well as numcl . All the SLEEF contributors All the contributors of c2ffi and cl-autowrap u/moon-chilled's sassy comment (Is that the term?) It's possible that I could have forgotten to mention somebody - so... yeah... happy number crunching!","title":"Introduction"},{"location":"intro/#numericals-dense-numericals-write-fast-code-fast","text":"Github Watch Star Issue In case of any inaccuracies, ambiguities or suggestions, please create an issue here . numericals / dense-numericals brings to you the performance of proto-NumPy with the debugging and development abilities of Common Lisp. Even before that","title":"numericals | dense-numericals: write fast code fast"},{"location":"intro/#why-two-names-numericals-and-dense-numericals","text":"These are the names of the two main asdf systems provided as a part of this project. numericals is designed to work with cl:array so that interfacing with the rest of the lisp ecosystem is trivial. dense-numericals is designed to work with dense-arrays:array which provides a numpy-like array object for common lisp. dense-arrays itself is fairly extensible, and trivial extensions are provided for static-vectors as well as cl-cuda *. *Currently dense-numericals only works with cl:vector as the backend storage. Extension to static-vectors or cl-cuda will be provided in the future if at least one user expresses their needs (through an email or an issue ). And while, in theory, this choice can be provided through a configuration variable, but it will only add to the usage overhead.","title":"Why two names: numericals and dense-numericals?"},{"location":"intro/#about-common-lisp","text":"Common Lisp is great for prototyping: REPL-based programming means that you can write your program one function at a time. Type, structure, function, and class redefinitions are handled more gracefully compared to most other languages. Global variables in common lisp have dynamic scope, while local variables have default lexical scope. This means you can use global variables locally :). See this example . Its condition system builds upon this and goes beyond traditional exception handling, allowing one to provide restarts for resuming a \"crashed\" program. But it is also great in the longer run: Optional static typing with implementations like SBCL means you do not have to worry yourself with type declarations during prototyping. But as your code and types stabilize, you can add the type declarations to aid documentation as well as speed and safety, while also inlining the small functions whose call overhead exceeds the work they do. A 1994 ANSI standard coupled with several portability libraries means that it is possible to write code that works without changes even after decades. Common Lisp also allows for the deployment of binaries, and there are tools to distribute the shared libraries that the binary depends upon. Indeed, there are limitations: If you want to write fast code, then redefinitions, dynamic scoping, and condition systems can get in the way. So, to some extent, it is a tradeoff that you can work around as your program stabilizes. The type system is much limited than ML like systems and does not provide true parametric types. But see this example . Coalton is an effort to work around this. It provides a ML like programming system built on top of Common Lisp.","title":"About common lisp"},{"location":"intro/#about-numericals","text":"Like Julia, Common Lisp (SBCL) already offers a solution to the purported two-language problem. However, lisp code built using generic functions is not helpful for solving this. specialized-function and static-dispatch help here, but generic functions have other disadvantages: inability to dispatch on specialized array types inability to dispatch on optional or keyword arguments In addition, while Common Lisp's ANSI standard offers compiler macros that allows one to control what specialized form a particular call site compiles to (see this or this ), compiler macros are of limited use without access to the type information of the arguments. Unfortunately, this type information is not available trivially . To access it, one requires a somewhat sophisticated machinery of CLTL2 in the form of cl-environments , cl-form-types , and polymorphic-functions . In particular, the latter overcomes both the disadvantages of generic functions, and allows dispatching over specialized array types, as well as optional or keyword arguments. And coupled with the former two, it is also able to dispatch statically to a reasonable extent if the call site is compiled with (optimize speed) with safety<speed and debug<speed . Put together, this allows compiling the following code: (disassemble (lambda (x) (declare (optimize speed) (type (simple-array single-float 1) x)) (nu:sin! x))) into code containing no high level function calls but only: ... ; BFA: 4D8B55F0 MOV R10, [R13-16] ; thread.alien-linkage-table-base ; BFE: 41FF92082E0000 CALL [R10+11784] ; &BMAS_ssin ; C05: 488BE3 MOV RSP, RBX ... and the following slight variant (using double-float instead of single-float ) (disassemble (lambda (x) (declare (optimize speed) (type (simple-array double-float 1) x)) (nu:sin! x))) into the following: ... ; 672: 4D8B55F0 MOV R10, [R13-16] ; thread.alien-linkage-table-base ; 676: 41FF92C82E0000 CALL [R10+11976] ; &BMAS_dsin ; 67D: 488BE3 MOV RSP, RBX ... What this aggressive inlining means is that the performance is minimally impacted with increasing loop lengths. Below, notice that the overall number of sin calculated are the same in each of the three cases, although the loop lengths are different. IMPL> (let ((a (nu:rand 100000 :type 'single-float)) (b (nu:rand 100000 :type 'single-float))) (declare (optimize speed) (type (simple-array single-float) a b)) (time (loop repeat 1000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.296 seconds of real time 0.292903 seconds of total run time (0.292903 user, 0.000000 system) 98.99% CPU 646,713,220 processor cycles 32,512 bytes consed NIL IMPL> (let ((a (nu:rand 1000 :type 'single-float)) (b (nu:rand 1000 :type 'single-float))) (declare (optimize speed) (type (simple-array single-float) a b)) (time (loop repeat 100000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.288 seconds of real time 0.290469 seconds of total run time (0.290469 user, 0.000000 system) 100.69% CPU 641,386,456 processor cycles 3,186,176 bytes consed NIL IMPL> (let ((a (nu:rand 10 :type 'single-float)) (b (nu:rand 10 :type 'single-float))) (declare (optimize speed) (type (simple-array single-float 1) a b)) (time (loop repeat 10000000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 1.579 seconds of real time 1.580698 seconds of total run time (1.580698 user, 0.000000 system) [ Run times consist of 0.017 seconds GC time, and 1.564 seconds non-GC time. ] 100.13% CPU 3,487,968,606 processor cycles 320,015,504 bytes consed NIL Without inlining, the performance can be impacted severely. For instance, below, the lack of (optimize speed) declaration prevents inlining. Note that the below can be optimized further by avoiding type checks if a (safety 0) declaration is added. IMPL> (let ((a (nu:rand 100000 :type 'single-float)) (b (nu:rand 100000 :type 'single-float))) (declare (type (simple-array single-float) a b)) (time (loop repeat 1000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.251 seconds of real time 0.251034 seconds of total run time (0.251034 user, 0.000000 system) 100.00% CPU 554,269,126 processor cycles 97,792 bytes consed NIL IMPL> (let ((a (nu:rand 1000 :type 'single-float)) (b (nu:rand 1000 :type 'single-float))) (declare (type (simple-array single-float) a b)) (time (loop repeat 100000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.308 seconds of real time 0.308766 seconds of total run time (0.308766 user, 0.000000 system) [ Run times consist of 0.013 seconds GC time, and 0.296 seconds non-GC time. ] 100.32% CPU 679,509,546 processor cycles 9,583,360 bytes consed NIL IMPL> (let ((a (nu:rand 10 :type 'single-float)) (b (nu:rand 10 :type 'single-float))) (declare (type (simple-array single-float 1) a b)) (time (loop repeat 10000000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 5.343 seconds of real time 5.349964 seconds of total run time (5.346585 user, 0.003379 system) [ Run times consist of 0.049 seconds GC time, and 5.301 seconds non-GC time. ] 100.13% CPU 11,804,640,438 processor cycles 959,968,016 bytes consed NIL This may or may not matter for most use cases. But when it does matter, a naive lisp implementation using generic functions stops being as useful, and one is either forced to reimplement the lisp code to suit their needs or move to a different language altogether which handles this well. Thus, numericals and dense-numericals intend to provide a solution to this separation between \"write code fast\" and \"write fast code\". PS: The above BMAS_ssin foreign function is built over SLEEF and uses SIMD to compute the sine. The equivalent SBCL non-SIMD equivalent is about 30 times slower: IMPL> (let ((a (nu:rand 100000 :type 'single-float :max 1.0)) (b (nu:rand 100000 :type 'single-float))) (declare (type (simple-array single-float 1) a b) (optimize speed)) (time (loop repeat 1000 do (loop for i below 100000 do (setf (row-major-aref b i) (sin (row-major-aref a i))))))) Evaluation took: 2.495 seconds of real time 2.493383 seconds of total run time (2.493383 user, 0.000000 system) 99.92% CPU 5,505,542,370 processor cycles 0 bytes consed NIL Some overhead of the computation indeed stems from the conversion of single-float to double-float and back again; however, even if we stick to double-float, there is a difference of about 10x. But that's also the point, if an application wants performance over accuracy, they should not be forced to look beyond lisp.","title":"About numericals"},{"location":"intro/#what-numericalsdense-numericals-do-not-intend-to-provide","text":"numericals / dense-numericals do not intend to offer a one-stop solution to the numerical computation ecosystem in common lisp. Even planning to do so is delusionary and should be considered a severe case of NIH syndrome. To that extent, we widely adopt: BMAS backed by SLEEF , and more recently, Eigen backed by BLAS and LAPACK , as well as gsll . This is further coupled with multithreading using lparallel and the C-library builtin OpenMP . As further testament to the profoundly found elsewhere attitude adopted here, we have numericals working with cl:array that can be used in conjunction with other libraries two systems numericals/magicl and dense-numericals/magicl are provided that are essentially wrappers around magicl Thus, we intend to provide facilities for writing fast code fast, but at the same time, we want to promote the use of existing facilities wherever appropriate.","title":"What numericals/dense-numericals do not intend to provide?"},{"location":"intro/#where-next","text":"Install it , Checkout the current state of numericals/dense-numericals , Or visit the API reference Old documentation page","title":"Where next?"},{"location":"intro/#acknowledgements","text":"Everyone who has contributed to SBCL. u/love5an and u/neil-lindquist for the required hand-holding and the gist . Paul Khuong for some blog posts . guicho271828 for SBCL Wiki as well as numcl . All the SLEEF contributors All the contributors of c2ffi and cl-autowrap u/moon-chilled's sassy comment (Is that the term?) It's possible that I could have forgotten to mention somebody - so... yeah... happy number crunching!","title":"Acknowledgements"},{"location":"reasonably-fast-clhs-arrays/","text":"Reasonably fast operations for CLHS provided arrays (ql:quickload \"numericals\") ;;; See here for instructions on installing quicklisp: https://quicklisp.org/ Since numericals is /faaar/ from completely, and covers only a very small set of operations involved in numerical computing, wherever appropriate, sister libraries will be mentioned. These include: alexandria : defacto utility library array-operations : minimal dependencies numcl Python's numpy through py4cl2 : not suited for performance, thus the main reason for this project numpy-file-format Also, see package local nicknames if required. Table of Contents Reasonably fast operations for Common Lisp arrays Constructing arrays Controlling how arrays are printed *print-array* *print-length* *print-right-margin* *print-level* *print-lines* Controlling the default array-element-type Reading and writing arrays to disk Constructing arrays Several functions for constructing arrays (out of thin air) include: numericals: zeros, ones, rand, zeros-like, ones-like, rand-like full, full-like, eye array-operations: zeros, zeros*, ones, ones*, rand, randn*, linspace, generate numcl: zeros, ones, arange, linspace, eye cl:make-array Recommended functions for - converting lists to arrays: numericals:asarray - copying arrays: numericals:copy Controlling how arrays are printed Common Lisp provides a number of parameters for this purpose: *print-array* CL-USER> (let ((*print-array* nil)) (princ (numericals:zeros 10)) nil) #<(SIMPLE-VECTOR 10) {103C7B8C0F}> NIL CL-USER> (let ((*print-array* t)) (princ (numericals:zeros 10)) nil) #(0 0 0 0 0 0 0 0 0 0) NIL *print-length* CL-USER> (let ((*print-length* 10)) (princ (numericals:zeros 100)) nil) #(0 0 0 0 0 0 0 0 0 0 ...) NIL CL-USER> (let ((*print-length* nil)) (princ (numericals:zeros 100)) nil) #(0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0) NIL *print-right-margin* CL-USER> (let ((*print-right-margin* 100)) (princ (numericals:zeros 100)) nil) #(0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0) NIL *print-level* The number of levels of an object that should be printed. *print-lines* The number of lines of the object that should be printed. Controlling the default array-element-type There are several variables in the numericals package that allow specifying the default element type of the output in one place, instead of having to specify it for each function individually. These include: *array-element-type* *default-float-format* CL-USER> (let ((numericals:*array-element-type* '(signed-byte 32))) (describe (numericals:add '(1 2 3) '(4 5 6)))) #(5 7 9) [simple specialized vector] Element-type: (SIGNED-BYTE 32) Length: 3 ; No value CL-USER> (let ((numericals:*array-element-type* 'double-float)) (describe (numericals:add '(1 2 3) '(4 5 6)))) #(5.0d0 7.0d0 9.0d0) [simple specialized vector] Element-type: DOUBLE-FLOAT Length: 3 ; No value Certain functions like sin, cos, tan, exp (and several others) are sensible mainly for floating point numbers. For these functions, the default element type of the output is controlled either through the input, or through *default-float-format*, while for other functions like zeros, ones, rand, add, subtract, divide, multiply, the default element type of the output is controlled either by the input, or by the value of *array-element-type*. CL-USER> (let ((numericals:*default-float-format* 'single-float)) (describe (numericals:sin '(1 2 3)))) #(0.84147096 0.9092974 0.14112) [simple specialized vector] Element-type: SINGLE-FLOAT Length: 3 ; No value CL-USER> (let ((numericals:*default-float-format* 'double-float)) (describe (numericals:sin '(1 2 3)))) #(0.8414709848078965d0 0.9092974268256817d0 0.1411200080598672d0) [simple specialized vector] Element-type: DOUBLE-FLOAT Length: 3 ; No value There also exists *array-element-type-alist* that allows specifying package local default element types if the need ever arises. All of these can be overridden for the particular function by explicitly specifying the out or type arguments wherever appropriate. Reading and writing arrays to disk As human-readable text files (slowest method currently): numpy: save, load As readable lisp objects: cl: read, write As binary objects useable by python: numpy-file-format: load-array, store-array","title":"Reasonably fast operations for CLHS provided arrays"},{"location":"reasonably-fast-clhs-arrays/#reasonably-fast-operations-for-clhs-provided-arrays","text":"(ql:quickload \"numericals\") ;;; See here for instructions on installing quicklisp: https://quicklisp.org/ Since numericals is /faaar/ from completely, and covers only a very small set of operations involved in numerical computing, wherever appropriate, sister libraries will be mentioned. These include: alexandria : defacto utility library array-operations : minimal dependencies numcl Python's numpy through py4cl2 : not suited for performance, thus the main reason for this project numpy-file-format Also, see package local nicknames if required. Table of Contents Reasonably fast operations for Common Lisp arrays Constructing arrays Controlling how arrays are printed *print-array* *print-length* *print-right-margin* *print-level* *print-lines* Controlling the default array-element-type Reading and writing arrays to disk","title":"Reasonably fast operations for CLHS provided arrays"},{"location":"reasonably-fast-clhs-arrays/#constructing-arrays","text":"Several functions for constructing arrays (out of thin air) include: numericals: zeros, ones, rand, zeros-like, ones-like, rand-like full, full-like, eye array-operations: zeros, zeros*, ones, ones*, rand, randn*, linspace, generate numcl: zeros, ones, arange, linspace, eye cl:make-array Recommended functions for - converting lists to arrays: numericals:asarray - copying arrays: numericals:copy","title":"Constructing arrays"},{"location":"reasonably-fast-clhs-arrays/#controlling-how-arrays-are-printed","text":"Common Lisp provides a number of parameters for this purpose:","title":"Controlling how arrays are printed"},{"location":"reasonably-fast-clhs-arrays/#print-array","text":"CL-USER> (let ((*print-array* nil)) (princ (numericals:zeros 10)) nil) #<(SIMPLE-VECTOR 10) {103C7B8C0F}> NIL CL-USER> (let ((*print-array* t)) (princ (numericals:zeros 10)) nil) #(0 0 0 0 0 0 0 0 0 0) NIL","title":"*print-array*"},{"location":"reasonably-fast-clhs-arrays/#print-length","text":"CL-USER> (let ((*print-length* 10)) (princ (numericals:zeros 100)) nil) #(0 0 0 0 0 0 0 0 0 0 ...) NIL CL-USER> (let ((*print-length* nil)) (princ (numericals:zeros 100)) nil) #(0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0) NIL","title":"*print-length*"},{"location":"reasonably-fast-clhs-arrays/#print-right-margin","text":"CL-USER> (let ((*print-right-margin* 100)) (princ (numericals:zeros 100)) nil) #(0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0) NIL","title":"*print-right-margin*"},{"location":"reasonably-fast-clhs-arrays/#print-level","text":"The number of levels of an object that should be printed.","title":"*print-level*"},{"location":"reasonably-fast-clhs-arrays/#print-lines","text":"The number of lines of the object that should be printed.","title":"*print-lines*"},{"location":"reasonably-fast-clhs-arrays/#controlling-the-default-array-element-type","text":"There are several variables in the numericals package that allow specifying the default element type of the output in one place, instead of having to specify it for each function individually. These include: *array-element-type* *default-float-format* CL-USER> (let ((numericals:*array-element-type* '(signed-byte 32))) (describe (numericals:add '(1 2 3) '(4 5 6)))) #(5 7 9) [simple specialized vector] Element-type: (SIGNED-BYTE 32) Length: 3 ; No value CL-USER> (let ((numericals:*array-element-type* 'double-float)) (describe (numericals:add '(1 2 3) '(4 5 6)))) #(5.0d0 7.0d0 9.0d0) [simple specialized vector] Element-type: DOUBLE-FLOAT Length: 3 ; No value Certain functions like sin, cos, tan, exp (and several others) are sensible mainly for floating point numbers. For these functions, the default element type of the output is controlled either through the input, or through *default-float-format*, while for other functions like zeros, ones, rand, add, subtract, divide, multiply, the default element type of the output is controlled either by the input, or by the value of *array-element-type*. CL-USER> (let ((numericals:*default-float-format* 'single-float)) (describe (numericals:sin '(1 2 3)))) #(0.84147096 0.9092974 0.14112) [simple specialized vector] Element-type: SINGLE-FLOAT Length: 3 ; No value CL-USER> (let ((numericals:*default-float-format* 'double-float)) (describe (numericals:sin '(1 2 3)))) #(0.8414709848078965d0 0.9092974268256817d0 0.1411200080598672d0) [simple specialized vector] Element-type: DOUBLE-FLOAT Length: 3 ; No value There also exists *array-element-type-alist* that allows specifying package local default element types if the need ever arises. All of these can be overridden for the particular function by explicitly specifying the out or type arguments wherever appropriate.","title":"Controlling the default array-element-type"},{"location":"reasonably-fast-clhs-arrays/#reading-and-writing-arrays-to-disk","text":"As human-readable text files (slowest method currently): numpy: save, load As readable lisp objects: cl: read, write As binary objects useable by python: numpy-file-format: load-array, store-array","title":"Reading and writing arrays to disk"},{"location":"trash/","text":"Unfortunately, quality doesn't dictate popularity , but popularity would dictate the number of libraries, right? Numerical or scientific computing is one such domain where popularity of the ecosystem seems to be a big factor driving the tool availability. While Common Lisp does have mgl-mat magicl and perhaps a few others, their functionality is still dwarfed by the more popular languages. So, without using projects like cffi , cl-autowrap , py4cl , py4cl2-cffi , it can be hard if not impossible to meet one's needs while staying in Common Lisp. numericals or dense-numericals does not intend offer","title":"Trash"}]}