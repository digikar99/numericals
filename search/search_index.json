{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"numericals | dense-numericals: write fast code fast Github Watch Star Issue In case of any inaccuracies, ambiguities or suggestions, please create an issue here . numericals / dense-numericals brings to you the performance of proto-NumPy with the debugging and development abilities of Common Lisp. Even before that Why two names: numericals and dense-numericals? These are the names of the two main asdf systems provided as a part of this project. numericals is designed to work with cl:array so that interfacing with the rest of the lisp ecosystem is trivial. dense-numericals is designed to work with dense-arrays:array which provides a numpy-like array object for common lisp. dense-arrays itself is fairly extensible, and trivial extensions are provided for static-vectors as well as cl-cuda *. *Currently dense-numericals only works with cl:vector as the backend storage. Extension to static-vectors or cl-cuda will be provided in the future if at least one user expresses their needs (through an email or an issue ). And while, in theory, this choice can be provided through a configuration variable, but it will only add to the usage overhead. About common lisp Common Lisp is great for prototyping: REPL-based programming means that you can write your program one function at a time. Type, structure, function, and class redefinitions are handled more gracefully compared to most other languages. Global variables in common lisp have dynamic scope, while local variables have default lexical scope. This means you can use global variables locally :). See this example . Its condition system builds upon this and goes beyond traditional exception handling, allowing one to provide restarts for resuming a \"crashed\" program. But it is also great in the longer run: Optional static typing with implementations like SBCL means you do not have to worry yourself with type declarations during prototyping. But as your code and types stabilize, you can add the type declarations to aid documentation as well as speed and safety, while also inlining the small functions whose call overhead exceeds the work they do. A 1994 ANSI standard coupled with several portability libraries means that it is possible to write code that works without changes even after decades. Common Lisp also allows for the deployment of binaries, and there are tools to distribute the shared libraries that the binary depends upon. Indeed, there are limitations: If you want to write fast code, then redefinitions, dynamic scoping, and condition systems can get in the way. So, to some extent, it is a tradeoff that you can work around as your program stabilizes. The type system is much limited than ML like systems and does not provide true parametric types. But see this example . Coalton is an effort to work around this. It provides a ML like programming system built on top of Common Lisp. About numericals Like Julia, Common Lisp (SBCL) already offers a solution to the purported two-language problem. However, lisp code built using generic functions is not helpful for solving this. specialized-function and static-dispatch help here, but generic functions have other disadvantages: inability to dispatch on specialized array types inability to dispatch on optional or keyword arguments In addition, while Common Lisp's ANSI standard offers compiler macros that allows one to control what specialized form a particular call site compiles to (see this or this ), compiler macros are of limited use without access to the type information of the arguments. Unfortunately, this type information is not available trivially . To access it, one requires a somewhat sophisticated machinery of CLTL2 in the form of cl-environments , cl-form-types , and polymorphic-functions . In particular, the latter overcomes both the disadvantages of generic functions, and allows dispatching over specialized array types, as well as optional or keyword arguments. And coupled with the former two, it is also able to dispatch statically to a reasonable extent if the call site is compiled with (optimize speed) with safety<speed and debug<speed . Put together, this allows compiling the following code: (disassemble (lambda (x) (declare (optimize speed) (type (simple-array single-float 1) x)) (nu:sin! x))) into code containing no high level function calls but only: ... ; BFA: 4D8B55F0 MOV R10, [R13-16] ; thread.alien-linkage-table-base ; BFE: 41FF92082E0000 CALL [R10+11784] ; &BMAS_ssin ; C05: 488BE3 MOV RSP, RBX ... and the following slight variant (using double-float instead of single-float ) (disassemble (lambda (x) (declare (optimize speed) (type (simple-array double-float 1) x)) (nu:sin! x))) into the following: ... ; 672: 4D8B55F0 MOV R10, [R13-16] ; thread.alien-linkage-table-base ; 676: 41FF92C82E0000 CALL [R10+11976] ; &BMAS_dsin ; 67D: 488BE3 MOV RSP, RBX ... What this aggressive inlining means is that the performance is minimally impacted with increasing loop lengths. Below, notice that the overall number of sin calculated are the same in each of the three cases, although the loop lengths are different. IMPL> (let ((a (nu:rand 100000 :type 'single-float)) (b (nu:rand 100000 :type 'single-float))) (declare (optimize speed) (type (simple-array single-float) a b)) (time (loop repeat 1000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.296 seconds of real time 0.292903 seconds of total run time (0.292903 user, 0.000000 system) 98.99% CPU 646,713,220 processor cycles 32,512 bytes consed NIL IMPL> (let ((a (nu:rand 1000 :type 'single-float)) (b (nu:rand 1000 :type 'single-float))) (declare (optimize speed) (type (simple-array single-float) a b)) (time (loop repeat 100000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.288 seconds of real time 0.290469 seconds of total run time (0.290469 user, 0.000000 system) 100.69% CPU 641,386,456 processor cycles 3,186,176 bytes consed NIL IMPL> (let ((a (nu:rand 10 :type 'single-float)) (b (nu:rand 10 :type 'single-float))) (declare (optimize speed) (type (simple-array single-float 1) a b)) (time (loop repeat 10000000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 1.579 seconds of real time 1.580698 seconds of total run time (1.580698 user, 0.000000 system) [ Run times consist of 0.017 seconds GC time, and 1.564 seconds non-GC time. ] 100.13% CPU 3,487,968,606 processor cycles 320,015,504 bytes consed NIL Without inlining, the performance can be impacted severely. For instance, below, the lack of (optimize speed) declaration prevents inlining. Note that the below can be optimized further by avoiding type checks if a (safety 0) declaration is added. IMPL> (let ((a (nu:rand 100000 :type 'single-float)) (b (nu:rand 100000 :type 'single-float))) (declare (type (simple-array single-float) a b)) (time (loop repeat 1000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.251 seconds of real time 0.251034 seconds of total run time (0.251034 user, 0.000000 system) 100.00% CPU 554,269,126 processor cycles 97,792 bytes consed NIL IMPL> (let ((a (nu:rand 1000 :type 'single-float)) (b (nu:rand 1000 :type 'single-float))) (declare (type (simple-array single-float) a b)) (time (loop repeat 100000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.308 seconds of real time 0.308766 seconds of total run time (0.308766 user, 0.000000 system) [ Run times consist of 0.013 seconds GC time, and 0.296 seconds non-GC time. ] 100.32% CPU 679,509,546 processor cycles 9,583,360 bytes consed NIL IMPL> (let ((a (nu:rand 10 :type 'single-float)) (b (nu:rand 10 :type 'single-float))) (declare (type (simple-array single-float 1) a b)) (time (loop repeat 10000000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 5.343 seconds of real time 5.349964 seconds of total run time (5.346585 user, 0.003379 system) [ Run times consist of 0.049 seconds GC time, and 5.301 seconds non-GC time. ] 100.13% CPU 11,804,640,438 processor cycles 959,968,016 bytes consed NIL This may or may not matter for most use cases. But when it does matter, a naive lisp implementation using generic functions stops being as useful, and one is either forced to reimplement the lisp code to suit their needs or move to a different language altogether which handles this well. Thus, numericals and dense-numericals intend to provide a solution to this separation between \"write code fast\" and \"write fast code\". PS: The above BMAS_ssin foreign function is built over SLEEF and uses SIMD to compute the sine. The equivalent SBCL non-SIMD equivalent is about 30 times slower: IMPL> (let ((a (nu:rand 100000 :type 'single-float :max 1.0)) (b (nu:rand 100000 :type 'single-float))) (declare (type (simple-array single-float 1) a b) (optimize speed)) (time (loop repeat 1000 do (loop for i below 100000 do (setf (row-major-aref b i) (sin (row-major-aref a i))))))) Evaluation took: 2.495 seconds of real time 2.493383 seconds of total run time (2.493383 user, 0.000000 system) 99.92% CPU 5,505,542,370 processor cycles 0 bytes consed NIL Some overhead of the computation indeed stems from the conversion of single-float to double-float and back again; however, even if we stick to double-float, there is a difference of about 10x. But that's also the point, if an application wants performance over accuracy, they should not be forced to look beyond lisp. What numericals/dense-numericals do not intend to provide? numericals / dense-numericals do not intend to offer a one-stop solution to the numerical computation ecosystem in common lisp. Even planning to do so is delusionary and should be considered a severe case of NIH syndrome. To that extent, we widely adopt: BMAS backed by SLEEF , and more recently, Eigen backed by BLAS and LAPACK , as well as gsll . This is further coupled with multithreading using lparallel and the C-library builtin OpenMP . As further testament to the profoundly found elsewhere attitude adopted here, we have numericals working with cl:array that can be used in conjunction with other libraries two systems numericals/magicl and dense-numericals/magicl are provided that are essentially wrappers around magicl Thus, we intend to provide facilities for writing fast code fast, but at the same time, we want to promote the use of existing facilities wherever appropriate. Where next? Install it , Checkout the current state of numericals/dense-numericals , Or visit the API reference Old documentation page Acknowledgements Everyone who has contributed to SBCL. u/love5an and u/neil-lindquist for the required hand-holding and the gist . Paul Khuong for some blog posts . guicho271828 for SBCL Wiki as well as numcl . All the SLEEF contributors All the contributors of c2ffi and cl-autowrap u/moon-chilled's sassy comment (Is that the term?) It's possible that I could have forgotten to mention somebody - so... yeah... happy number crunching!","title":"Introduction"},{"location":"#numericals-dense-numericals-write-fast-code-fast","text":"Github Watch Star Issue In case of any inaccuracies, ambiguities or suggestions, please create an issue here . numericals / dense-numericals brings to you the performance of proto-NumPy with the debugging and development abilities of Common Lisp. Even before that","title":"numericals | dense-numericals: write fast code fast"},{"location":"#why-two-names-numericals-and-dense-numericals","text":"These are the names of the two main asdf systems provided as a part of this project. numericals is designed to work with cl:array so that interfacing with the rest of the lisp ecosystem is trivial. dense-numericals is designed to work with dense-arrays:array which provides a numpy-like array object for common lisp. dense-arrays itself is fairly extensible, and trivial extensions are provided for static-vectors as well as cl-cuda *. *Currently dense-numericals only works with cl:vector as the backend storage. Extension to static-vectors or cl-cuda will be provided in the future if at least one user expresses their needs (through an email or an issue ). And while, in theory, this choice can be provided through a configuration variable, but it will only add to the usage overhead.","title":"Why two names: numericals and dense-numericals?"},{"location":"#about-common-lisp","text":"Common Lisp is great for prototyping: REPL-based programming means that you can write your program one function at a time. Type, structure, function, and class redefinitions are handled more gracefully compared to most other languages. Global variables in common lisp have dynamic scope, while local variables have default lexical scope. This means you can use global variables locally :). See this example . Its condition system builds upon this and goes beyond traditional exception handling, allowing one to provide restarts for resuming a \"crashed\" program. But it is also great in the longer run: Optional static typing with implementations like SBCL means you do not have to worry yourself with type declarations during prototyping. But as your code and types stabilize, you can add the type declarations to aid documentation as well as speed and safety, while also inlining the small functions whose call overhead exceeds the work they do. A 1994 ANSI standard coupled with several portability libraries means that it is possible to write code that works without changes even after decades. Common Lisp also allows for the deployment of binaries, and there are tools to distribute the shared libraries that the binary depends upon. Indeed, there are limitations: If you want to write fast code, then redefinitions, dynamic scoping, and condition systems can get in the way. So, to some extent, it is a tradeoff that you can work around as your program stabilizes. The type system is much limited than ML like systems and does not provide true parametric types. But see this example . Coalton is an effort to work around this. It provides a ML like programming system built on top of Common Lisp.","title":"About common lisp"},{"location":"#about-numericals","text":"Like Julia, Common Lisp (SBCL) already offers a solution to the purported two-language problem. However, lisp code built using generic functions is not helpful for solving this. specialized-function and static-dispatch help here, but generic functions have other disadvantages: inability to dispatch on specialized array types inability to dispatch on optional or keyword arguments In addition, while Common Lisp's ANSI standard offers compiler macros that allows one to control what specialized form a particular call site compiles to (see this or this ), compiler macros are of limited use without access to the type information of the arguments. Unfortunately, this type information is not available trivially . To access it, one requires a somewhat sophisticated machinery of CLTL2 in the form of cl-environments , cl-form-types , and polymorphic-functions . In particular, the latter overcomes both the disadvantages of generic functions, and allows dispatching over specialized array types, as well as optional or keyword arguments. And coupled with the former two, it is also able to dispatch statically to a reasonable extent if the call site is compiled with (optimize speed) with safety<speed and debug<speed . Put together, this allows compiling the following code: (disassemble (lambda (x) (declare (optimize speed) (type (simple-array single-float 1) x)) (nu:sin! x))) into code containing no high level function calls but only: ... ; BFA: 4D8B55F0 MOV R10, [R13-16] ; thread.alien-linkage-table-base ; BFE: 41FF92082E0000 CALL [R10+11784] ; &BMAS_ssin ; C05: 488BE3 MOV RSP, RBX ... and the following slight variant (using double-float instead of single-float ) (disassemble (lambda (x) (declare (optimize speed) (type (simple-array double-float 1) x)) (nu:sin! x))) into the following: ... ; 672: 4D8B55F0 MOV R10, [R13-16] ; thread.alien-linkage-table-base ; 676: 41FF92C82E0000 CALL [R10+11976] ; &BMAS_dsin ; 67D: 488BE3 MOV RSP, RBX ... What this aggressive inlining means is that the performance is minimally impacted with increasing loop lengths. Below, notice that the overall number of sin calculated are the same in each of the three cases, although the loop lengths are different. IMPL> (let ((a (nu:rand 100000 :type 'single-float)) (b (nu:rand 100000 :type 'single-float))) (declare (optimize speed) (type (simple-array single-float) a b)) (time (loop repeat 1000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.296 seconds of real time 0.292903 seconds of total run time (0.292903 user, 0.000000 system) 98.99% CPU 646,713,220 processor cycles 32,512 bytes consed NIL IMPL> (let ((a (nu:rand 1000 :type 'single-float)) (b (nu:rand 1000 :type 'single-float))) (declare (optimize speed) (type (simple-array single-float) a b)) (time (loop repeat 100000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.288 seconds of real time 0.290469 seconds of total run time (0.290469 user, 0.000000 system) 100.69% CPU 641,386,456 processor cycles 3,186,176 bytes consed NIL IMPL> (let ((a (nu:rand 10 :type 'single-float)) (b (nu:rand 10 :type 'single-float))) (declare (optimize speed) (type (simple-array single-float 1) a b)) (time (loop repeat 10000000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 1.579 seconds of real time 1.580698 seconds of total run time (1.580698 user, 0.000000 system) [ Run times consist of 0.017 seconds GC time, and 1.564 seconds non-GC time. ] 100.13% CPU 3,487,968,606 processor cycles 320,015,504 bytes consed NIL Without inlining, the performance can be impacted severely. For instance, below, the lack of (optimize speed) declaration prevents inlining. Note that the below can be optimized further by avoiding type checks if a (safety 0) declaration is added. IMPL> (let ((a (nu:rand 100000 :type 'single-float)) (b (nu:rand 100000 :type 'single-float))) (declare (type (simple-array single-float) a b)) (time (loop repeat 1000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.251 seconds of real time 0.251034 seconds of total run time (0.251034 user, 0.000000 system) 100.00% CPU 554,269,126 processor cycles 97,792 bytes consed NIL IMPL> (let ((a (nu:rand 1000 :type 'single-float)) (b (nu:rand 1000 :type 'single-float))) (declare (type (simple-array single-float) a b)) (time (loop repeat 100000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.308 seconds of real time 0.308766 seconds of total run time (0.308766 user, 0.000000 system) [ Run times consist of 0.013 seconds GC time, and 0.296 seconds non-GC time. ] 100.32% CPU 679,509,546 processor cycles 9,583,360 bytes consed NIL IMPL> (let ((a (nu:rand 10 :type 'single-float)) (b (nu:rand 10 :type 'single-float))) (declare (type (simple-array single-float 1) a b)) (time (loop repeat 10000000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 5.343 seconds of real time 5.349964 seconds of total run time (5.346585 user, 0.003379 system) [ Run times consist of 0.049 seconds GC time, and 5.301 seconds non-GC time. ] 100.13% CPU 11,804,640,438 processor cycles 959,968,016 bytes consed NIL This may or may not matter for most use cases. But when it does matter, a naive lisp implementation using generic functions stops being as useful, and one is either forced to reimplement the lisp code to suit their needs or move to a different language altogether which handles this well. Thus, numericals and dense-numericals intend to provide a solution to this separation between \"write code fast\" and \"write fast code\". PS: The above BMAS_ssin foreign function is built over SLEEF and uses SIMD to compute the sine. The equivalent SBCL non-SIMD equivalent is about 30 times slower: IMPL> (let ((a (nu:rand 100000 :type 'single-float :max 1.0)) (b (nu:rand 100000 :type 'single-float))) (declare (type (simple-array single-float 1) a b) (optimize speed)) (time (loop repeat 1000 do (loop for i below 100000 do (setf (row-major-aref b i) (sin (row-major-aref a i))))))) Evaluation took: 2.495 seconds of real time 2.493383 seconds of total run time (2.493383 user, 0.000000 system) 99.92% CPU 5,505,542,370 processor cycles 0 bytes consed NIL Some overhead of the computation indeed stems from the conversion of single-float to double-float and back again; however, even if we stick to double-float, there is a difference of about 10x. But that's also the point, if an application wants performance over accuracy, they should not be forced to look beyond lisp.","title":"About numericals"},{"location":"#what-numericalsdense-numericals-do-not-intend-to-provide","text":"numericals / dense-numericals do not intend to offer a one-stop solution to the numerical computation ecosystem in common lisp. Even planning to do so is delusionary and should be considered a severe case of NIH syndrome. To that extent, we widely adopt: BMAS backed by SLEEF , and more recently, Eigen backed by BLAS and LAPACK , as well as gsll . This is further coupled with multithreading using lparallel and the C-library builtin OpenMP . As further testament to the profoundly found elsewhere attitude adopted here, we have numericals working with cl:array that can be used in conjunction with other libraries two systems numericals/magicl and dense-numericals/magicl are provided that are essentially wrappers around magicl Thus, we intend to provide facilities for writing fast code fast, but at the same time, we want to promote the use of existing facilities wherever appropriate.","title":"What numericals/dense-numericals do not intend to provide?"},{"location":"#where-next","text":"Install it , Checkout the current state of numericals/dense-numericals , Or visit the API reference Old documentation page","title":"Where next?"},{"location":"#acknowledgements","text":"Everyone who has contributed to SBCL. u/love5an and u/neil-lindquist for the required hand-holding and the gist . Paul Khuong for some blog posts . guicho271828 for SBCL Wiki as well as numcl . All the SLEEF contributors All the contributors of c2ffi and cl-autowrap u/moon-chilled's sassy comment (Is that the term?) It's possible that I could have forgotten to mention somebody - so... yeah... happy number crunching!","title":"Acknowledgements"},{"location":"api/","text":"API Reference Configuration Package: numericals or dense-numericals numericals and dense-numericals come with a number of dynamically bound configuration variables that are put to use in non-inlined code. These include: *multithreaded-threshold* *default-float-format* *inline-with-multithreading* *array-element-type* *array-layout* *broadcast-automatically* Generating and transforming arrays Package: numericals or dense-numericals There are also a number of ways to generate arrays: from lists using asarray simple filled arrays using zeros ones rand full eye and their counterparts using zeros-like ones-like rand-like full-like You can also use other arrays to generate arrays using: copy transpose concat reshape Element-wise operators Package: numericals or dense-numericals Binary arithmetic operators: add subtract multiply divide Binary logical operators: two-arg-< two-arg-<= two-arg-= two-arg-/= two-arg-> two-arg->= Their n-ary counterparts: / * < <= = /= > >= Transcendental operators: sin asin sinh asinh cos acos cosh acosh tan atan tanh atanh exp expt log Rounding operators: abs ffloor floor fceiling ftruncate Miscellaneous: two-arg-max two-arg-min Array reduction operators Package: numericals or dense-numericals sum vdot maximum minimum Linear algebra operators Package: numericals.linalg or dense-numericals.linalg inv lu svd vdot cholesky solve eigvals qr pinv eigvecs rank norm2 det matmul","title":"API Reference"},{"location":"api/#api-reference","text":"","title":"API Reference"},{"location":"api/#configuration","text":"Package: numericals or dense-numericals numericals and dense-numericals come with a number of dynamically bound configuration variables that are put to use in non-inlined code. These include: *multithreaded-threshold* *default-float-format* *inline-with-multithreading* *array-element-type* *array-layout* *broadcast-automatically*","title":"Configuration"},{"location":"api/#generating-and-transforming-arrays","text":"Package: numericals or dense-numericals There are also a number of ways to generate arrays: from lists using asarray simple filled arrays using zeros ones rand full eye and their counterparts using zeros-like ones-like rand-like full-like You can also use other arrays to generate arrays using: copy transpose concat reshape","title":"Generating and transforming arrays"},{"location":"api/#element-wise-operators","text":"Package: numericals or dense-numericals Binary arithmetic operators: add subtract multiply divide Binary logical operators: two-arg-< two-arg-<= two-arg-= two-arg-/= two-arg-> two-arg->= Their n-ary counterparts: / * < <= = /= > >= Transcendental operators: sin asin sinh asinh cos acos cosh acosh tan atan tanh atanh exp expt log Rounding operators: abs ffloor floor fceiling ftruncate Miscellaneous: two-arg-max two-arg-min","title":"Element-wise operators"},{"location":"api/#array-reduction-operators","text":"Package: numericals or dense-numericals sum vdot maximum minimum","title":"Array reduction operators"},{"location":"api/#linear-algebra-operators","text":"Package: numericals.linalg or dense-numericals.linalg inv lu svd vdot cholesky solve eigvals qr pinv eigvecs rank norm2 det matmul","title":"Linear algebra operators"},{"location":"current-state/","text":"Current state of numericals | dense-numericals Included core There are two main asdf systems: numericals is designed to work with cl:array so that interfacing with the rest of the lisp ecosystem is trivial. dense-numericals is designed to work with dense-arrays:array Currently, (asdf:load-system \"numericals\") provides 3(+1) packages: numericals provides basic math functionality numericals.random provides array generators of random numbers sampled from various distributions numericals.linalg contains some common linear algebra operations Equivalent packages are provided by the (asdf:load-system \"dense-numericals\") asdf system: dense-numericals dense-numericals.random dense-numericals.linalg tests Tests for various functions are littered through all the files in the form of (5am:def-test ...) forms. Once the system is loaded, run (asdf:test-system \"numericals\") or (asdf:test-system \"dense-numericals\") to run the tests. dense-numericals and extensible-compound-types Using CL like specialized type declarations for dense-arrays:array requires the use of extensible-compound-types along with (cl:pushnew :extensible-compound-types cl:*features*) and a complete recompilation. See installation for more details. magicl The following two systems provide packages for using magicl functions: numericals/magicl , and dense-numericals/magicl Functions in the numericals.magicl and the dense-numericals.magicl package are essentially wrappers around magicl . Comparison with other libraries Native CL arrays As of this writing, The only libraries that offer broadcasted operations on arrays are this and numcl numcl does not yet have a focus on high performance - though, it should be possible to implement the current einsum based backend using BLAS and BMAS ; instead the focus there is on functionality; by contrast, the focus here is on performance first, and functionality second. Users do not have to choose. :mix option of uiop:define-package can be useful for mixing the two libraries as per user preferences Other minor differences wrt numcl include: Both (ones 2 3 :type 'single-float) and (ones '(2 3) :type 'single-float) are legal in numericals; while only the latter is legal in numcl/numpy numericals provides a *array-element-type-alist* equivalent to swank:*readtable-alist* to provide a package local way of specifying the default element-type for arrays. This can be further overriden by binding *array-element-type* . This does impose performance penalties however. numcl relies on JIT backed by specialized-function , while numericals relies on AOT backed by polymorphic-functions and cl-form-types . Again, these are not either-or, high level user functions can (in theory) utilize specialized-function, while the innards can use static-dispatch either by polymorphic-functions or static-dispatch or fast-generic-functions . In addition to these two, another performant library operating on CL arrays includes lla . Again, uiop:define-package with :mix can be used suitably. The author of numericals did not find other libraries operating on native CL arrays . Non-native CL arrays There are quite a few libraries in Common Lisp in this domain. I have only managed to take a peak at femlisp-matlisp . That said, the goal of numericals is not to replace python ecosystems, at least not in the short run, but instead to overcome the limitations of libraries like py4cl / 2 of sub-10,000 instructions per second. However, now there's also py4cl2-cffi which is about a 10 times faster than py4cl / 2 without using compiler macros yet. Foreign function interfaces gsll A number of functions in gsl and gsll operate on double precision floats, while some applications appreciate operators working on single precision floats without a conversion overhead. Thus, while extremely featureful, gsll is insufficient for everyone's needs. eigen eigen has wonderful documentation. I have only ever done a basic course in linear algebra, and often feel as if I am missing out on something when people work on linear algebra, but I found eigen's documentation to be superb! For instance, see this QuickRef page , or this Linear Algebra tutorial comparing different decompositions , or this linked page on benchmark comparison of various decompositions . So, if eigen and C++ suit your needs, you might as well use it! That said, it isn't the fastest in everything - SLEEF can often be faster in areas where it is specifically designed for. History Curiosity got the better of me one day, and I set out to explore the limits of numerical computing with Common Lisp. I mean - what does speed require? Just memory-locality and SIMD? SBCL has memory-locality. What about SIMD? Well, the functionality hasn't been \"standardized\" yet, and there are several attempts. Indeed, SBCL needs more documentation - think Emacs! But knowledge exists in people's heads. People are willing to share it. So, this was possible. PS: This library began as a reddit post , that, in turn, was triggered by this reddit post . You should probably use the latest SBCL (get from git) , at least SBCL-2.0.9. The build is fairly easy: sh make.sh && sh run-sbcl.sh # or install.sh or pick a binary from here . Project Predecessors sbcl-numericals numericals-2020.08 The project renaming reflects an attempt to separate the portable parts of the codebase from the SBCL-specific part, so that a portability attempt may be made in the future.","title":"Current State"},{"location":"current-state/#current-state-of-numericals-dense-numericals","text":"","title":"Current state of numericals | dense-numericals"},{"location":"current-state/#included","text":"","title":"Included"},{"location":"current-state/#core","text":"There are two main asdf systems: numericals is designed to work with cl:array so that interfacing with the rest of the lisp ecosystem is trivial. dense-numericals is designed to work with dense-arrays:array Currently, (asdf:load-system \"numericals\") provides 3(+1) packages: numericals provides basic math functionality numericals.random provides array generators of random numbers sampled from various distributions numericals.linalg contains some common linear algebra operations Equivalent packages are provided by the (asdf:load-system \"dense-numericals\") asdf system: dense-numericals dense-numericals.random dense-numericals.linalg","title":"core"},{"location":"current-state/#tests","text":"Tests for various functions are littered through all the files in the form of (5am:def-test ...) forms. Once the system is loaded, run (asdf:test-system \"numericals\") or (asdf:test-system \"dense-numericals\") to run the tests.","title":"tests"},{"location":"current-state/#dense-numericals-and-extensible-compound-types","text":"Using CL like specialized type declarations for dense-arrays:array requires the use of extensible-compound-types along with (cl:pushnew :extensible-compound-types cl:*features*) and a complete recompilation. See installation for more details.","title":"dense-numericals and extensible-compound-types"},{"location":"current-state/#magicl","text":"The following two systems provide packages for using magicl functions: numericals/magicl , and dense-numericals/magicl Functions in the numericals.magicl and the dense-numericals.magicl package are essentially wrappers around magicl .","title":"magicl"},{"location":"current-state/#comparison-with-other-libraries","text":"","title":"Comparison with other libraries"},{"location":"current-state/#native-cl-arrays","text":"As of this writing, The only libraries that offer broadcasted operations on arrays are this and numcl numcl does not yet have a focus on high performance - though, it should be possible to implement the current einsum based backend using BLAS and BMAS ; instead the focus there is on functionality; by contrast, the focus here is on performance first, and functionality second. Users do not have to choose. :mix option of uiop:define-package can be useful for mixing the two libraries as per user preferences Other minor differences wrt numcl include: Both (ones 2 3 :type 'single-float) and (ones '(2 3) :type 'single-float) are legal in numericals; while only the latter is legal in numcl/numpy numericals provides a *array-element-type-alist* equivalent to swank:*readtable-alist* to provide a package local way of specifying the default element-type for arrays. This can be further overriden by binding *array-element-type* . This does impose performance penalties however. numcl relies on JIT backed by specialized-function , while numericals relies on AOT backed by polymorphic-functions and cl-form-types . Again, these are not either-or, high level user functions can (in theory) utilize specialized-function, while the innards can use static-dispatch either by polymorphic-functions or static-dispatch or fast-generic-functions . In addition to these two, another performant library operating on CL arrays includes lla . Again, uiop:define-package with :mix can be used suitably. The author of numericals did not find other libraries operating on native CL arrays .","title":"Native CL arrays"},{"location":"current-state/#non-native-cl-arrays","text":"There are quite a few libraries in Common Lisp in this domain. I have only managed to take a peak at femlisp-matlisp . That said, the goal of numericals is not to replace python ecosystems, at least not in the short run, but instead to overcome the limitations of libraries like py4cl / 2 of sub-10,000 instructions per second. However, now there's also py4cl2-cffi which is about a 10 times faster than py4cl / 2 without using compiler macros yet.","title":"Non-native CL arrays"},{"location":"current-state/#foreign-function-interfaces","text":"","title":"Foreign function interfaces"},{"location":"current-state/#gsll","text":"A number of functions in gsl and gsll operate on double precision floats, while some applications appreciate operators working on single precision floats without a conversion overhead. Thus, while extremely featureful, gsll is insufficient for everyone's needs.","title":"gsll"},{"location":"current-state/#eigen","text":"eigen has wonderful documentation. I have only ever done a basic course in linear algebra, and often feel as if I am missing out on something when people work on linear algebra, but I found eigen's documentation to be superb! For instance, see this QuickRef page , or this Linear Algebra tutorial comparing different decompositions , or this linked page on benchmark comparison of various decompositions . So, if eigen and C++ suit your needs, you might as well use it! That said, it isn't the fastest in everything - SLEEF can often be faster in areas where it is specifically designed for.","title":"eigen"},{"location":"current-state/#history","text":"Curiosity got the better of me one day, and I set out to explore the limits of numerical computing with Common Lisp. I mean - what does speed require? Just memory-locality and SIMD? SBCL has memory-locality. What about SIMD? Well, the functionality hasn't been \"standardized\" yet, and there are several attempts. Indeed, SBCL needs more documentation - think Emacs! But knowledge exists in people's heads. People are willing to share it. So, this was possible. PS: This library began as a reddit post , that, in turn, was triggered by this reddit post . You should probably use the latest SBCL (get from git) , at least SBCL-2.0.9. The build is fairly easy: sh make.sh && sh run-sbcl.sh # or install.sh or pick a binary from here .","title":"History"},{"location":"current-state/#project-predecessors","text":"sbcl-numericals numericals-2020.08 The project renaming reflects an attempt to separate the portable parts of the codebase from the SBCL-specific part, so that a portability attempt may be made in the future.","title":"Project Predecessors"},{"location":"install/","text":"Installation Recall that two main asdf systems are provided: numericals , which works with cl:array , and dense-numericals , which works with dense-arrays Load either or both of them according to your needs. quicklisp is a defacto package manager for Common Lisp. All the below methods rely on the availability of the quicklisp client. The instructions to install it can be found here . Other useful resources to get started with quicklisp include: quicklisp - Common Lisp Libraries Installing quicklisp - The Common Lisp Cookbook Once the quicklisp client is installed, proceed with one of the below methods to install numericals or dense-numericals or both. Using quicklisp Simply quickload the required asdf system. (ql:quickload \"numericals\") ;;; OR (ql:quickload \"dense-numericals\") Depending on when you are reading this, the quicklisp versions could be a bit older than what you might wish for. If you want to use recent feature updates or bug fixes, you might want to use download-dependencies or ultralisp. The latest from github and download-dependencies First clone the github repository of numericals to where quicklisp can find. cd /path/to/quicklisp/local-projects/ # This is usually $HOME/quicklisp/local-projects git clone https://github.com/digikar99/numericals download-dependencies is a simple tool to download the dependencies of any given project. In order to use it, clone also the download-dependencies asdf system into your quicklisp local projects. git clone https://github.com/digikar99/download-dependencies Then create the directory where you want to download the dependencies for numericals or dense-numericals . mkdir -p /path/to/dependencies Finally, quickload and download the dependencies: (ql:quickload \"download-dependencies\") (let ((download-dependencies:*dependencies-home* #P\"/path/to/dependencies/\")) (download-dependencies:ensure-system \"numericals\")) Finally, you will need to instruct the quicklisp client to look into this path. After that, you can simply use quickload : (push #P\"/path/to/dependencies\" ql:*local-project-directories*) (ql:quickload \"numericals\") ;;; OR (ql:quickload \"dense-numericals\") Using extensible-compound-types If you are using numericals , you do not need to look into extensible-compound-types . TODO: What to do if you are using dense-numericals ? Using ultralisp Fetch from this dist of ultralisp . Once (ql:quickload \"numericals\") or (ql:quickload \"dense-numericals\") is successful; use inside your own package using :mix option of uiop:define-package (see above discussion), or package-local-nicknames . Using clpm TODO","title":"Installation"},{"location":"install/#installation","text":"Recall that two main asdf systems are provided: numericals , which works with cl:array , and dense-numericals , which works with dense-arrays Load either or both of them according to your needs. quicklisp is a defacto package manager for Common Lisp. All the below methods rely on the availability of the quicklisp client. The instructions to install it can be found here . Other useful resources to get started with quicklisp include: quicklisp - Common Lisp Libraries Installing quicklisp - The Common Lisp Cookbook Once the quicklisp client is installed, proceed with one of the below methods to install numericals or dense-numericals or both.","title":"Installation"},{"location":"install/#using-quicklisp","text":"Simply quickload the required asdf system. (ql:quickload \"numericals\") ;;; OR (ql:quickload \"dense-numericals\") Depending on when you are reading this, the quicklisp versions could be a bit older than what you might wish for. If you want to use recent feature updates or bug fixes, you might want to use download-dependencies or ultralisp.","title":"Using quicklisp"},{"location":"install/#the-latest-from-github-and-download-dependencies","text":"First clone the github repository of numericals to where quicklisp can find. cd /path/to/quicklisp/local-projects/ # This is usually $HOME/quicklisp/local-projects git clone https://github.com/digikar99/numericals download-dependencies is a simple tool to download the dependencies of any given project. In order to use it, clone also the download-dependencies asdf system into your quicklisp local projects. git clone https://github.com/digikar99/download-dependencies Then create the directory where you want to download the dependencies for numericals or dense-numericals . mkdir -p /path/to/dependencies Finally, quickload and download the dependencies: (ql:quickload \"download-dependencies\") (let ((download-dependencies:*dependencies-home* #P\"/path/to/dependencies/\")) (download-dependencies:ensure-system \"numericals\")) Finally, you will need to instruct the quicklisp client to look into this path. After that, you can simply use quickload : (push #P\"/path/to/dependencies\" ql:*local-project-directories*) (ql:quickload \"numericals\") ;;; OR (ql:quickload \"dense-numericals\")","title":"The latest from github and download-dependencies"},{"location":"install/#using-extensible-compound-types","text":"If you are using numericals , you do not need to look into extensible-compound-types . TODO: What to do if you are using dense-numericals ?","title":"Using extensible-compound-types"},{"location":"install/#using-ultralisp","text":"Fetch from this dist of ultralisp . Once (ql:quickload \"numericals\") or (ql:quickload \"dense-numericals\") is successful; use inside your own package using :mix option of uiop:define-package (see above discussion), or package-local-nicknames .","title":"Using ultralisp"},{"location":"install/#using-clpm","text":"TODO","title":"Using clpm"},{"location":"reasonably-fast-clhs-arrays/","text":"Reasonably fast operations for CLHS provided arrays (ql:quickload \"numericals\") ;;; See here for instructions on installing quicklisp: https://quicklisp.org/ Since numericals is /faaar/ from completely, and covers only a very small set of operations involved in numerical computing, wherever appropriate, sister libraries will be mentioned. These include: alexandria : defacto utility library array-operations : minimal dependencies numcl Python's numpy through py4cl2 : not suited for performance, thus the main reason for this project numpy-file-format Also, see package local nicknames if required. Table of Contents Reasonably fast operations for Common Lisp arrays Constructing arrays Controlling how arrays are printed *print-array* *print-length* *print-right-margin* *print-level* *print-lines* Controlling the default array-element-type Reading and writing arrays to disk Constructing arrays Several functions for constructing arrays (out of thin air) include: numericals: zeros, ones, rand, zeros-like, ones-like, rand-like full, full-like, eye array-operations: zeros, zeros*, ones, ones*, rand, randn*, linspace, generate numcl: zeros, ones, arange, linspace, eye cl:make-array Recommended functions for - converting lists to arrays: numericals:asarray - copying arrays: numericals:copy Controlling how arrays are printed Common Lisp provides a number of parameters for this purpose: *print-array* CL-USER> (let ((*print-array* nil)) (princ (numericals:zeros 10)) nil) #<(SIMPLE-VECTOR 10) {103C7B8C0F}> NIL CL-USER> (let ((*print-array* t)) (princ (numericals:zeros 10)) nil) #(0 0 0 0 0 0 0 0 0 0) NIL *print-length* CL-USER> (let ((*print-length* 10)) (princ (numericals:zeros 100)) nil) #(0 0 0 0 0 0 0 0 0 0 ...) NIL CL-USER> (let ((*print-length* nil)) (princ (numericals:zeros 100)) nil) #(0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0) NIL *print-right-margin* CL-USER> (let ((*print-right-margin* 100)) (princ (numericals:zeros 100)) nil) #(0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0) NIL *print-level* The number of levels of an object that should be printed. *print-lines* The number of lines of the object that should be printed. Controlling the default array-element-type There are several variables in the numericals package that allow specifying the default element type of the output in one place, instead of having to specify it for each function individually. These include: *array-element-type* *default-float-format* CL-USER> (let ((numericals:*array-element-type* '(signed-byte 32))) (describe (numericals:add '(1 2 3) '(4 5 6)))) #(5 7 9) [simple specialized vector] Element-type: (SIGNED-BYTE 32) Length: 3 ; No value CL-USER> (let ((numericals:*array-element-type* 'double-float)) (describe (numericals:add '(1 2 3) '(4 5 6)))) #(5.0d0 7.0d0 9.0d0) [simple specialized vector] Element-type: DOUBLE-FLOAT Length: 3 ; No value Certain functions like sin, cos, tan, exp (and several others) are sensible mainly for floating point numbers. For these functions, the default element type of the output is controlled either through the input, or through *default-float-format*, while for other functions like zeros, ones, rand, add, subtract, divide, multiply, the default element type of the output is controlled either by the input, or by the value of *array-element-type*. CL-USER> (let ((numericals:*default-float-format* 'single-float)) (describe (numericals:sin '(1 2 3)))) #(0.84147096 0.9092974 0.14112) [simple specialized vector] Element-type: SINGLE-FLOAT Length: 3 ; No value CL-USER> (let ((numericals:*default-float-format* 'double-float)) (describe (numericals:sin '(1 2 3)))) #(0.8414709848078965d0 0.9092974268256817d0 0.1411200080598672d0) [simple specialized vector] Element-type: DOUBLE-FLOAT Length: 3 ; No value There also exists *array-element-type-alist* that allows specifying package local default element types if the need ever arises. All of these can be overridden for the particular function by explicitly specifying the out or type arguments wherever appropriate. Reading and writing arrays to disk As human-readable text files (slowest method currently): numpy: save, load As readable lisp objects: cl: read, write As binary objects useable by python: numpy-file-format: load-array, store-array","title":"Reasonably fast operations for CLHS provided arrays"},{"location":"reasonably-fast-clhs-arrays/#reasonably-fast-operations-for-clhs-provided-arrays","text":"(ql:quickload \"numericals\") ;;; See here for instructions on installing quicklisp: https://quicklisp.org/ Since numericals is /faaar/ from completely, and covers only a very small set of operations involved in numerical computing, wherever appropriate, sister libraries will be mentioned. These include: alexandria : defacto utility library array-operations : minimal dependencies numcl Python's numpy through py4cl2 : not suited for performance, thus the main reason for this project numpy-file-format Also, see package local nicknames if required. Table of Contents Reasonably fast operations for Common Lisp arrays Constructing arrays Controlling how arrays are printed *print-array* *print-length* *print-right-margin* *print-level* *print-lines* Controlling the default array-element-type Reading and writing arrays to disk","title":"Reasonably fast operations for CLHS provided arrays"},{"location":"reasonably-fast-clhs-arrays/#constructing-arrays","text":"Several functions for constructing arrays (out of thin air) include: numericals: zeros, ones, rand, zeros-like, ones-like, rand-like full, full-like, eye array-operations: zeros, zeros*, ones, ones*, rand, randn*, linspace, generate numcl: zeros, ones, arange, linspace, eye cl:make-array Recommended functions for - converting lists to arrays: numericals:asarray - copying arrays: numericals:copy","title":"Constructing arrays"},{"location":"reasonably-fast-clhs-arrays/#controlling-how-arrays-are-printed","text":"Common Lisp provides a number of parameters for this purpose:","title":"Controlling how arrays are printed"},{"location":"reasonably-fast-clhs-arrays/#print-array","text":"CL-USER> (let ((*print-array* nil)) (princ (numericals:zeros 10)) nil) #<(SIMPLE-VECTOR 10) {103C7B8C0F}> NIL CL-USER> (let ((*print-array* t)) (princ (numericals:zeros 10)) nil) #(0 0 0 0 0 0 0 0 0 0) NIL","title":"*print-array*"},{"location":"reasonably-fast-clhs-arrays/#print-length","text":"CL-USER> (let ((*print-length* 10)) (princ (numericals:zeros 100)) nil) #(0 0 0 0 0 0 0 0 0 0 ...) NIL CL-USER> (let ((*print-length* nil)) (princ (numericals:zeros 100)) nil) #(0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0) NIL","title":"*print-length*"},{"location":"reasonably-fast-clhs-arrays/#print-right-margin","text":"CL-USER> (let ((*print-right-margin* 100)) (princ (numericals:zeros 100)) nil) #(0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0) NIL","title":"*print-right-margin*"},{"location":"reasonably-fast-clhs-arrays/#print-level","text":"The number of levels of an object that should be printed.","title":"*print-level*"},{"location":"reasonably-fast-clhs-arrays/#print-lines","text":"The number of lines of the object that should be printed.","title":"*print-lines*"},{"location":"reasonably-fast-clhs-arrays/#controlling-the-default-array-element-type","text":"There are several variables in the numericals package that allow specifying the default element type of the output in one place, instead of having to specify it for each function individually. These include: *array-element-type* *default-float-format* CL-USER> (let ((numericals:*array-element-type* '(signed-byte 32))) (describe (numericals:add '(1 2 3) '(4 5 6)))) #(5 7 9) [simple specialized vector] Element-type: (SIGNED-BYTE 32) Length: 3 ; No value CL-USER> (let ((numericals:*array-element-type* 'double-float)) (describe (numericals:add '(1 2 3) '(4 5 6)))) #(5.0d0 7.0d0 9.0d0) [simple specialized vector] Element-type: DOUBLE-FLOAT Length: 3 ; No value Certain functions like sin, cos, tan, exp (and several others) are sensible mainly for floating point numbers. For these functions, the default element type of the output is controlled either through the input, or through *default-float-format*, while for other functions like zeros, ones, rand, add, subtract, divide, multiply, the default element type of the output is controlled either by the input, or by the value of *array-element-type*. CL-USER> (let ((numericals:*default-float-format* 'single-float)) (describe (numericals:sin '(1 2 3)))) #(0.84147096 0.9092974 0.14112) [simple specialized vector] Element-type: SINGLE-FLOAT Length: 3 ; No value CL-USER> (let ((numericals:*default-float-format* 'double-float)) (describe (numericals:sin '(1 2 3)))) #(0.8414709848078965d0 0.9092974268256817d0 0.1411200080598672d0) [simple specialized vector] Element-type: DOUBLE-FLOAT Length: 3 ; No value There also exists *array-element-type-alist* that allows specifying package local default element types if the need ever arises. All of these can be overridden for the particular function by explicitly specifying the out or type arguments wherever appropriate.","title":"Controlling the default array-element-type"},{"location":"reasonably-fast-clhs-arrays/#reading-and-writing-arrays-to-disk","text":"As human-readable text files (slowest method currently): numpy: save, load As readable lisp objects: cl: read, write As binary objects useable by python: numpy-file-format: load-array, store-array","title":"Reading and writing arrays to disk"},{"location":"trash/","text":"Unfortunately, quality doesn't dictate popularity , but popularity would dictate the number of libraries, right? Numerical or scientific computing is one such domain where popularity of the ecosystem seems to be a big factor driving the tool availability. While Common Lisp does have mgl-mat magicl and perhaps a few others, their functionality is still dwarfed by the more popular languages. So, without using projects like cffi , cl-autowrap , py4cl , py4cl2-cffi , it can be hard if not impossible to meet one's needs while staying in Common Lisp. numericals or dense-numericals does not intend offer","title":"Trash"}]}