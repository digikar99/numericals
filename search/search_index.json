{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"numericals | dense-numericals: write fast code fast Github Watch Star Issue In case of any inaccuracies, ambiguities or suggestions, please create an issue here . numericals / dense-numericals brings you the performance of proto-NumPy with the debugging and development abilities of Common Lisp. numericals works with cl:array making it trivial to interface with rest of the lisp ecosystem dense-numericals works with dense-arrays:array provid a numpy-like array object for common lisp. dense-arrays itself is fairly extensible, and trivial extensions are provided for static-vectors as well as cl-cuda *. *Currently dense-numericals only works with cl:vector as the backend storage. Create an issue if you'd like dense-numericals to be extended to static-vectors or cl-cuda . Where next? What we mean by writing fast code fast? Install numericals or dense-numericals Compare numericals against other libraries See the manual History of numericals Old documentation page Limitations numericals / dense-numericals do not intend to offer a one-stop solution to the numerical computation ecosystem in common lisp. Even planning to do so is delusionary and should be considered a severe case of NIH syndrome. To that extent, we widely adopt: BMAS backed by SLEEF , and more recently, Eigen backed by BLAS and LAPACK This is further coupled with multithreading using lparallel and the C-library builtin OpenMP . As further testament to the profoundly found elsewhere attitude adopted here, we have numericals working with cl:array that can be used in conjunction with other libraries two systems numericals/magicl and dense-numericals/magicl that are essentially wrappers around magicl Thus, we intend to provide facilities for writing fast code fast, but at the same time, we want to make use of the existing facilities wherever appropriate. Note that optimizing code written using numericals and dense-numericals requires that the implementations support CLTL2, either natively, or through cl-environments . Currently, a bug in variable-information on CCL may prevent optimization. Type propagation requires that users write code using peltadot . Acknowledgements Everyone who has contributed to SBCL. u/love5an and u/neil-lindquist for the required hand-holding and the gist . Paul Khuong for some blog posts . guicho271828 for SBCL Wiki as well as numcl . All the SLEEF contributors All the contributors of c2ffi and cl-autowrap u/moon-chilled's sassy comment (Is that the term?) It's possible that I could have forgotten to mention somebody - so... yeah... happy number crunching!","title":"Introduction"},{"location":"#numericals-dense-numericals-write-fast-code-fast","text":"Github Watch Star Issue In case of any inaccuracies, ambiguities or suggestions, please create an issue here . numericals / dense-numericals brings you the performance of proto-NumPy with the debugging and development abilities of Common Lisp. numericals works with cl:array making it trivial to interface with rest of the lisp ecosystem dense-numericals works with dense-arrays:array provid a numpy-like array object for common lisp. dense-arrays itself is fairly extensible, and trivial extensions are provided for static-vectors as well as cl-cuda *. *Currently dense-numericals only works with cl:vector as the backend storage. Create an issue if you'd like dense-numericals to be extended to static-vectors or cl-cuda .","title":"numericals | dense-numericals: write fast code fast"},{"location":"#where-next","text":"What we mean by writing fast code fast? Install numericals or dense-numericals Compare numericals against other libraries See the manual History of numericals Old documentation page","title":"Where next?"},{"location":"#limitations","text":"numericals / dense-numericals do not intend to offer a one-stop solution to the numerical computation ecosystem in common lisp. Even planning to do so is delusionary and should be considered a severe case of NIH syndrome. To that extent, we widely adopt: BMAS backed by SLEEF , and more recently, Eigen backed by BLAS and LAPACK This is further coupled with multithreading using lparallel and the C-library builtin OpenMP . As further testament to the profoundly found elsewhere attitude adopted here, we have numericals working with cl:array that can be used in conjunction with other libraries two systems numericals/magicl and dense-numericals/magicl that are essentially wrappers around magicl Thus, we intend to provide facilities for writing fast code fast, but at the same time, we want to make use of the existing facilities wherever appropriate. Note that optimizing code written using numericals and dense-numericals requires that the implementations support CLTL2, either natively, or through cl-environments . Currently, a bug in variable-information on CCL may prevent optimization. Type propagation requires that users write code using peltadot .","title":"Limitations"},{"location":"#acknowledgements","text":"Everyone who has contributed to SBCL. u/love5an and u/neil-lindquist for the required hand-holding and the gist . Paul Khuong for some blog posts . guicho271828 for SBCL Wiki as well as numcl . All the SLEEF contributors All the contributors of c2ffi and cl-autowrap u/moon-chilled's sassy comment (Is that the term?) It's possible that I could have forgotten to mention somebody - so... yeah... happy number crunching!","title":"Acknowledgements"},{"location":"common-lisp-and-numericals/","text":"Writing Fast Code Fast Common Lisp Common Lisp is great for prototyping: REPL-based programming means that you can write your program one function at a time. Type, structure, function, and class redefinitions are handled more gracefully compared to most other languages. Global variables in common lisp have dynamic scope, while local variables have default lexical scope. This means you can use global variables locally :). See this example . Its condition system builds upon this and goes beyond traditional exception handling, allowing one to provide restarts for resuming a \"crashed\" program. But it is also great in the longer run: Optional static typing with implementations like SBCL means you do not have to worry yourself with type declarations during prototyping. But as your code and types stabilize, you can add the type declarations to aid documentation as well as speed and safety, while also inlining the small functions whose call overhead exceeds the work they do. A 1994 ANSI standard coupled with several portability libraries means that it is possible to write code that works without changes even after decades. Common Lisp also allows for the deployment of binaries, and there are tools to distribute the shared libraries that the binary depends upon. Indeed, there are limitations: If you want to write fast code, then redefinitions, dynamic scoping, and condition systems can get in the way. So, to some extent, it is a tradeoff that you can work around as your program stabilizes. The type system is much limited than ML like systems and does not provide true parametric types. But see this example . Coalton is an effort to work around this. It provides a ML like programming system built on top of Common Lisp. Numericals Like Julia, Common Lisp (SBCL) already offers a solution to the purported two-language problem. One can write high level code in Common Lisp. Coupled with the above mentioned facilities, this makes prototyping faster. However, in some cases, the same facilities, in particular the dynamic typing, can be prohibitive for performance. 1. Inlining with generics The particular aspect numericals / dense-numericals aim to solve concerns type declarations. Type declarations aids lisp compilers optimize code. It also aids documentation. However, while prototyping or for providing another number processing library unopinionated about the choice of types, one wants to leave the code untyped or atleast undertyped. To illustrate this, let us write our own very-simple-numpy package containing just one function. This will be a simple function add-array that takes in three arrays. It sums up the corresponding elements of the first two arrays and writes them to the third. We want add-array to be generic with respect to element types of the arrays. Thus, we declare that the three parameters array1 array2 out of a function add-array has type (simple-array * 1) rather than the more specific type (simple-array single-float 1) . (defpackage :very-simple-numpy (:use :cl) (:export #:add-array)) (in-package :very-simple-numpy) (defun add-array (array1 array2 out) (declare (type (simple-array * 1) array1 array2 out)) (loop :for i :below (array-total-size array1) :do (setf (aref out i) (+ (aref array1 i) (aref array2 i)))) out) This enables a user of very-simple-numpy to call add-array with any 1-dimensional arrays. Below, add-array-caller/single-float calls add-array with single-float arrays and add-array-caller/double-float calls add-array with double-float arrays. (defpackage :very-simple-numpy-user (:documentation \"Demo package demonstrating the user of VERY-SIMPLE-NUMPY.\") (:use :cl) (:local-nicknames (:nu :very-simple-numpy))) (in-package :very-simple-numpy-user) ;;; Let's ignore how exactly we got the arrays A B and OUT (defun add-array-caller/single-float (a b out) (declare (type (simple-array single-float 1) a b out)) (nu:add-array a b out)) (defun add-array-caller/double-float (a b out) (declare (type (simple-array double-float 1) a b out)) (nu:add-array a b out)) But what price are we paying for this generic-ness? What is the performance penalty compared to writing add-array directly for the specialized case of single-float? (defun add-array/single-float (a b out) (declare (type (simple-array single-float 1) a b out) (optimize speed)) (loop :for i :below (array-total-size a) :do (setf (aref out i) (+ (aref a i) (aref b i)))) out) VERY-SIMPLE-NUMPY-USER> (asdf:load-system \"array-operations\") T VERY-SIMPLE-NUMPY-USER> (let ((a (aops:rand* 'single-float 1000)) (b (aops:rand* 'single-float 1000)) (o (aops:rand* 'single-float 1000))) (time (loop repeat 100000 do (add-array-caller/single-float a b o)))) Evaluation took: 2.974 seconds of real time 2.973779 seconds of total run time (2.973779 user, 0.000000 system) 100.00% CPU 8,336,265,453 processor cycles 0 bytes consed NIL VERY-SIMPLE-NUMPY-USER> (let ((a (aops:rand* 'single-float 1000)) (b (aops:rand* 'single-float 1000)) (o (aops:rand* 'single-float 1000))) (time (loop repeat 1000000 do (add-array/single-float a b o)))) Evaluation took: 1.865 seconds of real time 1.864698 seconds of total run time (1.864698 user, 0.000000 system) 100.00% CPU 5,227,236,485 processor cycles 0 bytes consed NIL One notes that using add-array/single-float , one can perform ten times as many operations in about 2/3rd of the time. Thus, add-array/single-float is about 15 times faster than its generic counterpart add-array . It turns out that on SBCL, this problem can be easily overcome by declaring add-array to be inline and then recompiling add-array-caller/single-float again: (in-package :very-simple-numpy) (declaim (inline add-array)) (defun add-array (array1 array2 out) (declare (type (simple-array * 1) array1 array2 out)) (loop :for i :below (array-total-size array1) :do (setf (aref out i) (+ (aref array1 i) (aref array2 i)))) out) (in-package :very-simple-numpy-user) ;;; Let's ignore how exactly we got the arrays A B and OUT (defun add-array-caller/single-float (a b out) (declare (type (simple-array single-float 1) a b out)) (nu:add-array a b out)) If you evaluate the performance again, both add-array-caller/single-float and add-array/single-float would come out to be equivalent: VERY-SIMPLE-NUMPY-USER> (let ((a (aops:rand* 'single-float 1000)) (b (aops:rand* 'single-float 1000)) (o (aops:rand* 'single-float 1000))) (time (loop repeat 1000000 do (add-array-caller/single-float a b o)))) Evaluation took: 1.870 seconds of real time 1.870364 seconds of total run time (1.870364 user, 0.000000 system) 100.00% CPU 5,243,342,790 processor cycles 0 bytes consed NIL The problem seems solved. 2. Non-generic fast C functions Over the past few decades, excellent C libraries have been written for performant numerical computation. BLAS and LAPACK being the prime ones. Un/Fortunately, these are non-generic: CBLAS_sdot computes a dot product of two single-float vectors, while to compute a dot product of two double-float vectors, one needs to use CBLAS_ddot . Plus ultimately, hardware instructions are non-generic. Does that matter? Isn't SBCL fast enough? Fast enough is a tricky notion. It depends on your task. If your computations run for a few minutes, it doesn't matter if they take 10 minutes or 1 minute. But, if they run for hours or days, perhaps, it would be great if they could run in 1 hour instead of 10 hours? Are we as fast as we could be? For instance, the equivalent numericals/basic-math:add (or numericals:add ) ultimately calls out such non-generic C functions. And coupled with inlining, this gets us about another 10x performance boost. VERY-SIMPLE-NUMPY-USER> (let ((a (aops:rand* 'single-float 1000)) (b (aops:rand* 'single-float 1000)) (o (aops:rand* 'single-float 1000))) (declare (type (simple-array single-float 1) a b o) (optimize speed)) ;; You can throw in a (safety 0) declaration if you are confident ;; and don't want consing. (time (loop repeat 10000000 do (numericals:add a b :out o :broadcast nil)))) Evaluation took: 2.651 seconds of real time 2.650522 seconds of total run time (2.646750 user, 0.003772 system) [ Real times consist of 0.040 seconds GC time, and 2.611 seconds non-GC time. ] [ Run times consist of 0.040 seconds GC time, and 2.611 seconds non-GC time. ] 100.00% CPU 7,429,671,871 processor cycles 640,002,160 bytes consed NIL There isn't really anything mystic about the performance of code generated using C compilers vs SBCL. If we put in enough developer hours , we can get SBCL to produce code that is as fast as C compilers. What C compilers and the ecosystem currently has are A wide support for SIMD instructions across different platforms. SBCL only recently gained support for SIMD on x86-64, and even then AVX512 is missing. Optimized libraries for lots of different numerical computing tasks. It isn't merely about hardware support, the algorithms that do the computations need to be optimized too. And the C ecosystem just excels the Common Lisp ecosystem by miles. So, let's give credit where it's due! With Common Lisp and SBCL, you don't need to worry about memory management, you can develop your code one lisp form at a time, you have global-variables-done-right and the excellent condition system that allows you to inspect the stack without unwinding it, and lots. But when it comes to performance across a wide variety of platforms, the C ecosystem marches ahead. Thus, we want to provide generic lisp functions that ultimately call specialized C functions, and which can also be static dispatched and inlined if we want them to. We have two problems to solve: Dispatch on specialized lisp arrays Provide users the option to static-dispatch and inline the individual specializations Standard Common Lisp generic functions are unsuitable for both. They dispatch on classes, while specialized lisp arrays are not necessarily classes. Enabling this using meta-object protocol seems non-trivial. (But someone may prove me wrong!) Generic functions are also designed with dynamic dispatch in mind. However, static-dispatch can enable static dispatch. In addition, Common Lisp's ANSI standard offers compiler macros that allows one to control what specialized form a particular call site compiles to (see this or this ). However, compiler macros are of limited use without access to the type information of the arguments. Unfortunately, this type information is not available trivially . To access it, one requires a somewhat sophisticated machinery of CLTL2. And even then it is limited, because even on SBCL, type propagation happen in later stages of compilation after (compiler) macro expansions take place. 3. Peltadot and CLTL2 Portable CLTL2 is available in the form of cl-environments . peltadot builds over this and provides polymorphic-functions which overcome all of the above disadvantages of generic functions. These also perform type propagation using compiler macros, and also allows dispatching over optional or keyword arguments. An attempt to dispatch statically is only made if the call site is compiled with (optimize speed) with safety<speed and debug<speed . (defpackage :numericals-user (:use :peltadot) (:local-nicknames (:nu :numericals))) (in-package :numericals-user) Overall, this allows compiling the following code: (disassemble (lambda (x) (declare (optimize speed) (type (simple-array single-float 1) x)) (nu:sin! x))) into code containing no high level function calls but only: ... ; BFA: 4D8B55F0 MOV R10, [R13-16] ; thread.alien-linkage-table-base ; BFE: 41FF92082E0000 CALL [R10+11784] ; &BMAS_ssin ; C05: 488BE3 MOV RSP, RBX ... and the following slight variant (using double-float instead of single-float ) (disassemble (lambda (x) (declare (optimize speed) (type (simple-array double-float 1) x)) (nu:sin! x))) into the following: ... ; 672: 4D8B55F0 MOV R10, [R13-16] ; thread.alien-linkage-table-base ; 676: 41FF92C82E0000 CALL [R10+11976] ; &BMAS_dsin ; 67D: 488BE3 MOV RSP, RBX ... What this aggressive inlining means is that the performance is minimally impacted with increasing loop lengths. Below, notice that the overall number of sin operations calculated are the same in each of the three cases, although the loop lengths are different. NUMERICALS-USER> (let ((a (nu:rand 100000 :type 'single-float)) (b (nu:rand 100000 :type 'single-float))) (declare (optimize speed) (type (simple-array single-float) a b)) (time (loop repeat 1000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.100 seconds of real time 0.101061 seconds of total run time (0.101059 user, 0.000002 system) 101.00% CPU 283,283,628 processor cycles 32,752 bytes consed NIL NUMERICALS-USER> (let ((a (nu:rand 1000 :type 'single-float)) (b (nu:rand 1000 :type 'single-float))) (declare (optimize speed) (type (simple-array single-float) a b)) (time (loop repeat 100000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.103 seconds of real time 0.104586 seconds of total run time (0.104558 user, 0.000028 system) 101.94% CPU 293,246,978 processor cycles 3,176,944 bytes consed NIL NUMERICALS-USER> (let ((a (nu:rand 10 :type 'single-float)) (b (nu:rand 10 :type 'single-float))) (declare (optimize speed) (type (simple-array single-float 1) a b)) (time (loop repeat 10000000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.720 seconds of real time 0.722276 seconds of total run time (0.720217 user, 0.002059 system) [ Real times consist of 0.032 seconds GC time, and 0.688 seconds non-GC time. ] [ Run times consist of 0.031 seconds GC time, and 0.692 seconds non-GC time. ] 100.28% CPU 2,021,508,676 processor cycles 320,001,568 bytes consed NIL NUMERICALS-USER> (let ((a (nu:rand 10 :type 'single-float)) (b (nu:rand 10 :type 'single-float))) (declare (optimize speed (safety 0)) (type (simple-array single-float 1) a b)) (time (loop repeat 10000000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.365 seconds of real time 0.365534 seconds of total run time (0.365534 user, 0.000000 system) 100.27% CPU 1,025,879,135 processor cycles 0 bytes consed NIL Again, this may not matter for most use cases. But when it does matter, a naive pure lisp implementation using generic functions stops being as useful. Then, one is either forced to reimplement the lisp code to suit their needs or move to a different language altogether which handles this. PS: The above BMAS_ssin foreign function is built over SLEEF and uses SIMD to compute the sine. The equivalent SBCL non-SIMD equivalent is about 40 times slower: NUMERICALS-USER> (let ((a (nu:rand 100000 :type 'single-float :max 1.0)) (b (nu:rand 100000 :type 'single-float))) (declare (type (simple-array single-float 1) a b) (optimize speed)) (time (loop repeat 1000 do (loop for i below 100000 do (setf (row-major-aref b i) (sin (row-major-aref a i))))))) Evaluation took: 4.044 seconds of real time 4.039446 seconds of total run time (4.039446 user, 0.000000 system) 99.88% CPU 11,334,909,278 processor cycles 0 bytes consed NIL Some overhead of the computation indeed stems from the conversion of single-float to double-float and back again; however, even if we stick to double-float, there is a difference of about 10x. Summary Thus, \"write fast code fast\" can be broken down into two parts: \"write code fast\": get a first implementation -- a prototype -- built quickly without worrying about type declarations \"write fast code\": once the prototype is ready, optimize and document it by sprinkling it using type declarations This isn't much different from the standard ways of writing Common Lisp. However, as discussed above, there are limitations of standard Common Lisp, which the extension layer peltadot underneath numericals and dense-numericals tries to solve. In the future, this may be replaced or augmented by coalton .","title":"Writing Fast Code Fast"},{"location":"common-lisp-and-numericals/#writing-fast-code-fast","text":"","title":"Writing Fast Code Fast"},{"location":"common-lisp-and-numericals/#common-lisp","text":"Common Lisp is great for prototyping: REPL-based programming means that you can write your program one function at a time. Type, structure, function, and class redefinitions are handled more gracefully compared to most other languages. Global variables in common lisp have dynamic scope, while local variables have default lexical scope. This means you can use global variables locally :). See this example . Its condition system builds upon this and goes beyond traditional exception handling, allowing one to provide restarts for resuming a \"crashed\" program. But it is also great in the longer run: Optional static typing with implementations like SBCL means you do not have to worry yourself with type declarations during prototyping. But as your code and types stabilize, you can add the type declarations to aid documentation as well as speed and safety, while also inlining the small functions whose call overhead exceeds the work they do. A 1994 ANSI standard coupled with several portability libraries means that it is possible to write code that works without changes even after decades. Common Lisp also allows for the deployment of binaries, and there are tools to distribute the shared libraries that the binary depends upon. Indeed, there are limitations: If you want to write fast code, then redefinitions, dynamic scoping, and condition systems can get in the way. So, to some extent, it is a tradeoff that you can work around as your program stabilizes. The type system is much limited than ML like systems and does not provide true parametric types. But see this example . Coalton is an effort to work around this. It provides a ML like programming system built on top of Common Lisp.","title":"Common Lisp"},{"location":"common-lisp-and-numericals/#numericals","text":"Like Julia, Common Lisp (SBCL) already offers a solution to the purported two-language problem. One can write high level code in Common Lisp. Coupled with the above mentioned facilities, this makes prototyping faster. However, in some cases, the same facilities, in particular the dynamic typing, can be prohibitive for performance.","title":"Numericals"},{"location":"common-lisp-and-numericals/#1-inlining-with-generics","text":"The particular aspect numericals / dense-numericals aim to solve concerns type declarations. Type declarations aids lisp compilers optimize code. It also aids documentation. However, while prototyping or for providing another number processing library unopinionated about the choice of types, one wants to leave the code untyped or atleast undertyped. To illustrate this, let us write our own very-simple-numpy package containing just one function. This will be a simple function add-array that takes in three arrays. It sums up the corresponding elements of the first two arrays and writes them to the third. We want add-array to be generic with respect to element types of the arrays. Thus, we declare that the three parameters array1 array2 out of a function add-array has type (simple-array * 1) rather than the more specific type (simple-array single-float 1) . (defpackage :very-simple-numpy (:use :cl) (:export #:add-array)) (in-package :very-simple-numpy) (defun add-array (array1 array2 out) (declare (type (simple-array * 1) array1 array2 out)) (loop :for i :below (array-total-size array1) :do (setf (aref out i) (+ (aref array1 i) (aref array2 i)))) out) This enables a user of very-simple-numpy to call add-array with any 1-dimensional arrays. Below, add-array-caller/single-float calls add-array with single-float arrays and add-array-caller/double-float calls add-array with double-float arrays. (defpackage :very-simple-numpy-user (:documentation \"Demo package demonstrating the user of VERY-SIMPLE-NUMPY.\") (:use :cl) (:local-nicknames (:nu :very-simple-numpy))) (in-package :very-simple-numpy-user) ;;; Let's ignore how exactly we got the arrays A B and OUT (defun add-array-caller/single-float (a b out) (declare (type (simple-array single-float 1) a b out)) (nu:add-array a b out)) (defun add-array-caller/double-float (a b out) (declare (type (simple-array double-float 1) a b out)) (nu:add-array a b out)) But what price are we paying for this generic-ness? What is the performance penalty compared to writing add-array directly for the specialized case of single-float? (defun add-array/single-float (a b out) (declare (type (simple-array single-float 1) a b out) (optimize speed)) (loop :for i :below (array-total-size a) :do (setf (aref out i) (+ (aref a i) (aref b i)))) out) VERY-SIMPLE-NUMPY-USER> (asdf:load-system \"array-operations\") T VERY-SIMPLE-NUMPY-USER> (let ((a (aops:rand* 'single-float 1000)) (b (aops:rand* 'single-float 1000)) (o (aops:rand* 'single-float 1000))) (time (loop repeat 100000 do (add-array-caller/single-float a b o)))) Evaluation took: 2.974 seconds of real time 2.973779 seconds of total run time (2.973779 user, 0.000000 system) 100.00% CPU 8,336,265,453 processor cycles 0 bytes consed NIL VERY-SIMPLE-NUMPY-USER> (let ((a (aops:rand* 'single-float 1000)) (b (aops:rand* 'single-float 1000)) (o (aops:rand* 'single-float 1000))) (time (loop repeat 1000000 do (add-array/single-float a b o)))) Evaluation took: 1.865 seconds of real time 1.864698 seconds of total run time (1.864698 user, 0.000000 system) 100.00% CPU 5,227,236,485 processor cycles 0 bytes consed NIL One notes that using add-array/single-float , one can perform ten times as many operations in about 2/3rd of the time. Thus, add-array/single-float is about 15 times faster than its generic counterpart add-array . It turns out that on SBCL, this problem can be easily overcome by declaring add-array to be inline and then recompiling add-array-caller/single-float again: (in-package :very-simple-numpy) (declaim (inline add-array)) (defun add-array (array1 array2 out) (declare (type (simple-array * 1) array1 array2 out)) (loop :for i :below (array-total-size array1) :do (setf (aref out i) (+ (aref array1 i) (aref array2 i)))) out) (in-package :very-simple-numpy-user) ;;; Let's ignore how exactly we got the arrays A B and OUT (defun add-array-caller/single-float (a b out) (declare (type (simple-array single-float 1) a b out)) (nu:add-array a b out)) If you evaluate the performance again, both add-array-caller/single-float and add-array/single-float would come out to be equivalent: VERY-SIMPLE-NUMPY-USER> (let ((a (aops:rand* 'single-float 1000)) (b (aops:rand* 'single-float 1000)) (o (aops:rand* 'single-float 1000))) (time (loop repeat 1000000 do (add-array-caller/single-float a b o)))) Evaluation took: 1.870 seconds of real time 1.870364 seconds of total run time (1.870364 user, 0.000000 system) 100.00% CPU 5,243,342,790 processor cycles 0 bytes consed NIL The problem seems solved.","title":"1. Inlining with generics"},{"location":"common-lisp-and-numericals/#2-non-generic-fast-c-functions","text":"Over the past few decades, excellent C libraries have been written for performant numerical computation. BLAS and LAPACK being the prime ones. Un/Fortunately, these are non-generic: CBLAS_sdot computes a dot product of two single-float vectors, while to compute a dot product of two double-float vectors, one needs to use CBLAS_ddot . Plus ultimately, hardware instructions are non-generic. Does that matter? Isn't SBCL fast enough? Fast enough is a tricky notion. It depends on your task. If your computations run for a few minutes, it doesn't matter if they take 10 minutes or 1 minute. But, if they run for hours or days, perhaps, it would be great if they could run in 1 hour instead of 10 hours? Are we as fast as we could be? For instance, the equivalent numericals/basic-math:add (or numericals:add ) ultimately calls out such non-generic C functions. And coupled with inlining, this gets us about another 10x performance boost. VERY-SIMPLE-NUMPY-USER> (let ((a (aops:rand* 'single-float 1000)) (b (aops:rand* 'single-float 1000)) (o (aops:rand* 'single-float 1000))) (declare (type (simple-array single-float 1) a b o) (optimize speed)) ;; You can throw in a (safety 0) declaration if you are confident ;; and don't want consing. (time (loop repeat 10000000 do (numericals:add a b :out o :broadcast nil)))) Evaluation took: 2.651 seconds of real time 2.650522 seconds of total run time (2.646750 user, 0.003772 system) [ Real times consist of 0.040 seconds GC time, and 2.611 seconds non-GC time. ] [ Run times consist of 0.040 seconds GC time, and 2.611 seconds non-GC time. ] 100.00% CPU 7,429,671,871 processor cycles 640,002,160 bytes consed NIL There isn't really anything mystic about the performance of code generated using C compilers vs SBCL. If we put in enough developer hours , we can get SBCL to produce code that is as fast as C compilers. What C compilers and the ecosystem currently has are A wide support for SIMD instructions across different platforms. SBCL only recently gained support for SIMD on x86-64, and even then AVX512 is missing. Optimized libraries for lots of different numerical computing tasks. It isn't merely about hardware support, the algorithms that do the computations need to be optimized too. And the C ecosystem just excels the Common Lisp ecosystem by miles. So, let's give credit where it's due! With Common Lisp and SBCL, you don't need to worry about memory management, you can develop your code one lisp form at a time, you have global-variables-done-right and the excellent condition system that allows you to inspect the stack without unwinding it, and lots. But when it comes to performance across a wide variety of platforms, the C ecosystem marches ahead. Thus, we want to provide generic lisp functions that ultimately call specialized C functions, and which can also be static dispatched and inlined if we want them to. We have two problems to solve: Dispatch on specialized lisp arrays Provide users the option to static-dispatch and inline the individual specializations Standard Common Lisp generic functions are unsuitable for both. They dispatch on classes, while specialized lisp arrays are not necessarily classes. Enabling this using meta-object protocol seems non-trivial. (But someone may prove me wrong!) Generic functions are also designed with dynamic dispatch in mind. However, static-dispatch can enable static dispatch. In addition, Common Lisp's ANSI standard offers compiler macros that allows one to control what specialized form a particular call site compiles to (see this or this ). However, compiler macros are of limited use without access to the type information of the arguments. Unfortunately, this type information is not available trivially . To access it, one requires a somewhat sophisticated machinery of CLTL2. And even then it is limited, because even on SBCL, type propagation happen in later stages of compilation after (compiler) macro expansions take place.","title":"2. Non-generic fast C functions"},{"location":"common-lisp-and-numericals/#3-peltadot-and-cltl2","text":"Portable CLTL2 is available in the form of cl-environments . peltadot builds over this and provides polymorphic-functions which overcome all of the above disadvantages of generic functions. These also perform type propagation using compiler macros, and also allows dispatching over optional or keyword arguments. An attempt to dispatch statically is only made if the call site is compiled with (optimize speed) with safety<speed and debug<speed . (defpackage :numericals-user (:use :peltadot) (:local-nicknames (:nu :numericals))) (in-package :numericals-user) Overall, this allows compiling the following code: (disassemble (lambda (x) (declare (optimize speed) (type (simple-array single-float 1) x)) (nu:sin! x))) into code containing no high level function calls but only: ... ; BFA: 4D8B55F0 MOV R10, [R13-16] ; thread.alien-linkage-table-base ; BFE: 41FF92082E0000 CALL [R10+11784] ; &BMAS_ssin ; C05: 488BE3 MOV RSP, RBX ... and the following slight variant (using double-float instead of single-float ) (disassemble (lambda (x) (declare (optimize speed) (type (simple-array double-float 1) x)) (nu:sin! x))) into the following: ... ; 672: 4D8B55F0 MOV R10, [R13-16] ; thread.alien-linkage-table-base ; 676: 41FF92C82E0000 CALL [R10+11976] ; &BMAS_dsin ; 67D: 488BE3 MOV RSP, RBX ... What this aggressive inlining means is that the performance is minimally impacted with increasing loop lengths. Below, notice that the overall number of sin operations calculated are the same in each of the three cases, although the loop lengths are different. NUMERICALS-USER> (let ((a (nu:rand 100000 :type 'single-float)) (b (nu:rand 100000 :type 'single-float))) (declare (optimize speed) (type (simple-array single-float) a b)) (time (loop repeat 1000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.100 seconds of real time 0.101061 seconds of total run time (0.101059 user, 0.000002 system) 101.00% CPU 283,283,628 processor cycles 32,752 bytes consed NIL NUMERICALS-USER> (let ((a (nu:rand 1000 :type 'single-float)) (b (nu:rand 1000 :type 'single-float))) (declare (optimize speed) (type (simple-array single-float) a b)) (time (loop repeat 100000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.103 seconds of real time 0.104586 seconds of total run time (0.104558 user, 0.000028 system) 101.94% CPU 293,246,978 processor cycles 3,176,944 bytes consed NIL NUMERICALS-USER> (let ((a (nu:rand 10 :type 'single-float)) (b (nu:rand 10 :type 'single-float))) (declare (optimize speed) (type (simple-array single-float 1) a b)) (time (loop repeat 10000000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.720 seconds of real time 0.722276 seconds of total run time (0.720217 user, 0.002059 system) [ Real times consist of 0.032 seconds GC time, and 0.688 seconds non-GC time. ] [ Run times consist of 0.031 seconds GC time, and 0.692 seconds non-GC time. ] 100.28% CPU 2,021,508,676 processor cycles 320,001,568 bytes consed NIL NUMERICALS-USER> (let ((a (nu:rand 10 :type 'single-float)) (b (nu:rand 10 :type 'single-float))) (declare (optimize speed (safety 0)) (type (simple-array single-float 1) a b)) (time (loop repeat 10000000 do (nu:sin a :out b :broadcast nil)))) Evaluation took: 0.365 seconds of real time 0.365534 seconds of total run time (0.365534 user, 0.000000 system) 100.27% CPU 1,025,879,135 processor cycles 0 bytes consed NIL Again, this may not matter for most use cases. But when it does matter, a naive pure lisp implementation using generic functions stops being as useful. Then, one is either forced to reimplement the lisp code to suit their needs or move to a different language altogether which handles this. PS: The above BMAS_ssin foreign function is built over SLEEF and uses SIMD to compute the sine. The equivalent SBCL non-SIMD equivalent is about 40 times slower: NUMERICALS-USER> (let ((a (nu:rand 100000 :type 'single-float :max 1.0)) (b (nu:rand 100000 :type 'single-float))) (declare (type (simple-array single-float 1) a b) (optimize speed)) (time (loop repeat 1000 do (loop for i below 100000 do (setf (row-major-aref b i) (sin (row-major-aref a i))))))) Evaluation took: 4.044 seconds of real time 4.039446 seconds of total run time (4.039446 user, 0.000000 system) 99.88% CPU 11,334,909,278 processor cycles 0 bytes consed NIL Some overhead of the computation indeed stems from the conversion of single-float to double-float and back again; however, even if we stick to double-float, there is a difference of about 10x.","title":"3. Peltadot and CLTL2"},{"location":"common-lisp-and-numericals/#summary","text":"Thus, \"write fast code fast\" can be broken down into two parts: \"write code fast\": get a first implementation -- a prototype -- built quickly without worrying about type declarations \"write fast code\": once the prototype is ready, optimize and document it by sprinkling it using type declarations This isn't much different from the standard ways of writing Common Lisp. However, as discussed above, there are limitations of standard Common Lisp, which the extension layer peltadot underneath numericals and dense-numericals tries to solve. In the future, this may be replaced or augmented by coalton .","title":"Summary"},{"location":"comparison/","text":"Comparison with other libraries There are two main asdf systems: numericals is designed to work with cl:array so that interfacing with the rest of the lisp ecosystem is trivial. dense-numericals is designed to work with dense-arrays:array Each of these has a number of systems and corresponding packages: utils basic-math transcendental statistics linalg random These can be loaded individually. For example (asdf:load-system \"numericals/random\") . Native CL arrays As of this writing, The only libraries that offer broadcasted operations on arrays are this and numcl numcl does not yet have a focus on high performance - though, it should be possible to implement the current einsum based backend using BLAS and BMAS ; instead the focus there is on functionality; by contrast, the focus here is on performance first, and functionality second. Users do not have to choose. :mix option of uiop:define-package can be useful for mixing the two libraries as per user preferences Other minor differences wrt numcl include: Both (ones 2 3 :type 'single-float) and (ones '(2 3) :type 'single-float) are legal in numericals; while only the latter is legal in numcl/numpy numericals provides a *array-element-type-alist* similar to swank:*readtable-alist* to provide a package local way of specifying the default element-type for arrays. This can be further overriden by binding *array-element-type* . This does impose performance penalties however. numcl relies on JIT backed by specialized-function , while numericals relies on AOT backed by polymorphic-functions . Again, these are not either-or, high level user functions can (in theory) utilize specialized-function, while the innards can use static-dispatch either by polymorphic-functions or static-dispatch or fast-generic-functions . In addition to these two, another performant library operating on CL arrays includes lla . Again, uiop:define-package with :mix and :local-nicknames can be used suitably. The author of numericals did not find other libraries operating on native CL arrays . Non-native CL arrays There are quite a few libraries in Common Lisp in this domain. I have only managed to take a peak at femlisp-matlisp . That said, the goal of numericals is not to replace python ecosystems, at least not in the short run, but instead to overcome the limitations of libraries like py4cl / 2 of sub-10,000 instructions per second. However, now there's also py4cl2-cffi which is about a 10 times slower than CPython itself. Foreign function interfaces gsll A number of functions in gsl and gsll operate on double precision floats, while some applications appreciate operators working on single precision floats without a conversion overhead. Thus, while extremely featureful, gsll is insufficient for everyone's needs. eigen eigen has wonderful documentation. I have only ever done a basic course in linear algebra, and often feel as if I am missing out on something when people work on linear algebra, but I found eigen's documentation to be superb! For instance, see this QuickRef page , or this Linear Algebra tutorial comparing different decompositions , or this linked page on benchmark comparison of various decompositions . So, if eigen and C++ suit your needs, you might as well use it! That said, it isn't the fastest in everything - SLEEF can often be faster in areas where it is specifically designed for.","title":"Current State"},{"location":"comparison/#comparison-with-other-libraries","text":"There are two main asdf systems: numericals is designed to work with cl:array so that interfacing with the rest of the lisp ecosystem is trivial. dense-numericals is designed to work with dense-arrays:array Each of these has a number of systems and corresponding packages: utils basic-math transcendental statistics linalg random These can be loaded individually. For example (asdf:load-system \"numericals/random\") .","title":"Comparison with other libraries"},{"location":"comparison/#native-cl-arrays","text":"As of this writing, The only libraries that offer broadcasted operations on arrays are this and numcl numcl does not yet have a focus on high performance - though, it should be possible to implement the current einsum based backend using BLAS and BMAS ; instead the focus there is on functionality; by contrast, the focus here is on performance first, and functionality second. Users do not have to choose. :mix option of uiop:define-package can be useful for mixing the two libraries as per user preferences Other minor differences wrt numcl include: Both (ones 2 3 :type 'single-float) and (ones '(2 3) :type 'single-float) are legal in numericals; while only the latter is legal in numcl/numpy numericals provides a *array-element-type-alist* similar to swank:*readtable-alist* to provide a package local way of specifying the default element-type for arrays. This can be further overriden by binding *array-element-type* . This does impose performance penalties however. numcl relies on JIT backed by specialized-function , while numericals relies on AOT backed by polymorphic-functions . Again, these are not either-or, high level user functions can (in theory) utilize specialized-function, while the innards can use static-dispatch either by polymorphic-functions or static-dispatch or fast-generic-functions . In addition to these two, another performant library operating on CL arrays includes lla . Again, uiop:define-package with :mix and :local-nicknames can be used suitably. The author of numericals did not find other libraries operating on native CL arrays .","title":"Native CL arrays"},{"location":"comparison/#non-native-cl-arrays","text":"There are quite a few libraries in Common Lisp in this domain. I have only managed to take a peak at femlisp-matlisp . That said, the goal of numericals is not to replace python ecosystems, at least not in the short run, but instead to overcome the limitations of libraries like py4cl / 2 of sub-10,000 instructions per second. However, now there's also py4cl2-cffi which is about a 10 times slower than CPython itself.","title":"Non-native CL arrays"},{"location":"comparison/#foreign-function-interfaces","text":"","title":"Foreign function interfaces"},{"location":"comparison/#gsll","text":"A number of functions in gsl and gsll operate on double precision floats, while some applications appreciate operators working on single precision floats without a conversion overhead. Thus, while extremely featureful, gsll is insufficient for everyone's needs.","title":"gsll"},{"location":"comparison/#eigen","text":"eigen has wonderful documentation. I have only ever done a basic course in linear algebra, and often feel as if I am missing out on something when people work on linear algebra, but I found eigen's documentation to be superb! For instance, see this QuickRef page , or this Linear Algebra tutorial comparing different decompositions , or this linked page on benchmark comparison of various decompositions . So, if eigen and C++ suit your needs, you might as well use it! That said, it isn't the fastest in everything - SLEEF can often be faster in areas where it is specifically designed for.","title":"eigen"},{"location":"history/","text":"History Curiosity got the better of me one day, and I set out to explore the limits of numerical computing with Common Lisp. I mean - what does speed require? Just memory-locality and SIMD? SBCL has memory-locality. What about SIMD? Well, the functionality hasn't been \"standardized\" yet, and there are several attempts. Indeed, SBCL needs more documentation - think Emacs! But knowledge exists in people's heads. People are willing to share it. So, this was possible. PS: This library began as a reddit post , that, in turn, was triggered by this reddit post . You should probably use the latest SBCL (get from git) , at least SBCL-2.0.9. The build is fairly easy: sh make.sh && sh run-sbcl.sh # or install.sh or pick a binary from here . Project Predecessors sbcl-numericals numericals-2020.08 The project renaming reflects an attempt to separate the portable parts of the codebase from the SBCL-specific part, so that a portability attempt may be made in the future.","title":"History"},{"location":"history/#history","text":"Curiosity got the better of me one day, and I set out to explore the limits of numerical computing with Common Lisp. I mean - what does speed require? Just memory-locality and SIMD? SBCL has memory-locality. What about SIMD? Well, the functionality hasn't been \"standardized\" yet, and there are several attempts. Indeed, SBCL needs more documentation - think Emacs! But knowledge exists in people's heads. People are willing to share it. So, this was possible. PS: This library began as a reddit post , that, in turn, was triggered by this reddit post . You should probably use the latest SBCL (get from git) , at least SBCL-2.0.9. The build is fairly easy: sh make.sh && sh run-sbcl.sh # or install.sh or pick a binary from here .","title":"History"},{"location":"history/#project-predecessors","text":"sbcl-numericals numericals-2020.08 The project renaming reflects an attempt to separate the portable parts of the codebase from the SBCL-specific part, so that a portability attempt may be made in the future.","title":"Project Predecessors"},{"location":"install/","text":"Installation Two main asdf systems are provided: numericals which works with cl:array , and dense-numericals which works with dense-arrays Load either or both of them according to your needs. Without quicklisp client: ocicl Install ocicl as per the instructions here . Once ocicl is installed, setup, and the runtime loaded into the lisp image, simply: (asdf:load-system \"numericals\") ;;; OR (asdf:load-system \"dense-numericals\") Using quicklisp client quicklisp is a defacto package manager for Common Lisp. All the below methods rely on the availability of the quicklisp client. The instructions to install it can be found here . Other useful resources to get started with quicklisp include: quicklisp - Common Lisp Libraries Installing quicklisp - The Common Lisp Cookbook Once the quicklisp client is installed, proceed with one of the below methods to install numericals or dense-numericals or both. Through quicklisp dist alone Simply quickload the required asdf system. (ql:quickload \"numericals\") ;;; OR (ql:quickload \"dense-numericals\") Depending on when you are reading this, the quicklisp versions could be a bit older than what you might wish for. If you want to use recent feature updates or bug fixes, you might want to use ultralisp or download-dependencies. Using ultralisp dist If you are trying Ultralisp for the first time: (ql-dist:install-dist \"http://dist.ultralisp.org/\" :prompt nil) If you have been using ultralisp, simply update its dist: (ql:update-dist \"ultralisp\") Once (ql:quickload \"numericals\") or (ql:quickload \"dense-numericals\") is successful; use inside your own package using :mix option of uiop:define-package , or package-local-nicknames . Using download-dependencies First clone the github repository of numericals to where quicklisp can find. cd /path/to/quicklisp/local-projects/ # This is usually $HOME/quicklisp/local-projects git clone https://github.com/digikar99/numericals download-dependencies is a simple tool to download the dependencies of any given project. In order to use it, clone also the download-dependencies asdf system into your quicklisp local projects. git clone https://github.com/digikar99/download-dependencies Then create the directory where you want to download the dependencies for numericals or dense-numericals . mkdir -p /path/to/dependencies Finally, quickload and download the dependencies: (ql:quickload \"download-dependencies\") (let ((download-dependencies:*dependencies-home* #P\"/path/to/dependencies/\")) (download-dependencies:ensure-system \"numericals\")) Finally, you will need to instruct the quicklisp client to look into this path. After that, you can simply use quickload : (push #P\"/path/to/dependencies\" ql:*local-project-directories*) (ql:quickload \"numericals\") ;;; OR (ql:quickload \"dense-numericals\") Using clpm TODO Using ros TODO Needs a way to install packages outside github.","title":"Installation"},{"location":"install/#installation","text":"Two main asdf systems are provided: numericals which works with cl:array , and dense-numericals which works with dense-arrays Load either or both of them according to your needs.","title":"Installation"},{"location":"install/#without-quicklisp-client-ocicl","text":"Install ocicl as per the instructions here . Once ocicl is installed, setup, and the runtime loaded into the lisp image, simply: (asdf:load-system \"numericals\") ;;; OR (asdf:load-system \"dense-numericals\")","title":"Without quicklisp client: ocicl"},{"location":"install/#using-quicklisp-client","text":"quicklisp is a defacto package manager for Common Lisp. All the below methods rely on the availability of the quicklisp client. The instructions to install it can be found here . Other useful resources to get started with quicklisp include: quicklisp - Common Lisp Libraries Installing quicklisp - The Common Lisp Cookbook Once the quicklisp client is installed, proceed with one of the below methods to install numericals or dense-numericals or both.","title":"Using quicklisp client"},{"location":"install/#through-quicklisp-dist-alone","text":"Simply quickload the required asdf system. (ql:quickload \"numericals\") ;;; OR (ql:quickload \"dense-numericals\") Depending on when you are reading this, the quicklisp versions could be a bit older than what you might wish for. If you want to use recent feature updates or bug fixes, you might want to use ultralisp or download-dependencies.","title":"Through quicklisp dist alone"},{"location":"install/#using-ultralisp-dist","text":"If you are trying Ultralisp for the first time: (ql-dist:install-dist \"http://dist.ultralisp.org/\" :prompt nil) If you have been using ultralisp, simply update its dist: (ql:update-dist \"ultralisp\") Once (ql:quickload \"numericals\") or (ql:quickload \"dense-numericals\") is successful; use inside your own package using :mix option of uiop:define-package , or package-local-nicknames .","title":"Using ultralisp dist"},{"location":"install/#using-download-dependencies","text":"First clone the github repository of numericals to where quicklisp can find. cd /path/to/quicklisp/local-projects/ # This is usually $HOME/quicklisp/local-projects git clone https://github.com/digikar99/numericals download-dependencies is a simple tool to download the dependencies of any given project. In order to use it, clone also the download-dependencies asdf system into your quicklisp local projects. git clone https://github.com/digikar99/download-dependencies Then create the directory where you want to download the dependencies for numericals or dense-numericals . mkdir -p /path/to/dependencies Finally, quickload and download the dependencies: (ql:quickload \"download-dependencies\") (let ((download-dependencies:*dependencies-home* #P\"/path/to/dependencies/\")) (download-dependencies:ensure-system \"numericals\")) Finally, you will need to instruct the quicklisp client to look into this path. After that, you can simply use quickload : (push #P\"/path/to/dependencies\" ql:*local-project-directories*) (ql:quickload \"numericals\") ;;; OR (ql:quickload \"dense-numericals\")","title":"Using download-dependencies"},{"location":"install/#using-clpm","text":"TODO","title":"Using clpm"},{"location":"install/#using-ros","text":"TODO Needs a way to install packages outside github.","title":"Using ros"},{"location":"manual/","text":"Manual There are two main asdf systems: numericals is designed to work with cl:array so that interfacing with the rest of the lisp ecosystem is trivial. dense-numericals is designed to work with dense-arrays:array Each of these has a number of systems and corresponding packages: utils basic-math transcendental statistics linalg random These can be loaded individually. For example (asdf:load-system \"numericals/random\") . Table of Contents Utilities Configuration Generating arrays Modifying arrays Transforming arrays Element-wise operators Array reduction operators Basic Math Utilities and array operations Standard arithmetic operations Comparison operations Rounding operations arg-maximum arg-minimum Other basic operations Transcendental Operations Linear Algebra Random Statistics magicl tests Utilities Package: numericals/utils or dense-numericals/utils Configuration numericals and dense-numericals come with a number of dynamically bound configuration variables that are put to use in non-inlined code. These include: *array-element-type* Variable Default Unbound If BOUND, this is the default value of the or TYPE (or also ELEMENT-TYPE for DENSE-ARRAYS) argument. Overrides *array-element-type-alist* . Is overriden by explicitly passing an TYPE (or also ELEMENT-TYPE for DENSE-ARRAYS) argument. *array-element-type-alist* Variable Default Value: NIL An ALIST mapping package to the default element-type used in that package. Inspired from SWANK:*READTABLE-ALIST* Overrides none. Is overriden by *array-element-type* when bound, or by explicitly passing an TYPE (or also ELEMENT-TYPE for DENSE-ARRAYS) argument. *array-layout* Variable Default Value: :ROW-MAJOR For dense-numericals this specifies the default layout constructed by make-array and constructor functions like asarray , zeros , ones , etc in the DENSE-ARRAYS-PLUS-LITE package. For numericals , this is a dummy variable provided so that code written for numericals may be easily upgradeable to dense-numericals . *broadcast-automatically* Variable Default Value: T If non-NIL, operations automatically perform broadcasting as necessary. If NIL, broadcasting is expected to be performed by the user. Such strictness can be helpful to locate bugs. Broadcasting follows the numpy broadcasting semantics . *default-float-format* Variable Default Value: SINGLE-FLOAT Used for converting non-float arrays to float arrays for floating-point operations like trigonometric functions. *inline-with-multithreading* Variable Default Value: NIL Inlining is usually necessary for smaller arrays; for such arrays multithreading becomes unnecessary. If this parameter is non-NIL, code using multithreading would be emitted; otherwise, the code would be skipped. This is only relevant for transcendental functions which uses lparallel for multithreading. *multithreaded-threshold* Variable Default Value: 80000 The lower bound of the array size beyond which LPARALLEL is used for distributing [transcendental] operations across multiple threads. NOTE: It is not defined if this bound is inclusive or exclusive. Generating arrays Beyond the cl:make-array and dense-arrays:make-array , a number of utilities are provided to generate arrays: From lists of elements: asarray Function: (asarray array-like &key out type layout) array-like can be a list, nested lists, with the nodes ultimately containing numbers or arrays. type indicates the element-type of the array to be generated. The elements from array-like will be coerced to this type type can also be auto , in which case, the element type of the array to be generated will be guessed from the element types of array-like layout is either :row-major or :column-major . However, cl:array can only be :row-major , thus, :column-major is only applicable for dense-arrays:array . Direct ways (avoid list allocation) More direct ways to generate arrays (without allocating lists) include the functions zeros and ones . Both have the common lambda list: (shape &key type layout) The shape is a list of numbers, but it can also be a spliced list of numbers. (numericals:zeros 3 :type 'single-float) ;=> #(0.0 0.0 0.0) (numericals:zeros 2 3 :type 'double-float) ;=> #2A((0.0d0 0.0d0 0.0d0) (0.0d0 0.0d0 0.0d0)) (numericals:zeros '(2 3) :type 'double-float) ;=> #2A((0.0d0 0.0d0 0.0d0) (0.0d0 0.0d0 0.0d0)) (numericals:ones '(2 3) :type 'double-float) ;=> #2A((1.0d0 1.0d0 1.0d0) (1.0d0 1.0d0 1.0d0)) In cases where prefilling an array with 0 or 1 is not important, there is also the empty function. A generalization of ones is the full function, which takes in the additional keyword argument value . (numericals:full 4 :type 'double-float :value 42) ;=> #(42.0d0 42.0d0 42.0d0 42.0d0) In addition to these, arrays with uniform random numbers can be generated using the rand function. The lambda list for this is similar to zeros or ones but it has the additional keyword arguments min and max indicating the range of the uniform number distribution. These have the default values 0 and 1 respectively. (numericals:rand 3 :type 'single-float) ;=> #(0.90184736 0.7570008 0.094017744) (numericals:rand 3 :type 'single-float :min -10.0 :max 10.0) ;=> #(8.339874 1.0285072 2.144558) All of these have a XXX -like counterpart which takes in an existing array and generates an array with shape and element-type similar to the input array. Modifying arrays fill aref* Transforming arrays numericals/utils and dense-numericals/utils only provides transpose and reshape . The other tranformation function concat is provided by [basic-math]. transpose Transposes an array along one or more axes. reshape Element-wise operators abs Miscellaneous: two-arg-max two-arg-min Array reduction operators Package: numericals or dense-numericals sum vdot maximum minimum arg-maximum arg-minimum Basic Math This functionality is provided by the numericals/basic-math or dense-numericals/basic-math systems and packages. Broadly, These can be divided into the following groups. Utilities and array operations copy coerce astype concat shape reshape Standard arithmetic operations Binary operations that take in two arguments and return a new result: add subtract multiply divide Their common lambda list can be given by: Lambda List: (x y &key broadcast out) The inputs X and Y to these functions can be numbers, arrays, or array-like objects (such as lists or lists of lists). The keyword OUT argument can be supplied to use an existing pre-allocated array and avoid array allocation. The default value of BROADCAST is given by *broadcast-automatically* but can be overriden by supplying the keyword BROADCAST argument. When this is NIL, arguments must be of the same shape. Equivalent operations which modify the first argument assuming it is an array end with a '!'. add! subtract! divide! multiply! In contrast to their non-destructive counterparts, their lambda lists do not contain the OUT argument. The first argument is implicitly taken as the OUT . Lambda List: (x y &key broadcast) Finally, there are the n-ary operations corresponding to the lisp functions. + - / * These take in any number of arguments, which can be number, arrays, lists or nested lists, and also an optional keyword argument OUT . In cases where the arguments are non-arrays, for binary operations, they are converted to a type given by (or (when (boundp '*array-element-type*) *array-element-type*) (cdr (assoc *package* *array-element-type-alist*)) t) for n-ary operations, the types are upgraded according to the function numericals/basic-math/impl::normalize-arguments/dmas . The type upgradation also occurs if the arrays are of heterogeneous types and the OUT argument is unsupplied. This upgradation is performed by numericals/common:max-type . When compiled with (optimize speed) , an attempt is made using compiler macros to convert calls to n-ary operations into the binary operations. However, this can be fragile, and for performance reasons, users are recommended to use the binary operations. Comparison operations Similar to the arithmetic , these operations can again be grouped into functions that take in two arguments: two-arg-< two-arg-<= two-arg-= two-arg-/= two-arg-> two-arg->= Their lambda lists is exactly identical to the arithmetic operations. Lambda List: (x y &key broadcast out) CL counterparts that take in two or more than two arguments are given by: < <= = /= > >= In contrast to their CL counterparts, these functions return a 0 or a 1 instead of NIL or T respectively when their arguments are scalar. When the arguments are arrays or array-like, the output is an array of element type (unsigned-byte 8) . This makes it easy to use SIMD-accelerated operations from BMAS and can make a massive difference in performance. Note below that numericals:two-arg-< is about 25 times faster than cl:< . CL-USER> (let ((a (numericals:rand 10 10 :type 'single-float)) (b (numericals:rand 10 10 :type 'single-float)) (c (numericals:rand 10 10 :type t))) (declare (optimize speed) (type (simple-array single-float (10 10)) a b) (type (simple-array t (10 10)) c)) (time (loop repeat 1000000 do (loop for i below 10 do (loop for j below 10 do (setf (aref c i j) (cl:< (aref a i j) (aref b i j)))))))) Evaluation took: 4.939 seconds of real time 4.938077 seconds of total run time (4.938077 user, 0.000000 system) 99.98% CPU 13,843,343,776 processor cycles 0 bytes consed NIL CL-USER> (let ((a (numericals:rand 10 10 :type 'single-float)) (b (numericals:rand 10 10 :type 'single-float)) (c (numericals:rand 10 10 :type '(unsigned-byte 8)))) (declare (optimize speed) (type (simple-array single-float (10 10)) a b) (type (simple-array (unsigned-byte 8) (10 10)) c)) (time (loop repeat 1000000 do (numericals:two-arg-< a b :out c :broadcast nil)))) Evaluation took: 0.187 seconds of real time 0.187028 seconds of total run time (0.187028 user, 0.000000 system) 100.00% CPU 524,357,913 processor cycles 144,019,840 bytes consed NIL Rounding operations CL has four rounding operations. Their counterparts are given by the following shadowing symbols in numericals/basic-math or dense-numericals/basic-math . ffloor fceiling fround ftruncate Lambda List: (value &key broadcast out) Similar to the arithmetic functions , these also have the in-place counterparts which assume that the first argument is an array and is the implicit OUT argument. ffloor! fceiling! fround! ftruncate! arg-maximum Polymorphic Function: (arg-maximum array-like &rest args923 &key (axis NIL axis924) (keep-dims NIL keep-dims925) (out NIL out926)) Find the index of the maximum element along the axis . arg-minimum Polymorphic Function: (arg-minimum array-like &rest args932 &key (axis NIL axis933) (keep-dims NIL keep-dims934) (out NIL out935)) Find the index of the minimum element along the axis . Other basic operations abs! matmul two-arg-matmul dot max two-arg-max min two-arg-min sum maximum minimum logand two-arg-logand logior two-arg-logior logxor two-arg-logxor lognot logandc1 logandc2 lognand lognor logorc1 logorc2 ;; #:logtest ;; #:logbitp ;; #:logcount Transcendental Operations Linear Algebra Package: numericals/linalg or dense-numericals/linalg The functions in these packages use the Eigen library of C++. In actuality, a lite C interface is used. cholesky Function: (cholesky array-like &key out) Compute the cholesky decomposition of positive definite 2D matrices given by array-like . This uses the Eigen::LLT to perform the computation. For a matrix A, it returns L such that A = L * L^C where L is lower triangular, and L^C is the conjugate of L. References: http://www.eigen.tuxfamily.org/dox/classEigen_1_1LLT.html det Function: (det array-like &key out) Calculate the determinant of 2D matrices given by array-like eigvals Function: (eigvals array-like &key eigvals) Use Eigen::EigenSolver to compute the eigenvalues of the 2D square matrix given by array-like . eigvecs Function: (eigvecs array-like &key eigvals eigvecs) Use Eigen::EigenSolver to compute the eigenvalues and eigvectors of the 2D square matrix given by array-like . inv Function: (inv array-like &key out) Calculate the inverse of 2D matrices given by array-like lu Function: (lu array-like &key lu p q) Calculate the lu decomposition of array-like using Eigen::FullPivLU . For input A, it returns three matrices p , lu , and q such that A=P^{\u22121} L U Q^{\u22121} where L is unit-lower-triangular, U is upper-triangular, and p and q are permutation matrices. The matrix lu contains L below the diagonal and U above the diagonal. TODO: matmul seems missing. The following code illustrates the decomposition and the reconstruction (let ((a (asarray '((1 2 3) (4 5 6)) :type 'single-float))) (multiple-value-bind (p lu q) (lu a) (print lu) (matmul (inv p) (asarray '((1 0 0) (0.5 1 0)) :type 'single-float) ; unit lower triangular (asarray '((6 4 5) (0 -1 -0.5)) :type 'single-float) ; upper triangular (inv q)))) #| #<STANDARD-DENSE-ARRAY :ROW-MAJOR 2x3 SINGLE-FLOAT ( 6.000 4.000 5.000 ) ( 0.500 -1.000 -0.500 ) {101110E403}> #<STANDARD-DENSE-ARRAY :ROW-MAJOR 2x3 SINGLE-FLOAT ( 1.000 2.000 3.000 ) ( 4.000 5.000 6.000 ) {101110E893}> |# References: https://eigen.tuxfamily.org/dox/classEigen_1_1FullPivLU.html norm2 Function: (norm2 array-like) Calculate the L2 norm of vectors or the frobenius norm of 2D matrix. outer No documentation found for outer pinv Function: (pinv array-like &key out) Calculate the psuedo inverse of 2D matrices given by array-like . qr Function: (qr array-like &key q r) Calculate the qr decomposition of array-like . rank Function: (rank array-like &key out tol) Use Eigen::ColPivHouseholderQR to calculate the rank of the matrix given by array-like . The tolerance or threshold is given by tol . If not supplied or given as zero, it is the default value set by the eigen's methods. solve Function: (solve a b &rest args987 &key (out NIL out988)) Solves a system of linear equation A*X = b and returns X as the output. At the time of this writing, it uses the Eigen::partialPivQr for square matrices Eigen::householderQR for non-square matrices. References: Eigen::ColPivHouseholderQR documentation for more details Eigen Linear Algebra Tutorial svd Function: (svd array-like s u v) Calculate the svd decomposition of array-like using Eigen::BDCSVD . For input m-by-n input A, it returns u , s , and v such that A = U S V^H where u is a m-by-m unitary, v is a n-by-n unitary, and s is a m-by-n real positive matrix which is zero outside of its main diagonal The diagonal entries of s are known as the singular values of A and the columns of u and v are known as the left and right singular vectors of A respectively. The following code illustrates the decomposition and the reconstruction (FIXME: Update): (let ((a (asarray '((1 2 3) (4 5 6)) :type 'single-float))) (multiple-value-bind (p lu q) (lu a) (print lu) (matmul (inv p) (asarray '((1 0 0) (0.5 1 0)) :type 'single-float) ; unit lower triangular (asarray '((6 4 5) (0 -1 -0.5)) :type 'single-float) ; upper triangular (inv q)))) #| #<STANDARD-DENSE-ARRAY :ROW-MAJOR 2x3 SINGLE-FLOAT ( 6.000 4.000 5.000 ) ( 0.500 -1.000 -0.500 ) {101110E403}> #<STANDARD-DENSE-ARRAY :ROW-MAJOR 2x3 SINGLE-FLOAT ( 1.000 2.000 3.000 ) ( 4.000 5.000 6.000 ) {101110E893}> |# References: https://eigen.tuxfamily.org/dox/classEigen_1_1BDCSVD.html https://eigen.tuxfamily.org/dox/classEigen_1_1JacobiSVD.html vdot Function: (vdot a b) Treat the two input arrays as 1D vectors and calculate their dot product. Random Statistics magicl The following two systems provide packages for using magicl functions: numericals/magicl and dense-numericals/magicl Functions in the numericals/magicl and the dense-numericals/magicl package are essentially wrappers around magicl and return cl:array and dense-arrays:array respectively. This can be helpful for using magicl with other lisp packages such as numcl or lisp-stat. tests Run (asdf:test-system \"numericals\") or (asdf:test-system \"dense-numericals\") . This will load the \"numericals/tests\" or \"dense-numericals/tests\" system respectively and run the tests.","title":"Manual"},{"location":"manual/#manual","text":"There are two main asdf systems: numericals is designed to work with cl:array so that interfacing with the rest of the lisp ecosystem is trivial. dense-numericals is designed to work with dense-arrays:array Each of these has a number of systems and corresponding packages: utils basic-math transcendental statistics linalg random These can be loaded individually. For example (asdf:load-system \"numericals/random\") . Table of Contents Utilities Configuration Generating arrays Modifying arrays Transforming arrays Element-wise operators Array reduction operators Basic Math Utilities and array operations Standard arithmetic operations Comparison operations Rounding operations arg-maximum arg-minimum Other basic operations Transcendental Operations Linear Algebra Random Statistics magicl tests","title":"Manual"},{"location":"manual/#utilities","text":"Package: numericals/utils or dense-numericals/utils","title":"Utilities"},{"location":"manual/#configuration","text":"numericals and dense-numericals come with a number of dynamically bound configuration variables that are put to use in non-inlined code. These include:","title":"Configuration"},{"location":"manual/#array-element-type","text":"Variable Default Unbound If BOUND, this is the default value of the or TYPE (or also ELEMENT-TYPE for DENSE-ARRAYS) argument. Overrides *array-element-type-alist* . Is overriden by explicitly passing an TYPE (or also ELEMENT-TYPE for DENSE-ARRAYS) argument.","title":"*array-element-type*"},{"location":"manual/#array-element-type-alist","text":"Variable Default Value: NIL An ALIST mapping package to the default element-type used in that package. Inspired from SWANK:*READTABLE-ALIST* Overrides none. Is overriden by *array-element-type* when bound, or by explicitly passing an TYPE (or also ELEMENT-TYPE for DENSE-ARRAYS) argument.","title":"*array-element-type-alist*"},{"location":"manual/#array-layout","text":"Variable Default Value: :ROW-MAJOR For dense-numericals this specifies the default layout constructed by make-array and constructor functions like asarray , zeros , ones , etc in the DENSE-ARRAYS-PLUS-LITE package. For numericals , this is a dummy variable provided so that code written for numericals may be easily upgradeable to dense-numericals .","title":"*array-layout*"},{"location":"manual/#broadcast-automatically","text":"Variable Default Value: T If non-NIL, operations automatically perform broadcasting as necessary. If NIL, broadcasting is expected to be performed by the user. Such strictness can be helpful to locate bugs. Broadcasting follows the numpy broadcasting semantics .","title":"*broadcast-automatically*"},{"location":"manual/#default-float-format","text":"Variable Default Value: SINGLE-FLOAT Used for converting non-float arrays to float arrays for floating-point operations like trigonometric functions.","title":"*default-float-format*"},{"location":"manual/#inline-with-multithreading","text":"Variable Default Value: NIL Inlining is usually necessary for smaller arrays; for such arrays multithreading becomes unnecessary. If this parameter is non-NIL, code using multithreading would be emitted; otherwise, the code would be skipped. This is only relevant for transcendental functions which uses lparallel for multithreading.","title":"*inline-with-multithreading*"},{"location":"manual/#multithreaded-threshold","text":"Variable Default Value: 80000 The lower bound of the array size beyond which LPARALLEL is used for distributing [transcendental] operations across multiple threads. NOTE: It is not defined if this bound is inclusive or exclusive.","title":"*multithreaded-threshold*"},{"location":"manual/#generating-arrays","text":"Beyond the cl:make-array and dense-arrays:make-array , a number of utilities are provided to generate arrays:","title":"Generating arrays"},{"location":"manual/#from-lists-of-elements-asarray","text":"Function: (asarray array-like &key out type layout) array-like can be a list, nested lists, with the nodes ultimately containing numbers or arrays. type indicates the element-type of the array to be generated. The elements from array-like will be coerced to this type type can also be auto , in which case, the element type of the array to be generated will be guessed from the element types of array-like layout is either :row-major or :column-major . However, cl:array can only be :row-major , thus, :column-major is only applicable for dense-arrays:array .","title":"From lists of elements: asarray"},{"location":"manual/#direct-ways-avoid-list-allocation","text":"More direct ways to generate arrays (without allocating lists) include the functions zeros and ones . Both have the common lambda list: (shape &key type layout) The shape is a list of numbers, but it can also be a spliced list of numbers. (numericals:zeros 3 :type 'single-float) ;=> #(0.0 0.0 0.0) (numericals:zeros 2 3 :type 'double-float) ;=> #2A((0.0d0 0.0d0 0.0d0) (0.0d0 0.0d0 0.0d0)) (numericals:zeros '(2 3) :type 'double-float) ;=> #2A((0.0d0 0.0d0 0.0d0) (0.0d0 0.0d0 0.0d0)) (numericals:ones '(2 3) :type 'double-float) ;=> #2A((1.0d0 1.0d0 1.0d0) (1.0d0 1.0d0 1.0d0)) In cases where prefilling an array with 0 or 1 is not important, there is also the empty function. A generalization of ones is the full function, which takes in the additional keyword argument value . (numericals:full 4 :type 'double-float :value 42) ;=> #(42.0d0 42.0d0 42.0d0 42.0d0) In addition to these, arrays with uniform random numbers can be generated using the rand function. The lambda list for this is similar to zeros or ones but it has the additional keyword arguments min and max indicating the range of the uniform number distribution. These have the default values 0 and 1 respectively. (numericals:rand 3 :type 'single-float) ;=> #(0.90184736 0.7570008 0.094017744) (numericals:rand 3 :type 'single-float :min -10.0 :max 10.0) ;=> #(8.339874 1.0285072 2.144558) All of these have a XXX -like counterpart which takes in an existing array and generates an array with shape and element-type similar to the input array.","title":"Direct ways (avoid list allocation)"},{"location":"manual/#modifying-arrays","text":"","title":"Modifying arrays"},{"location":"manual/#fill","text":"","title":"fill"},{"location":"manual/#aref","text":"","title":"aref*"},{"location":"manual/#transforming-arrays","text":"numericals/utils and dense-numericals/utils only provides transpose and reshape . The other tranformation function concat is provided by [basic-math].","title":"Transforming arrays"},{"location":"manual/#transpose","text":"Transposes an array along one or more axes.","title":"transpose"},{"location":"manual/#reshape","text":"","title":"reshape"},{"location":"manual/#element-wise-operators","text":"abs Miscellaneous: two-arg-max two-arg-min","title":"Element-wise operators"},{"location":"manual/#array-reduction-operators","text":"Package: numericals or dense-numericals sum vdot maximum minimum arg-maximum arg-minimum","title":"Array reduction operators"},{"location":"manual/#basic-math","text":"This functionality is provided by the numericals/basic-math or dense-numericals/basic-math systems and packages. Broadly, These can be divided into the following groups.","title":"Basic Math"},{"location":"manual/#utilities-and-array-operations","text":"copy coerce astype concat shape reshape","title":"Utilities and array operations"},{"location":"manual/#standard-arithmetic-operations","text":"Binary operations that take in two arguments and return a new result: add subtract multiply divide Their common lambda list can be given by: Lambda List: (x y &key broadcast out) The inputs X and Y to these functions can be numbers, arrays, or array-like objects (such as lists or lists of lists). The keyword OUT argument can be supplied to use an existing pre-allocated array and avoid array allocation. The default value of BROADCAST is given by *broadcast-automatically* but can be overriden by supplying the keyword BROADCAST argument. When this is NIL, arguments must be of the same shape. Equivalent operations which modify the first argument assuming it is an array end with a '!'. add! subtract! divide! multiply! In contrast to their non-destructive counterparts, their lambda lists do not contain the OUT argument. The first argument is implicitly taken as the OUT . Lambda List: (x y &key broadcast) Finally, there are the n-ary operations corresponding to the lisp functions. + - / * These take in any number of arguments, which can be number, arrays, lists or nested lists, and also an optional keyword argument OUT . In cases where the arguments are non-arrays, for binary operations, they are converted to a type given by (or (when (boundp '*array-element-type*) *array-element-type*) (cdr (assoc *package* *array-element-type-alist*)) t) for n-ary operations, the types are upgraded according to the function numericals/basic-math/impl::normalize-arguments/dmas . The type upgradation also occurs if the arrays are of heterogeneous types and the OUT argument is unsupplied. This upgradation is performed by numericals/common:max-type . When compiled with (optimize speed) , an attempt is made using compiler macros to convert calls to n-ary operations into the binary operations. However, this can be fragile, and for performance reasons, users are recommended to use the binary operations.","title":"Standard arithmetic operations"},{"location":"manual/#comparison-operations","text":"Similar to the arithmetic , these operations can again be grouped into functions that take in two arguments: two-arg-< two-arg-<= two-arg-= two-arg-/= two-arg-> two-arg->= Their lambda lists is exactly identical to the arithmetic operations. Lambda List: (x y &key broadcast out) CL counterparts that take in two or more than two arguments are given by: < <= = /= > >= In contrast to their CL counterparts, these functions return a 0 or a 1 instead of NIL or T respectively when their arguments are scalar. When the arguments are arrays or array-like, the output is an array of element type (unsigned-byte 8) . This makes it easy to use SIMD-accelerated operations from BMAS and can make a massive difference in performance. Note below that numericals:two-arg-< is about 25 times faster than cl:< . CL-USER> (let ((a (numericals:rand 10 10 :type 'single-float)) (b (numericals:rand 10 10 :type 'single-float)) (c (numericals:rand 10 10 :type t))) (declare (optimize speed) (type (simple-array single-float (10 10)) a b) (type (simple-array t (10 10)) c)) (time (loop repeat 1000000 do (loop for i below 10 do (loop for j below 10 do (setf (aref c i j) (cl:< (aref a i j) (aref b i j)))))))) Evaluation took: 4.939 seconds of real time 4.938077 seconds of total run time (4.938077 user, 0.000000 system) 99.98% CPU 13,843,343,776 processor cycles 0 bytes consed NIL CL-USER> (let ((a (numericals:rand 10 10 :type 'single-float)) (b (numericals:rand 10 10 :type 'single-float)) (c (numericals:rand 10 10 :type '(unsigned-byte 8)))) (declare (optimize speed) (type (simple-array single-float (10 10)) a b) (type (simple-array (unsigned-byte 8) (10 10)) c)) (time (loop repeat 1000000 do (numericals:two-arg-< a b :out c :broadcast nil)))) Evaluation took: 0.187 seconds of real time 0.187028 seconds of total run time (0.187028 user, 0.000000 system) 100.00% CPU 524,357,913 processor cycles 144,019,840 bytes consed NIL","title":"Comparison operations"},{"location":"manual/#rounding-operations","text":"CL has four rounding operations. Their counterparts are given by the following shadowing symbols in numericals/basic-math or dense-numericals/basic-math . ffloor fceiling fround ftruncate Lambda List: (value &key broadcast out) Similar to the arithmetic functions , these also have the in-place counterparts which assume that the first argument is an array and is the implicit OUT argument. ffloor! fceiling! fround! ftruncate!","title":"Rounding operations"},{"location":"manual/#arg-maximum","text":"Polymorphic Function: (arg-maximum array-like &rest args923 &key (axis NIL axis924) (keep-dims NIL keep-dims925) (out NIL out926)) Find the index of the maximum element along the axis .","title":"arg-maximum"},{"location":"manual/#arg-minimum","text":"Polymorphic Function: (arg-minimum array-like &rest args932 &key (axis NIL axis933) (keep-dims NIL keep-dims934) (out NIL out935)) Find the index of the minimum element along the axis .","title":"arg-minimum"},{"location":"manual/#other-basic-operations","text":"abs! matmul two-arg-matmul dot max two-arg-max min two-arg-min sum maximum minimum logand two-arg-logand logior two-arg-logior logxor two-arg-logxor lognot logandc1 logandc2 lognand lognor logorc1 logorc2 ;; #:logtest ;; #:logbitp ;; #:logcount","title":"Other basic operations"},{"location":"manual/#transcendental-operations","text":"","title":"Transcendental Operations"},{"location":"manual/#linear-algebra","text":"Package: numericals/linalg or dense-numericals/linalg The functions in these packages use the Eigen library of C++. In actuality, a lite C interface is used.","title":"Linear Algebra"},{"location":"manual/#cholesky","text":"Function: (cholesky array-like &key out) Compute the cholesky decomposition of positive definite 2D matrices given by array-like . This uses the Eigen::LLT to perform the computation. For a matrix A, it returns L such that A = L * L^C where L is lower triangular, and L^C is the conjugate of L. References: http://www.eigen.tuxfamily.org/dox/classEigen_1_1LLT.html","title":"cholesky"},{"location":"manual/#det","text":"Function: (det array-like &key out) Calculate the determinant of 2D matrices given by array-like","title":"det"},{"location":"manual/#eigvals","text":"Function: (eigvals array-like &key eigvals) Use Eigen::EigenSolver to compute the eigenvalues of the 2D square matrix given by array-like .","title":"eigvals"},{"location":"manual/#eigvecs","text":"Function: (eigvecs array-like &key eigvals eigvecs) Use Eigen::EigenSolver to compute the eigenvalues and eigvectors of the 2D square matrix given by array-like .","title":"eigvecs"},{"location":"manual/#inv","text":"Function: (inv array-like &key out) Calculate the inverse of 2D matrices given by array-like","title":"inv"},{"location":"manual/#lu","text":"Function: (lu array-like &key lu p q) Calculate the lu decomposition of array-like using Eigen::FullPivLU . For input A, it returns three matrices p , lu , and q such that A=P^{\u22121} L U Q^{\u22121} where L is unit-lower-triangular, U is upper-triangular, and p and q are permutation matrices. The matrix lu contains L below the diagonal and U above the diagonal. TODO: matmul seems missing. The following code illustrates the decomposition and the reconstruction (let ((a (asarray '((1 2 3) (4 5 6)) :type 'single-float))) (multiple-value-bind (p lu q) (lu a) (print lu) (matmul (inv p) (asarray '((1 0 0) (0.5 1 0)) :type 'single-float) ; unit lower triangular (asarray '((6 4 5) (0 -1 -0.5)) :type 'single-float) ; upper triangular (inv q)))) #| #<STANDARD-DENSE-ARRAY :ROW-MAJOR 2x3 SINGLE-FLOAT ( 6.000 4.000 5.000 ) ( 0.500 -1.000 -0.500 ) {101110E403}> #<STANDARD-DENSE-ARRAY :ROW-MAJOR 2x3 SINGLE-FLOAT ( 1.000 2.000 3.000 ) ( 4.000 5.000 6.000 ) {101110E893}> |# References: https://eigen.tuxfamily.org/dox/classEigen_1_1FullPivLU.html","title":"lu"},{"location":"manual/#norm2","text":"Function: (norm2 array-like) Calculate the L2 norm of vectors or the frobenius norm of 2D matrix.","title":"norm2"},{"location":"manual/#outer","text":"No documentation found for outer","title":"outer"},{"location":"manual/#pinv","text":"Function: (pinv array-like &key out) Calculate the psuedo inverse of 2D matrices given by array-like .","title":"pinv"},{"location":"manual/#qr","text":"Function: (qr array-like &key q r) Calculate the qr decomposition of array-like .","title":"qr"},{"location":"manual/#rank","text":"Function: (rank array-like &key out tol) Use Eigen::ColPivHouseholderQR to calculate the rank of the matrix given by array-like . The tolerance or threshold is given by tol . If not supplied or given as zero, it is the default value set by the eigen's methods.","title":"rank"},{"location":"manual/#solve","text":"Function: (solve a b &rest args987 &key (out NIL out988)) Solves a system of linear equation A*X = b and returns X as the output. At the time of this writing, it uses the Eigen::partialPivQr for square matrices Eigen::householderQR for non-square matrices. References: Eigen::ColPivHouseholderQR documentation for more details Eigen Linear Algebra Tutorial","title":"solve"},{"location":"manual/#svd","text":"Function: (svd array-like s u v) Calculate the svd decomposition of array-like using Eigen::BDCSVD . For input m-by-n input A, it returns u , s , and v such that A = U S V^H where u is a m-by-m unitary, v is a n-by-n unitary, and s is a m-by-n real positive matrix which is zero outside of its main diagonal The diagonal entries of s are known as the singular values of A and the columns of u and v are known as the left and right singular vectors of A respectively. The following code illustrates the decomposition and the reconstruction (FIXME: Update): (let ((a (asarray '((1 2 3) (4 5 6)) :type 'single-float))) (multiple-value-bind (p lu q) (lu a) (print lu) (matmul (inv p) (asarray '((1 0 0) (0.5 1 0)) :type 'single-float) ; unit lower triangular (asarray '((6 4 5) (0 -1 -0.5)) :type 'single-float) ; upper triangular (inv q)))) #| #<STANDARD-DENSE-ARRAY :ROW-MAJOR 2x3 SINGLE-FLOAT ( 6.000 4.000 5.000 ) ( 0.500 -1.000 -0.500 ) {101110E403}> #<STANDARD-DENSE-ARRAY :ROW-MAJOR 2x3 SINGLE-FLOAT ( 1.000 2.000 3.000 ) ( 4.000 5.000 6.000 ) {101110E893}> |# References: https://eigen.tuxfamily.org/dox/classEigen_1_1BDCSVD.html https://eigen.tuxfamily.org/dox/classEigen_1_1JacobiSVD.html","title":"svd"},{"location":"manual/#vdot","text":"Function: (vdot a b) Treat the two input arrays as 1D vectors and calculate their dot product.","title":"vdot"},{"location":"manual/#random","text":"","title":"Random"},{"location":"manual/#statistics","text":"","title":"Statistics"},{"location":"manual/#magicl","text":"The following two systems provide packages for using magicl functions: numericals/magicl and dense-numericals/magicl Functions in the numericals/magicl and the dense-numericals/magicl package are essentially wrappers around magicl and return cl:array and dense-arrays:array respectively. This can be helpful for using magicl with other lisp packages such as numcl or lisp-stat.","title":"magicl"},{"location":"manual/#tests","text":"Run (asdf:test-system \"numericals\") or (asdf:test-system \"dense-numericals\") . This will load the \"numericals/tests\" or \"dense-numericals/tests\" system respectively and run the tests.","title":"tests"},{"location":"reasonably-fast-clhs-arrays/","text":"Reasonably fast operations for CLHS provided arrays (ql:quickload \"numericals\") ;;; See here for instructions on installing quicklisp: https://quicklisp.org/ Since numericals is /faaar/ from completely, and covers only a very small set of operations involved in numerical computing, wherever appropriate, sister libraries will be mentioned. These include: alexandria : defacto utility library array-operations : minimal dependencies numcl Python's numpy through py4cl2 : not suited for performance, thus the main reason for this project numpy-file-format Also, see package local nicknames if required. Table of Contents Reasonably fast operations for Common Lisp arrays Constructing arrays Controlling how arrays are printed *print-array* *print-length* *print-right-margin* *print-level* *print-lines* Controlling the default array-element-type Reading and writing arrays to disk Constructing arrays Several functions for constructing arrays (out of thin air) include: numericals: zeros, ones, rand, zeros-like, ones-like, rand-like full, full-like, eye array-operations: zeros, zeros*, ones, ones*, rand, randn*, linspace, generate numcl: zeros, ones, arange, linspace, eye cl:make-array Recommended functions for - converting lists to arrays: numericals:asarray - copying arrays: numericals:copy Controlling how arrays are printed Common Lisp provides a number of parameters for this purpose: *print-array* CL-USER> (let ((*print-array* nil)) (princ (numericals:zeros 10)) nil) #<(SIMPLE-VECTOR 10) {103C7B8C0F}> NIL CL-USER> (let ((*print-array* t)) (princ (numericals:zeros 10)) nil) #(0 0 0 0 0 0 0 0 0 0) NIL *print-length* CL-USER> (let ((*print-length* 10)) (princ (numericals:zeros 100)) nil) #(0 0 0 0 0 0 0 0 0 0 ...) NIL CL-USER> (let ((*print-length* nil)) (princ (numericals:zeros 100)) nil) #(0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0) NIL *print-right-margin* CL-USER> (let ((*print-right-margin* 100)) (princ (numericals:zeros 100)) nil) #(0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0) NIL *print-level* The number of levels of an object that should be printed. *print-lines* The number of lines of the object that should be printed. Controlling the default array-element-type There are several variables in the numericals package that allow specifying the default element type of the output in one place, instead of having to specify it for each function individually. These include: *array-element-type* *default-float-format* CL-USER> (let ((numericals:*array-element-type* '(signed-byte 32))) (describe (numericals:add '(1 2 3) '(4 5 6)))) #(5 7 9) [simple specialized vector] Element-type: (SIGNED-BYTE 32) Length: 3 ; No value CL-USER> (let ((numericals:*array-element-type* 'double-float)) (describe (numericals:add '(1 2 3) '(4 5 6)))) #(5.0d0 7.0d0 9.0d0) [simple specialized vector] Element-type: DOUBLE-FLOAT Length: 3 ; No value Certain functions like sin, cos, tan, exp (and several others) are sensible mainly for floating point numbers. For these functions, the default element type of the output is controlled either through the input, or through *default-float-format*, while for other functions like zeros, ones, rand, add, subtract, divide, multiply, the default element type of the output is controlled either by the input, or by the value of *array-element-type*. CL-USER> (let ((numericals:*default-float-format* 'single-float)) (describe (numericals:sin '(1 2 3)))) #(0.84147096 0.9092974 0.14112) [simple specialized vector] Element-type: SINGLE-FLOAT Length: 3 ; No value CL-USER> (let ((numericals:*default-float-format* 'double-float)) (describe (numericals:sin '(1 2 3)))) #(0.8414709848078965d0 0.9092974268256817d0 0.1411200080598672d0) [simple specialized vector] Element-type: DOUBLE-FLOAT Length: 3 ; No value There also exists *array-element-type-alist* that allows specifying package local default element types if the need ever arises. All of these can be overridden for the particular function by explicitly specifying the out or type arguments wherever appropriate. Reading and writing arrays to disk As human-readable text files (slowest method currently): numpy: save, load As readable lisp objects: cl: read, write As binary objects useable by python: numpy-file-format: load-array, store-array","title":"Reasonably fast operations for CLHS provided arrays"},{"location":"reasonably-fast-clhs-arrays/#reasonably-fast-operations-for-clhs-provided-arrays","text":"(ql:quickload \"numericals\") ;;; See here for instructions on installing quicklisp: https://quicklisp.org/ Since numericals is /faaar/ from completely, and covers only a very small set of operations involved in numerical computing, wherever appropriate, sister libraries will be mentioned. These include: alexandria : defacto utility library array-operations : minimal dependencies numcl Python's numpy through py4cl2 : not suited for performance, thus the main reason for this project numpy-file-format Also, see package local nicknames if required. Table of Contents Reasonably fast operations for Common Lisp arrays Constructing arrays Controlling how arrays are printed *print-array* *print-length* *print-right-margin* *print-level* *print-lines* Controlling the default array-element-type Reading and writing arrays to disk","title":"Reasonably fast operations for CLHS provided arrays"},{"location":"reasonably-fast-clhs-arrays/#constructing-arrays","text":"Several functions for constructing arrays (out of thin air) include: numericals: zeros, ones, rand, zeros-like, ones-like, rand-like full, full-like, eye array-operations: zeros, zeros*, ones, ones*, rand, randn*, linspace, generate numcl: zeros, ones, arange, linspace, eye cl:make-array Recommended functions for - converting lists to arrays: numericals:asarray - copying arrays: numericals:copy","title":"Constructing arrays"},{"location":"reasonably-fast-clhs-arrays/#controlling-how-arrays-are-printed","text":"Common Lisp provides a number of parameters for this purpose:","title":"Controlling how arrays are printed"},{"location":"reasonably-fast-clhs-arrays/#print-array","text":"CL-USER> (let ((*print-array* nil)) (princ (numericals:zeros 10)) nil) #<(SIMPLE-VECTOR 10) {103C7B8C0F}> NIL CL-USER> (let ((*print-array* t)) (princ (numericals:zeros 10)) nil) #(0 0 0 0 0 0 0 0 0 0) NIL","title":"*print-array*"},{"location":"reasonably-fast-clhs-arrays/#print-length","text":"CL-USER> (let ((*print-length* 10)) (princ (numericals:zeros 100)) nil) #(0 0 0 0 0 0 0 0 0 0 ...) NIL CL-USER> (let ((*print-length* nil)) (princ (numericals:zeros 100)) nil) #(0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0) NIL","title":"*print-length*"},{"location":"reasonably-fast-clhs-arrays/#print-right-margin","text":"CL-USER> (let ((*print-right-margin* 100)) (princ (numericals:zeros 100)) nil) #(0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0) NIL","title":"*print-right-margin*"},{"location":"reasonably-fast-clhs-arrays/#print-level","text":"The number of levels of an object that should be printed.","title":"*print-level*"},{"location":"reasonably-fast-clhs-arrays/#print-lines","text":"The number of lines of the object that should be printed.","title":"*print-lines*"},{"location":"reasonably-fast-clhs-arrays/#controlling-the-default-array-element-type","text":"There are several variables in the numericals package that allow specifying the default element type of the output in one place, instead of having to specify it for each function individually. These include: *array-element-type* *default-float-format* CL-USER> (let ((numericals:*array-element-type* '(signed-byte 32))) (describe (numericals:add '(1 2 3) '(4 5 6)))) #(5 7 9) [simple specialized vector] Element-type: (SIGNED-BYTE 32) Length: 3 ; No value CL-USER> (let ((numericals:*array-element-type* 'double-float)) (describe (numericals:add '(1 2 3) '(4 5 6)))) #(5.0d0 7.0d0 9.0d0) [simple specialized vector] Element-type: DOUBLE-FLOAT Length: 3 ; No value Certain functions like sin, cos, tan, exp (and several others) are sensible mainly for floating point numbers. For these functions, the default element type of the output is controlled either through the input, or through *default-float-format*, while for other functions like zeros, ones, rand, add, subtract, divide, multiply, the default element type of the output is controlled either by the input, or by the value of *array-element-type*. CL-USER> (let ((numericals:*default-float-format* 'single-float)) (describe (numericals:sin '(1 2 3)))) #(0.84147096 0.9092974 0.14112) [simple specialized vector] Element-type: SINGLE-FLOAT Length: 3 ; No value CL-USER> (let ((numericals:*default-float-format* 'double-float)) (describe (numericals:sin '(1 2 3)))) #(0.8414709848078965d0 0.9092974268256817d0 0.1411200080598672d0) [simple specialized vector] Element-type: DOUBLE-FLOAT Length: 3 ; No value There also exists *array-element-type-alist* that allows specifying package local default element types if the need ever arises. All of these can be overridden for the particular function by explicitly specifying the out or type arguments wherever appropriate.","title":"Controlling the default array-element-type"},{"location":"reasonably-fast-clhs-arrays/#reading-and-writing-arrays-to-disk","text":"As human-readable text files (slowest method currently): numpy: save, load As readable lisp objects: cl: read, write As binary objects useable by python: numpy-file-format: load-array, store-array","title":"Reading and writing arrays to disk"},{"location":"trash/","text":"Unfortunately, quality doesn't dictate popularity , but popularity would dictate the number of libraries, right? Numerical or scientific computing is one such domain where popularity of the ecosystem seems to be a big factor driving the tool availability. While Common Lisp does have mgl-mat magicl and perhaps a few others, their functionality is still dwarfed by the more popular languages. So, without using projects like cffi , cl-autowrap , py4cl , py4cl2-cffi , it can be hard if not impossible to meet one's needs while staying in Common Lisp. numericals or dense-numericals does not intend offer","title":"Trash"}]}